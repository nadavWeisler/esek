{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EldadKeha\\AppData\\Local\\Temp\\ipykernel_43824\\1883083308.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.optimize as opt\n",
    "from scipy.stats import pearsonr, norm, t, rankdata, median_abs_deviation, ncf,f, bootstrap\n",
    "import scipy.special as special\n",
    "import pandas as pd\n",
    "import pingouin\n",
    "\n",
    "# Relevant Fnctions for Interval/Ratio Correlations\n",
    "\n",
    "### 0. The non Central F function \n",
    "def Non_Central_CI_F(F_Statistic, df1, df2, confidence_level):\n",
    "    Upper_Limit = 1 - (1 - confidence_level) / 2\n",
    "    Lower_Limit = 1 - Upper_Limit\n",
    "    Lower_CI_Difference_Value = 1\n",
    "\n",
    "    def Lower_CI(F_Statistic, df1, df2, Upper_Limit, Lower_CI_Difference_Value):\n",
    "        Lower_Bound = [0.001, F_Statistic / 2, F_Statistic]       \n",
    "        while ncf.cdf(F_Statistic, df1, df2, Lower_Bound[0]) < Upper_Limit:\n",
    "            return [0, f.cdf(F_Statistic, df1, df2)] if f.cdf(F_Statistic, df1, df2) < Upper_Limit else None\n",
    "            Lower_Bound = [Lower_Bound[0] / 4, Lower_Bound[0], Lower_Bound[2]]   \n",
    "        while ncf.cdf(F_Statistic, df1, df2, Lower_Bound[2]) > Upper_Limit: Lower_Bound = [Lower_Bound[0], Lower_Bound[2], Lower_Bound[2] + F_Statistic]     \n",
    "        while Lower_CI_Difference_Value > 0.0000001:\n",
    "            Lower_Bound = [Lower_Bound[0], (Lower_Bound[0] + Lower_Bound[1]) / 2, Lower_Bound[1]] if ncf.cdf(F_Statistic, df1, df2, Lower_Bound[1]) < Upper_Limit else [Lower_Bound[1], (Lower_Bound[1] + Lower_Bound[2]) / 2, Lower_Bound[2]]  \n",
    "            Lower_CI_Difference_Value = abs(ncf.cdf(F_Statistic, df1, df2, Lower_Bound[1]) - Upper_Limit)        \n",
    "        return [Lower_Bound[1]]\n",
    "    \n",
    "    def Upper_CI(F_Statistic, df1, df2, Lower_Limit, Lower_CI_Difference_Value):\n",
    "        Upper_Bound = [F_Statistic, 2 * F_Statistic, 3 * F_Statistic]\n",
    "        while ncf.cdf(F_Statistic, df1, df2, Upper_Bound[0]) < Lower_Limit:Upper_Bound = [Upper_Bound[0] / 4, Upper_Bound[0], Upper_Bound[2]]\n",
    "        while ncf.cdf(F_Statistic, df1, df2, Upper_Bound[2]) > Lower_Limit: Upper_Bound = [Upper_Bound[0], Upper_Bound[2], Upper_Bound[2] + F_Statistic]\n",
    "        while Lower_CI_Difference_Value > 0.00001: Upper_Bound = [Upper_Bound[0], (Upper_Bound[0] + Upper_Bound[1]) / 2, Upper_Bound[1]] if ncf.cdf(F_Statistic, df1, df2, Upper_Bound[1]) < Lower_Limit else [Upper_Bound[1], (Upper_Bound[1] + Upper_Bound[2]) / 2, Upper_Bound[2]]; Lower_CI_Difference_Value = abs(ncf.cdf(F_Statistic, df1, df2, Upper_Bound[1]) - Lower_Limit)\n",
    "        return [Upper_Bound[1]]\n",
    "    \n",
    "    # Calculate lower and upper bounds\n",
    "    Lower_CI_Final = Lower_CI(F_Statistic, df1, df2, Upper_Limit, Lower_CI_Difference_Value)[0]\n",
    "    Upper_CI_Final = Upper_CI(F_Statistic, df1, df2, Lower_Limit, Lower_CI_Difference_Value)[0]\n",
    "\n",
    "    return Lower_CI_Final, Upper_CI_Final\n",
    "\n",
    "\n",
    "# 1. Main Function Pearson Correlation\n",
    "def pearson_correlation(x,y,confidence_level = 0.95):\n",
    "    Pearson_Correlation, Pearson_pvalue =  pearsonr(x,y)\n",
    "\n",
    "    sample_size = len(x)\n",
    "    Sample_1_Mean = np.mean(x)    \n",
    "    Sample_2_Mean = np.mean(y)\n",
    "    Sample_1_Standard_Deviation = np.std(x, ddof = 1)    \n",
    "    Sample_2_Standard_Deviation = np.std(y, ddof = 1)\n",
    "    Slope = (Sample_2_Standard_Deviation/Sample_1_Standard_Deviation) * Pearson_Correlation\n",
    "    constant = -(Slope * Sample_1_Mean - Sample_2_Mean)\n",
    "    Predicted_Values = Slope*np.array(x) + constant\n",
    "    Sum_of_square_deviations_from_mean_y = np.sum((y - Sample_2_Mean)**2) ##Sstotal\n",
    "    Sum_of_square_deviations_from_mean_x = np.sum((x - Sample_1_Mean)**2) ##Ssx\n",
    "    Sum_of_residuals = np.sum((y-Predicted_Values)**2) #SSres\n",
    "    sum_of_regression = Sum_of_square_deviations_from_mean_y - Sum_of_residuals #SSreg\n",
    "    Standard_error_slope = np.sqrt((1/(sample_size-2)) * (Sum_of_residuals/Sum_of_square_deviations_from_mean_x)) \n",
    "    t_score_of_the_slope = Slope / Standard_error_slope\n",
    "    Rsquare = (sum_of_regression/Sum_of_square_deviations_from_mean_y)\n",
    "    Approximated_r = Pearson_Correlation + (Pearson_Correlation*(1-Rsquare)) / (2*(sample_size-3))\n",
    "\n",
    "    # Approximated Standrd Errors - Gnambs, 2023\n",
    "    Fisher_SE = (1-Rsquare) / np.sqrt(sample_size*(1+Rsquare))  #Fisher, 1896\n",
    "    Fisher_Filon_SE = (1-Rsquare) / np.sqrt(sample_size)  #Fisher & Filon, 1898\n",
    "    Soper_1_SE = ((1-Rsquare) / np.sqrt(sample_size-1))  # Soper, 1913, Large Sample\n",
    "    Soper_2_SE = ((1-Rsquare)/np.sqrt(sample_size)) * ( 1 + (1+5.5*Rsquare) / (2*sample_size))  # Soper, 1913\n",
    "    Soper_3_SE = ((1-Rsquare)/np.sqrt(sample_size-1)) * (1 + (11*Rsquare) / (4*(sample_size-1)))  # Soper, 1913\n",
    "    Hoteling = ((1-Rsquare)/np.sqrt(sample_size-1)) * (1 + ((11*Rsquare) / (4*(sample_size-1))) + ((-192*Rsquare+479*Rsquare**2)/ (32*(sample_size-1)**2)))  # Hoteling, 1953\n",
    "    Ghosh_SE = np.sqrt((1 - ((sample_size - 2) * (1 - Rsquare) / (sample_size - 1) * special.hyp2f1(1, 1, (sample_size + 1) / 2, Rsquare)) - ((2 / (sample_size - 1) * (math.gamma(sample_size / 2) / math.gamma((sample_size - 1) / 2))**2) * np.sqrt(Rsquare) * special.hyp2f1(0.5, 0.5, (sample_size + 1) / 2, Rsquare))**2))\n",
    "    Hedges_SE = np.sqrt((Pearson_Correlation * special.hyp2f1(0.5, 0.5, (sample_size - 2) / 2, 1 - Rsquare))**2 - (1 - (sample_size - 3) * (1 - Rsquare) * special.hyp2f1(1, 1, sample_size / 2, 1 - Rsquare) / (sample_size - 2)))\n",
    "    Bonett_SE = (1-Rsquare) / np.sqrt(sample_size - 3)\n",
    "    Regression_SE = np.sqrt((1-Rsquare) / (sample_size - 2))\n",
    " \n",
    "    # Different Confidence Intervals for Pearson Correlation:\n",
    "    zcrit = norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "    tcrit = t.ppf(1 - (1 - confidence_level) / 2, sample_size-2)\n",
    "\n",
    "    # 1. Fisher Based CI's (1921)\n",
    "    Zr = 0.5 * np.log((1+Pearson_Correlation) / (1-Pearson_Correlation)) # Fisher's Transformation\n",
    "    Standard_Error_ZR =  1 / (np.sqrt(sample_size-3)) #Fisher (1921) Approximation of the Variance\n",
    "    Z_Lower = Zr - zcrit * Standard_Error_ZR\n",
    "    Z_Upper = Zr + zcrit * Standard_Error_ZR\n",
    "    Pearosn_Fisher_CI_lower = (np.exp(2*Z_Lower) - 1) / (np.exp(2*Z_Lower) + 1)\n",
    "    Pearosn_Fisher_CI_upper = (np.exp(2*Z_Upper) - 1) / (np.exp(2*Z_Upper) + 1)\n",
    "\n",
    "    # 2. Olivoto et al., 2018 - Half Widthe Confidecne Interval\n",
    "    Half_Width = (0.45304**abs(Pearson_Correlation)) * 2.25152 * (sample_size**-0.50089)\n",
    "    Lower_Olivoto = Pearson_Correlation - Half_Width\n",
    "    Upperr_Olivoto = Pearson_Correlation + Half_Width\n",
    "\n",
    "    # 3. Olkin & Fin, 1995\n",
    "    Standard_Error_R_Sqaure = np.sqrt((4 * Rsquare * (1- Rsquare)**2*(sample_size-2)**2) / ((sample_size**2 -1)*(sample_size+3)))# Olkin & Finn, 1995\n",
    "    Pearson_CI_Olkin_Fin_Lower = np.sign(Slope) * np.sqrt(abs(Rsquare - zcrit * Standard_Error_R_Sqaure))\n",
    "    Pearson_CI_Olkin_Fin_Upper = np.sqrt(Rsquare + zcrit * Standard_Error_R_Sqaure)\n",
    "\n",
    "    # 4. Non-Central CI's based on F distribtion\n",
    "    Lowerc_NCP_CI, Upper_NCP_CI = Non_Central_CI_F(t_score_of_the_slope**2, 1, sample_size-2, confidence_level)\n",
    "    Pearson_CI_Eta_lower = np.sqrt(Lowerc_NCP_CI / (Lowerc_NCP_CI + (sample_size-2)))\n",
    "    Pearson_CI_Eta_upper = np.sqrt(Upper_NCP_CI / (Upper_NCP_CI + (sample_size-2)))\n",
    "\n",
    "    # 5. Bootstrapping CI's\n",
    "    def pearson_r(x, y):\n",
    "        return pearsonr(x, y)[0]\n",
    "    Bootstrap_Sample = bootstrap((x, y), pearson_r, n_resamples = 1000, vectorized=False, paired=True, random_state=np.random.default_rng(), confidence_level = confidence_level)\n",
    "    Pearosn_Bootstrapping_CI = Bootstrap_Sample.confidence_interval\n",
    "\n",
    "    # 6. Bonett 2008 Procedure\n",
    "    Lower_ci_Bonett = math.tanh(math.atanh(Pearson_Correlation) - zcrit * Bonett_SE)\n",
    "    Upper_ci_Bonett= math.tanh(math.atanh(Pearson_Correlation) + zcrit * Bonett_SE)\n",
    "\n",
    "    # 7. Null Counter-Null Interval\n",
    "    Lower_ci_Bonett = math.tanh(math.atanh(Pearson_Correlation) - zcrit * Bonett_SE)\n",
    "    Upper_ci_Bonett= math.tanh(math.atanh(Pearson_Correlation) + zcrit * Bonett_SE)\n",
    "\n",
    "\n",
    "    # More Alternative Effect Sizes for Pearson Correlation \n",
    "    Common_Language_Effect_Size_Dunlap = np.arcsin(Pearson_Correlation) / math.pi + 0.\n",
    "    Counter_Null_EffectSize = np.sqrt(4*Rsquare/(1+3*Rsquare))    \n",
    "    Absolute_R_Square = 1 - (sum_of_regression / (np.sum((y - np.median(y))**2))) # Bertsimas et al., 2008\n",
    "\n",
    "    # Confidence Intervals for other Effect Sizes\n",
    "    Absolute_R_Square_SE = (1-Absolute_R_Square**2) / np.sqrt(sample_size - 3)\n",
    "\n",
    "    T_Statistic_CLES = Common_Language_Effect_Size_Dunlap * np.sqrt((sample_size-2) / (1 - Common_Language_Effect_Size_Dunlap**2))\n",
    "\n",
    "\n",
    "    Lower_ci_CLES = np.arcsin(Lower_ci_Bonett) / math.pi + 0.\n",
    "    Upper_ci_CLES = np.arcsin(Upper_ci_Bonett) / math.pi + 0.\n",
    "    #Lower_ci_Counter_Null = np.sqrt(4*Lower_ci_Bonett/(1+3*Lower_ci_Bonett))\n",
    "    #Upper_ci_Counter_Null = np.sqrt(4*Upper_ci_Bonett/(1+3*Upper_ci_Bonett))\n",
    "    Lower_ci_Absolute_R = math.tanh(math.atanh(Absolute_R_Square_SE) - zcrit * Absolute_R_Square_SE)\n",
    "    Upper_ci_Absolute_R = math.tanh(math.atanh(Pearson_Correlation) + zcrit * Absolute_R_Square_SE)\n",
    "\n",
    " \n",
    "    results = {}\n",
    "    results[\"Pearson Correlation\"] = (Pearson_Correlation)\n",
    "    results[\"t score\"] = round(t_score_of_the_slope, 4)\n",
    "    results[\"Degrees of Freedom\"] = round(sample_size-2, 4)\n",
    "    results[\"Pearson Correlation P-value\"] = round(Pearson_pvalue, 4)\n",
    "    results[\"Standrd Error of the Slope\"] = round(Standard_error_slope, 4)\n",
    "    results[\"Constant\"] = round(constant, 4)\n",
    "    results[\"Slope\"] = round(Slope, 4)\n",
    "\n",
    "    results[\"Standard Error Fisher \"] = round(Fisher_SE, 4)\n",
    "    results[\"Standard Error Fisher & Filon\"] = round(Fisher_Filon_SE, 4)\n",
    "    results[\"Standard Error Soper-I\"] = round(Soper_1_SE, 4)\n",
    "    results[\"Standard Error Soper-II\"] = round(Soper_2_SE, 4)\n",
    "    results[\"Standard Error Soper-III\"] = round(Soper_3_SE, 4)\n",
    "    results[\"Standard Error Hoteling\"] = round(Hoteling, 4)\n",
    "    results[\"Standard Error Ghosh\"] = round(Ghosh_SE, 4)\n",
    "    results[\"Standard Error Hedges\"] = round(Hedges_SE, 4)\n",
    "    results[\"Standard Error Bonett\"] = round(Bonett_SE, 4)    \n",
    "    results[\"Standard Error From Regression\"] = round(Regression_SE, 4)\n",
    "\n",
    "    results[\"Fisher's Zr\"] = (Zr)\n",
    "    results[\"Standard Error of Zr\"] = (Standard_Error_ZR)\n",
    "    results[\"Confidence Intervals Fisher (1921)\"] = f\"({round(Pearosn_Fisher_CI_lower, 4)}, {round(Pearosn_Fisher_CI_upper, 4)})\"\n",
    "    results[\"Confidence Intervals Olivoto\"] = f\"({round(Lower_Olivoto, 4)}, {round(Upperr_Olivoto, 4)})\"\n",
    "    results[\"Confidence Intervals for R-square (Olkin & Fin)\"] = f\"({round(Pearson_CI_Olkin_Fin_Lower, 4)}, {round(Pearson_CI_Olkin_Fin_Upper, 4)})\"\n",
    "\n",
    "    results[\"Common Language Effect Size (Dunlap, 1994)\"] = (Common_Language_Effect_Size_Dunlap)\n",
    "    results[\"Approximated r (Hedges & Olkin, 1985)\"] = (Approximated_r)\n",
    "    results[\"Counter_Null_EffectSize\"] = (Counter_Null_EffectSize)\n",
    "    results[\"Absolute R Square\"] = (Absolute_R_Square)\n",
    "    results[\"Absolute R Square SE\"] = round(Absolute_R_Square_SE, 4)\n",
    "\n",
    "    results[\"Confidence Intervals CLES\"] = f\"({round(Lower_ci_CLES, 4)}, {round(Upper_ci_CLES, 4)})\"\n",
    "    results[\"Confidence Intervals Absolute R\"] = f\"({round(Lower_ci_Absolute_R, 4)}, {round(Upper_ci_Absolute_R, 4)})\"\n",
    "\n",
    "    results[\"Confidence Intervals Bootstrapping\"] = f\"({round(Pearosn_Bootstrapping_CI[0], 4)}, {round(Pearosn_Bootstrapping_CI[1], 4)})\"\n",
    "    results[\"Confidence Intervals Bonett\"] = f\"({round(Lower_ci_Bonett, 4)}, {round(Upper_ci_Bonett, 4)})\"\n",
    "    results[\"Confidence Intervals Eta Lower\"] = round(Pearson_CI_Eta_lower, 4)\n",
    "    results[\"Confidence Intervals Eta Upper\"] = round(Pearson_CI_Eta_upper, 4)\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "# 2. Different R_Square Estimators\n",
    "\n",
    "def Rsquare_Estimation(Rsquare, sample_size, Number_Of_Predictors):\n",
    "    dftotal = sample_size-1\n",
    "    df_residual = sample_size - Number_Of_Predictors - 1\n",
    "    df = sample_size - Number_Of_Predictors\n",
    "\n",
    "    # Adjusted R square\n",
    "    term1 = (sample_size - 3) * (1 - Rsquare) / df_residual\n",
    "    Smith = 1 - (sample_size / (df)) * (1 - Rsquare) # Smith, 1929\n",
    "    Ezekiel = 1 - (dftotal / df_residual) * (1 - Rsquare) #Ezekiel, 1930\n",
    "    Wherry = 1 - (dftotal / df) * (1 - Rsquare) #Wherry, 1931\n",
    "    olkin_pratt = 1 - term1 * special.hyp2f1(1, 1, (df + 1) / 2, 1 - Rsquare) #Olkin & Pratt, 1958\n",
    "    olkin_pratt = 1 - term1 * special.hyp2f1(1, 1, (df + 1) / 2, 1 - Rsquare) #Olkin & Pratt, 1958\n",
    "\n",
    "    Cattin = 1 - term1 * ((1 + (2 * (1 - Rsquare)) / df_residual) + ((8 * (1 - Rsquare) ** 2) / (df_residual * (df + 3)))) # Cattin, 1980 (Approximation to Olkin and Pratt)\n",
    "    Pratt = 1 - (((sample_size - 3) * (1 - Rsquare)) / df_residual) * (1 + (2 * (1 - Rsquare)) / (df - 2.3)) # Pratt, 1964\n",
    "    Herzberg = 1 - (((sample_size - 3) * (1 - Rsquare)) / df_residual) * (1 + (2 * (1 - Rsquare)) / (df+1)) # Herzberg, 1969\n",
    "    Claudy =   1 - (((sample_size - 4) * (1 - Rsquare)) / df_residual) * (1 + (2 * (1 - Rsquare)) / (df+1)) # Herzberg, 1978\n",
    "    def Alf_Graf_MLE(Rsquare, sample_size, Number_Of_Predictors):     # Alf and Graf 2002, MLE\n",
    "        return opt.minimize_scalar(lambda rho: (1 - rho) ** (sample_size / 2) * (special.hyp2f1(0.5 * sample_size, 0.5 * sample_size, 0.5 * Number_Of_Predictors, rho * Rsquare)) * -1,bounds=(0, 1),method='bounded').x\n",
    "    AlfGraf = Alf_Graf_MLE(Rsquare, sample_size, Number_Of_Predictors)\n",
    "\n",
    "    # Squared Cross-Validity Coefficient\n",
    "    Lord = 1 - (sample_size+Number_Of_Predictors+1) / (sample_size-Number_Of_Predictors-1) * (1-Rsquare) #Uhl & Eisenberg, 1970, also known as the Lord formula (it is most cited by this name)\n",
    "    Lord_Nicholson = 1 - ((sample_size+Number_Of_Predictors +1) / sample_size) * (dftotal/df_residual) * (1-Rsquare) \n",
    "    Darlington_Stein = 1 - ((sample_size + 1) / sample_size) * (dftotal/df_residual) * ((sample_size-2) / (df-2)) * (1-Rsquare)\n",
    "    Burket = (sample_size*Rsquare - Number_Of_Predictors) / (np.sqrt(Rsquare)*df) \n",
    "    Brown_Large_Sample = (((df - 3) * Ezekiel**2 +     Ezekiel)     / ((sample_size-2*Number_Of_Predictors - 2) * Ezekiel + Number_Of_Predictors))\n",
    "    Brown_small_Sample = (((df - 3) * olkin_pratt**2 + olkin_pratt) / ((sample_size-2*Number_Of_Predictors - 2) * olkin_pratt + Number_Of_Predictors))\n",
    "    Rozeboom = 1 - ((sample_size+Number_Of_Predictors) / df) * (1 - Rsquare) #Rozeboom, 1978\n",
    "    Rozeboom2_Large_Sample = Ezekiel * ((1 + (Number_Of_Predictors/ (df-2)) * ((1-Ezekiel)/Ezekiel))**-1) #Rozeboom, 1981 \n",
    "    Rozeboom2_small_Sample = olkin_pratt * ((1 + (Number_Of_Predictors/ (df-2)) * ((1-olkin_pratt)/olkin_pratt))**-1) #Rozeboom, 1981\n",
    "    Claudy1_Large_Sample = (2* Ezekiel - (Rsquare))\n",
    "    Claudy1_Small_Sample = (2* olkin_pratt - (Rsquare))\n",
    "\n",
    "    results = {\n",
    "        \"Smith (1928)\": Smith,\n",
    "        \"Ezekiel (1930)\": Ezekiel,\n",
    "        \"Wherry (1931)\": Wherry,\n",
    "        \"Olkin & Pratt (1958)\": olkin_pratt,\n",
    "        \"Olkin & Pratt, Pratt's Approximation (1964)\": Pratt,\n",
    "        \"Olkin & Pratt, Herzberg's Approximation (1968)\": Herzberg,\n",
    "        \"Olkin & Pratt, Claudy's Approximation (1978)\": Claudy,\n",
    "        \"Olkin & Pratt, Cattin's Approximation (1980)\": Cattin,\n",
    "        \"Alf and Graf MLE (2002)\": AlfGraf,\n",
    "        \"Lord(1950)\": Lord, \n",
    "        \"Lord_Nicholson(1960)\": Lord_Nicholson,\n",
    "        \"Darlington/Stein (1967/1960)\": Darlington_Stein,\n",
    "        \"Burket (1964)\": Burket,\n",
    "        \"Brown_Large_Samples (1975)\": Brown_Large_Sample,\n",
    "        \"Brown_Small_Samples (1975)\": Brown_small_Sample,\n",
    "        \"Rozeboom I (1978)\": Rozeboom,\n",
    "        \"Rozeboom II Large Samples- (1978)\": Rozeboom2_Large_Sample,\n",
    "        \"Rozeboom II Small Samples (1978)\": Rozeboom2_small_Sample, \n",
    "        \"Claudy-I, Large Samples (1978)\": Claudy1_Large_Sample,\n",
    "        \"Claudy-I, Small Samples (1978)\": Claudy1_Small_Sample,   \n",
    "    }\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "\n",
    "# 3. Alternative Robust Measures of Pearson Correlation (Including Skipped Correlation)\n",
    "def Robust_Measures_Interval(x,y, confidence_level = 0.95): \n",
    "    \n",
    "    # A. Percentage Bend Correlation\n",
    "    sample_size = len(x)\n",
    "    Omega_Hat_X= sorted((abs(np.array(x) - np.median(x))))[math.floor((1 - 0.2**2) * sample_size) - 1]\n",
    "    psix = (x-np.array(np.median(x)))/Omega_Hat_X\n",
    "    i1 = sum((x_i - np.median(x)) / Omega_Hat_X < -1 for x_i in x)\n",
    "    i2 = sum((x_i - np.median(np.median(x))) / Omega_Hat_X > 1 for x_i in x)\n",
    "    Sx = np.sum(np.where(np.logical_or(psix < -1, psix > 1), 0, x))\n",
    "    Phi_x = (Omega_Hat_X * (i2 - i1) + Sx) / (sample_size - i1 - i2)\n",
    "    Ai = np.clip((x - Phi_x) / Omega_Hat_X, -1, 1)\n",
    "\n",
    "    Omega_Hat_Y = sorted((abs(np.array(y) - np.median(y))))[math.floor((1 - 0.2**2) * sample_size) - 1]\n",
    "    psiy = (y - np.array(np.median(y))) / Omega_Hat_Y\n",
    "    i1_y = sum((y_i - np.median(y)) / Omega_Hat_Y < -1 for y_i in y)\n",
    "    i2_y = sum((y_i - np.median(np.median(y))) / Omega_Hat_Y > 1 for y_i in y)\n",
    "    Sy = np.sum(np.where(np.logical_or(psiy < -1, psiy > 1), 0, y))\n",
    "    Phi_y = (Omega_Hat_Y * (i2_y - i1_y) + Sy) / (sample_size - i1_y - i2_y)\n",
    "    Bi = np.clip((y - Phi_y) / Omega_Hat_Y, -1, 1)\n",
    "\n",
    "    Percentage_Bend_Correlation = np.sum(Ai * Bi)/np.sqrt(np.sum(Ai**2) * np.sum(Bi**2))\n",
    "\n",
    "    # B. Winsorized Correlation\n",
    "    lower_items = int(np.floor(0.2 * sample_size)) + 1\n",
    "    upper_items = len(x) - lower_items + 1\n",
    "    sorted_x = np.sort(x)\n",
    "    sorted_y = np.sort(y)\n",
    "    \n",
    "    Winzorized_X = np.where((x <= sorted_x[lower_items - 1]) | (x >= sorted_x[upper_items - 1]),np.where(x <= sorted_x[lower_items - 1], sorted_x[lower_items - 1], sorted_x[upper_items - 1]),x)\n",
    "    Winzorized_Y = np.where((y <= sorted_y[lower_items - 1]) | (y >= sorted_y[upper_items - 1]),np.where(y <= sorted_y[lower_items - 1], sorted_y[lower_items - 1], sorted_y[upper_items - 1]),y)\n",
    "    Winzorized_Correlation = np.corrcoef(Winzorized_X, Winzorized_Y)[0, 1]\n",
    "\n",
    "    # C. Biweight Midcorrelation (Verified with asbio for r)\n",
    "    Ui = (x - np.median(x)) / (9 * t.ppf(0.75, 100000) * 1.4826*np.median(np.abs(x- np.median(x))))\n",
    "    Vi = (y - np.median(y)) / (9 * t.ppf(0.75, 100000) * 1.4826*np.median(np.abs(y - np.median(y))))\n",
    "    Ai = np.where((Ui <= -1) | (Ui >= 1), 0, 1)\n",
    "    Bi = np.where((Vi <= -1) | (Vi >= 1), 0, 1)\n",
    "\n",
    "    Sxx = (np.sqrt(sample_size) * np.sqrt(sum((Ai * ((x - np.median(x))**2)) * ((1 - Ui**2)**4))) /  abs(sum(Ai * (1 - Ui**2) * (1 - 5 * Ui**2))))**2\n",
    "    Syy = (np.sqrt(sample_size) * np.sqrt(sum((Bi * ((y - np.median(y))**2)) * ((1 - Vi**2)**4))) / (abs(sum(Bi * (1 - Vi**2) * (1 - 5 * Vi**2)))))**2\n",
    "    Sxy = sample_size * sum((Ai *(x - np.median(x))) * ((1-Ui**2)**2) * (Bi* (y-np.median(y))) * ((1-Vi**2)**2)) / (sum((Ai* (1-Ui**2)) * (1-5*Ui**2)) * sum((Bi* (1-Vi**2)) * (1 - 5 * Vi**2)))\n",
    "    Biweight_midcorrelation = Sxy / (np.sqrt(Sxx * Syy))\n",
    "\n",
    "    # D. Gausian Rank Correlation \n",
    "    Normalized_X = norm.ppf((np.argsort(x) + 1) / (len(x) + 1))\n",
    "    Normalized_Y = norm.ppf((np.argsort(y) + 1) / (len(y) + 1))\n",
    "    Gaussian_Rank_Correlation = pearsonr(Normalized_X, Normalized_Y)\n",
    "    zcrit = t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    Normalized_X = norm.ppf((rankdata(x) / (len(x) + 1)))\n",
    "    Normalized_Y = norm.ppf((rankdata(y) / (len(y) + 1)))\n",
    "    Gaussian_Rank_Correlation, GRS_pvalue = pearsonr(Normalized_X, Normalized_Y)\n",
    "\n",
    "    # E. Robust correlational median estimator (10% trimming)\n",
    "    trimmedx = x.astype(float)\n",
    "    trimmedy = y.astype(float)\n",
    "    trimmedx[np.logical_or(trimmedx <= np.percentile(trimmedx, 10), trimmedx >= np.percentile(trimmedx, 90))] = np.nan\n",
    "    trimmedy[np.logical_or(trimmedy <= np.percentile(trimmedy, 10), trimmedy >= np.percentile(trimmedy, 90))] = np.nan\n",
    "    trimmed_data = pd.DataFrame({'x': trimmedx, 'y': trimmedy}).dropna()\n",
    "\n",
    "    median_trimmed_x = np.median(trimmed_data['x'])\n",
    "    median_trimmed_y = np.median(trimmed_data['y'])\n",
    "\n",
    "    Trimmed_Covariance = np.sum((np.array(trimmed_data['x']) - median_trimmed_x) * (np.array(trimmed_data['y']) - median_trimmed_y))\n",
    "    Squared_Deviation_Median_x = np.sum((median_trimmed_x -   np.array(trimmed_data['x']))**2)\n",
    "    Squared_Deviation_Median_y = np.sum((median_trimmed_y - np.array(trimmed_data['y']))**2)\n",
    "    rCME = float(((Trimmed_Covariance)) / (np.sqrt((Squared_Deviation_Median_x)*Squared_Deviation_Median_y)))\n",
    "\n",
    "    # F. MAD correlation Coefficient\n",
    "    Median_x = np.median(x)\n",
    "    Median_y = np.median(y)\n",
    "    MAD_x = np.median(abs(x - Median_x))\n",
    "    MAD_y = np.median(abs(y - Median_y))\n",
    "\n",
    "    u = ((x - np.median(x)) / (MAD_x*np.sqrt(2))) + ((y - np.median(y)) / (MAD_y*np.sqrt(2)))\n",
    "    v = ((x - np.median(x)) / (MAD_x*np.sqrt(2))) - ((y - np.median(y)) / (MAD_y*np.sqrt(2)))\n",
    "    median_u= np.median(u)\n",
    "    median_v= np.median(v)\n",
    "\n",
    "    rmad = ((np.median(abs(u - median_u)))**2 -  (np.median(abs(v - median_v)))**2) / ((np.median(abs(u - median_u)))**2 + (np.median(abs(v - median_v)))**2) \n",
    "    rmed = ((np.median(abs(u)))**2 - (np.median(abs(v)))**2) / ((np.median(abs(u)))**2 + (np.median(abs(v)))**2)\n",
    "\n",
    "    # Statistics and p-values \n",
    "    degrees_of_freedom = sample_size-2\n",
    "    degrees_of_freedom_WC = sample_size - 2 * int(np.floor(0.2 * sample_size)) - 2\n",
    "    degrees_of_freedom_rCME = len(np.array(trimmed_data['x'])) - 2\n",
    "  \n",
    "    T_Statistic_PB = Percentage_Bend_Correlation * np.sqrt((degrees_of_freedom) / (1 - Percentage_Bend_Correlation**2))\n",
    "    T_Statistic_WC = Winzorized_Correlation * np.sqrt((degrees_of_freedom_WC - 2) / (1 - Winzorized_Correlation**2))\n",
    "    T_statistic_BM = Biweight_midcorrelation* np.sqrt((degrees_of_freedom - 2) / (1 - Biweight_midcorrelation**2))\n",
    "    T_Statistic_rCME = rCME * np.sqrt((degrees_of_freedom_rCME - 2) / (1 - rCME**2))\n",
    "    T_Statistic_rmed = rmed * np.sqrt((degrees_of_freedom - 2) / (1 - rmed**2))\n",
    "    T_statistic_rmad = rmad* np.sqrt((degrees_of_freedom - 2) / (1 - rmad**2))\n",
    "    T_statistic_Gaussian_Rank_Correlation = Gaussian_Rank_Correlation * np.sqrt((degrees_of_freedom - 2) / (1 - Gaussian_Rank_Correlation**2))\n",
    "\n",
    "    P_value_BM =        t.sf(abs(T_statistic_BM), degrees_of_freedom)    \n",
    "    P_Value_Winzoried = t.sf(abs(T_Statistic_WC), degrees_of_freedom_WC)\n",
    "    P_value_PB  =       t.sf(abs(T_Statistic_PB), degrees_of_freedom)\n",
    "    P_value_rmed =      t.sf(abs(T_Statistic_rmed), degrees_of_freedom)    \n",
    "    P_Value_rmad =      t.sf(abs(T_statistic_rmad), degrees_of_freedom)\n",
    "    P_value_rCME  =     t.sf(abs(T_Statistic_rCME), degrees_of_freedom_rCME)\n",
    "\n",
    "    # Confidence Intervals (Fisher Based)\n",
    "    SE_Bonett_BM = (1 - Biweight_midcorrelation**2) / np.sqrt(sample_size-3)\n",
    "    SE_Bonett_GRC = (1 - Gaussian_Rank_Correlation**2) / np.sqrt(sample_size-3)\n",
    "    SE_Bonett_Winsorized = (1 - Winzorized_Correlation**2) / np.sqrt(degrees_of_freedom_WC-1)\n",
    "    SE_Bonett_PB = (1 - Percentage_Bend_Correlation**2) / np.sqrt(sample_size-3)\n",
    "    SE_Bonett_rmed = (1 - rmed**2) / np.sqrt(sample_size-3)\n",
    "    SE_Bonett_rmad = (1 - rmad**2) / np.sqrt(sample_size-3)\n",
    "    SE_Bonett_rCME = ((1 - rCME**2) / np.sqrt(degrees_of_freedom_rCME-1))\n",
    "\n",
    "    Lower_ci_BM = math.tanh(math.atanh(Biweight_midcorrelation) - zcrit * SE_Bonett_BM)\n",
    "    Upper_ci_BM = math.tanh(math.atanh(Biweight_midcorrelation) + zcrit * SE_Bonett_BM)\n",
    "    Lower_ci_GRC = math.tanh(math.atanh(Gaussian_Rank_Correlation) - zcrit * SE_Bonett_GRC)\n",
    "    Upper_ci_GRC = math.tanh(math.atanh(Gaussian_Rank_Correlation) + zcrit * SE_Bonett_GRC)\n",
    "    Lower_ci_Winsorized = math.tanh(math.atanh(Winzorized_Correlation) - zcrit * SE_Bonett_Winsorized)\n",
    "    Upper_ci_Winsorized = math.tanh(math.atanh(Winzorized_Correlation) + zcrit * SE_Bonett_Winsorized)\n",
    "    Lower_ci_PB = math.tanh(math.atanh(Percentage_Bend_Correlation) - zcrit * SE_Bonett_PB)\n",
    "    Upper_ci_PB = math.tanh(math.atanh(Percentage_Bend_Correlation) + zcrit * SE_Bonett_PB)\n",
    "    #Lower_ci_rmed = math.tanh(math.atanh(rmed) - zcrit * SE_Bonett_rmed)\n",
    "    #Upper_ci_rmed = math.tanh(math.atanh(rmed) + zcrit * SE_Bonett_rmed)\n",
    "    #Lower_ci_rmad = math.tanh(math.atanh(rmad) - zcrit * SE_Bonett_rmad)\n",
    "    #Upper_ci_rmad = math.tanh(math.atanh(rmad) + zcrit * SE_Bonett_rmad)\n",
    "    #Lower_ci_rCME = math.tanh(math.atanh(rCME) - zcrit * SE_Bonett_rCME)\n",
    "    #Upper_ci_rCME = math.tanh(math.atanh(rCME) + zcrit * SE_Bonett_rCME)\n",
    "\n",
    "    results = {\n",
    "    \"Trimmed Data\": trimmed_data,\n",
    "    \"Robust Correlation Median Estimator\": rCME,\n",
    "    \"Robust Correlational median estimator SE\": \"The Trimmed Data is Too Small, Can't Calculate the Variance\" if degrees_of_freedom_rCME <=1 else SE_Bonett_rCME,\n",
    "    \"Robust Correlational median Estimator Stat\": \"The Trimmed Data is Too Small\" if degrees_of_freedom_rCME <=1 else T_Statistic_rCME ,\n",
    "    \"Robust Correlational median estimator p-value\": \"The Trimmed Data is Too Small\" if degrees_of_freedom_rCME <=1 else P_value_rCME,\n",
    "    \"Robust Correlational median estimator Degrees of Freedom\": degrees_of_freedom_rCME ,\n",
    "    #\"Robust Correlational median estimator CI\": (Lower_ci_rCME, Upper_ci_rCME),\n",
    "    \n",
    "    \"Biweight Midcorrelation\": Biweight_midcorrelation,\n",
    "    \"Biweight Midcorrelation SE\": SE_Bonett_BM,\n",
    "    \"Biweight Midcorrelation Statistic\": T_statistic_BM,\n",
    "    \"Biweight Midcorrelation p-value\": P_value_BM,\n",
    "    \"Biweight Midcorrelation CI\": (Lower_ci_BM, Upper_ci_BM),\n",
    "    \n",
    "    \"Gaussian Rank Correlation\": Gaussian_Rank_Correlation,\n",
    "    \"Gaussian Rank Correlation SE\": SE_Bonett_GRC,\n",
    "    \"Gaussian Rank Correlation Statistic\": T_statistic_Gaussian_Rank_Correlation,\n",
    "    \"Gaussian Rank Correlation p-value\": GRS_pvalue,\n",
    "    \"Gaussian Rank Correlation CI\": (Lower_ci_GRC, Upper_ci_GRC),\n",
    "    \n",
    "    \"Winsorized Correlation\": Winzorized_Correlation,\n",
    "    \"Winsorized Correlation SE\": SE_Bonett_Winsorized,\n",
    "    \"Winsorized Correlation Statistic\": T_Statistic_WC,\n",
    "    \"Winsorized Correlation p-value\": P_Value_Winzoried,\n",
    "    \"Winsorized Correlation CI\": (Lower_ci_Winsorized, Upper_ci_Winsorized),\n",
    "    \n",
    "    \"Percentage Bend Correlation\": Percentage_Bend_Correlation,\n",
    "    \"Percentage Bend Correlation SE\": SE_Bonett_PB,\n",
    "    \"Percentage Bend Correlation Statistic\": T_Statistic_PB,\n",
    "    \"Percentage Bend Correlation p-value\": P_value_PB,\n",
    "    \"Percentage Bend Correlation CI\": (Lower_ci_PB, Upper_ci_PB),\n",
    "    \n",
    "    \"MAD correlation Coefficient\": rmad,\n",
    "    \"MAD correlation Coefficient SE\": SE_Bonett_rmad,\n",
    "    \"MAD correlation Coefficient Statistic\": T_statistic_rmad,\n",
    "    \"MAD correlation Coefficient p-value\": P_Value_rmad,\n",
    "    #\"MAD correlation Coefficient CI\": (Lower_ci_rmad, Upper_ci_rmad),\n",
    "    \n",
    "    \"Robust correlational median estimator\": rmed,\n",
    "    \"Robust correlational median estimator SE\": SE_Bonett_rmed,\n",
    "    \"Robust correlational median estimator Statistic\": T_Statistic_rmed,\n",
    "    \"Robust correlational median estimator p-value\": P_value_rmed,\n",
    "    #\"Robust correlational median estimator CI\": (Lower_ci_rmed, Upper_ci_rmed)\n",
    "     }\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def skipped_Correlation(x, y, confidence_level):\n",
    "    from pingouin.utils import _is_sklearn_installed\n",
    "\n",
    "    _is_sklearn_installed(raise_error=True)\n",
    "    from scipy.stats import chi2\n",
    "    from sklearn.covariance import MinCovDet\n",
    "\n",
    "    sample_size = len(x)\n",
    "    X = np.column_stack((x, y))\n",
    "    nrows, ncols = X.shape\n",
    "    gval = np.sqrt(chi2.ppf(0.975, 2))\n",
    "    center = MinCovDet(random_state=42).fit(X).location_\n",
    "    B = X - center\n",
    "    bot = (B**2).sum(axis=1)\n",
    "    dis = np.zeros(shape=(nrows, nrows))\n",
    "    for i in np.arange(nrows):\n",
    "        if bot[i] != 0:  # Avoid division by zero error\n",
    "            dis[i, :] = np.linalg.norm(B.dot(B[i, :, None]) * B[i, :] / bot[i], axis=1)\n",
    "\n",
    "    def ideal_forth_interquartile_range(x):\n",
    "        n = len(x)\n",
    "        j = int(np.floor(n / 4 + 5 / 12))\n",
    "        y = np.sort(x)\n",
    "        g = (n / 4) - j + (5 / 12)\n",
    "        low = (1 - g) * y[j - 1] + g * y[j]\n",
    "        k = n - j + 1\n",
    "        up = (1 - g) * y[k - 1] + g * y[k - 2]\n",
    "        return up - low\n",
    "\n",
    "    # One can either use the MAD or the IQR (see Wilcox 2012)\n",
    "    iqr = np.apply_along_axis(ideal_forth_interquartile_range, 1, dis)\n",
    "    thresh_IQR = np.median(dis, axis=1) + gval * iqr\n",
    "    outliers_iqr = np.apply_along_axis(np.greater, 0, dis, thresh_IQR).any(axis=0)\n",
    "    mad = np.apply_along_axis(median_abs_deviation, 1, dis)\n",
    "    thresh_mad = np.median(dis, axis=1) + gval * mad\n",
    "    outliers_mad = np.apply_along_axis(np.greater, 0, dis, thresh_mad).any(axis=0)\n",
    "\n",
    "    # Compute correlation on remaining data\n",
    "    Pearson_Skipped_Correlation_IQR, pval_Skipped_Correlation_IQR = pearsonr(X[~outliers_iqr, 0], X[~outliers_iqr, 1])\n",
    "    Pearson_Skipped_Correlation_MAD, pval_Skipped_Correlation_MAD = pearsonr(X[~outliers_mad, 0], X[~outliers_mad, 1])\n",
    "\n",
    "    # Confidence Intervals using Bonett (2008)\n",
    "    zcrit = norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "    sample_size_Skipped_iqr = len(X[~outliers_iqr, 0])\n",
    "    sample_size_Skipped_mad = len(X[~outliers_mad, 0])\n",
    "\n",
    "    SE_Bonett_Skipped_Correlation_IQR = (1 - Pearson_Skipped_Correlation_IQR**2) / np.sqrt(sample_size_Skipped_iqr-3)\n",
    "    SE_Bonett_Skipped_Correlation_MAD = (1 - Pearson_Skipped_Correlation_MAD**2) / np.sqrt(sample_size_Skipped_mad-3)\n",
    "\n",
    "    Lower_ci_Skipped_IQR = math.tanh(math.atanh(Pearson_Skipped_Correlation_IQR) - zcrit * SE_Bonett_Skipped_Correlation_IQR)\n",
    "    Upper_ci_Skipped_IQR= math.tanh(math.atanh(Pearson_Skipped_Correlation_IQR) + zcrit * SE_Bonett_Skipped_Correlation_IQR)\n",
    "    Lower_ci_Skipped_MAD = math.tanh(math.atanh(Pearson_Skipped_Correlation_MAD) - zcrit * SE_Bonett_Skipped_Correlation_MAD)\n",
    "    Upper_ci_Skipped_MAD = math.tanh(math.atanh(Pearson_Skipped_Correlation_MAD) + zcrit * SE_Bonett_Skipped_Correlation_MAD)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Skipped Correlation IQR based\"] = Pearson_Skipped_Correlation_IQR\n",
    "    results[\"Skipped Correlation MAD based\"] = Pearson_Skipped_Correlation_MAD\n",
    "    results[\"Skipped Correlation IQR based p-value\"] = pval_Skipped_Correlation_IQR\n",
    "    results[\"Skipped Correlation MAD based p-value\"] = pval_Skipped_Correlation_MAD\n",
    "    results[\"Skipped Correlation IQR based CI's\"] = [Lower_ci_Skipped_IQR, Upper_ci_Skipped_IQR]\n",
    "    results[\"Skipped Correlation MAD based CI's\"] = [Lower_ci_Skipped_MAD, Upper_ci_Skipped_MAD]\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "class OrdinalInterval():\n",
    "    @staticmethod\n",
    "    def IntervalOrdinal_from_Data(params: dict) -> dict:\n",
    "        \n",
    "        # Set Params:\n",
    "        Variable1 = params[\"Variable 1\"]\n",
    "        Variable2 = params[\"Variable 2\"]\n",
    "        confidence_level_percentages = params[\"Confidence Level\"]\n",
    "\n",
    "        # Convrt the variables into vectors\n",
    "        confidence_level = confidence_level_percentages/ 100\n",
    "\n",
    "        R_Square = pearsonr(Variable1,Variable2)[0]**2\n",
    "        sample_size = len(Variable1)\n",
    "        Rsqare_Estiamtion_Output = Rsquare_Estimation(R_Square, sample_size, 1)\n",
    "\n",
    "        # Calculate Effect Sizes using All Functions        \n",
    "        skipped_Correlation_measures = skipped_Correlation(Variable1, Variable2, confidence_level)\n",
    "        Robust_Correaltions_Output = Robust_Measures_Interval(Variable1,Variable2,confidence_level)        \n",
    "        Main_Procedure_Output = pearson_correlation(Variable1, Variable2,confidence_level)\n",
    "\n",
    "        results = {}\n",
    "        results[\"Skipped Corrleation \"] =  skipped_Correlation_measures\n",
    "        results[\"_______________________\"] = \"\"\n",
    "        results[\"Robust Correlation\"] =  Robust_Correaltions_Output\n",
    "        results[\"______________________\"] = \"\"\n",
    "        results[\"Rsqare Estiamtion Output\"] =  Rsqare_Estiamtion_Output\n",
    "        results[\"________________________\"] = \"\"\n",
    "        results[\"General Procedure Pearson\"] =  Main_Procedure_Output\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
