{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import scipy.stats as st\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, iqr, median_abs_deviation, beta, norm, nct, t, f, ncf, chi2,  median_test, hmean, wilcoxon, hypergeom, rankdata, mannwhitneyu, binom, fisher_exact, barnard_exact, chi2_contingency\n",
    "from scipy.special import hyp2f1\n",
    "from numpy import around, array2string\n",
    "import numpy as np\n",
    "import numpy as geek\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import statistics\n",
    "import researchpy as rp\n",
    "import pingouin as pg\n",
    "from pingouin import cochran\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from collections import Counter\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "from scipy.optimize import newton, root_scalar\n",
    "from scipy.stats.contingency import odds_ratio, relative_risk\n",
    "import rpy2.robjects as robjects\n",
    "from statsmodels.stats.contingency_tables import mcnemar, cochrans_q\n",
    "from statsmodels.stats.proportion import proportion_confint, confint_proportions_2indep\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n",
    "#Function for counting negative and positive ranks\n",
    "def SumNegative(x):\n",
    "    sum = 0\n",
    "    for i in x:\n",
    "        if i < 0:\n",
    "            sum += i\n",
    "    return sum\n",
    "\n",
    "def SumPositive(x):\n",
    "    sum = 0\n",
    "    for i in x:\n",
    "        if i > 0:\n",
    "            sum += i\n",
    "    return sum\n",
    "\n",
    "def Wide_to_Long_2_coloumns(x,y):\n",
    "    sample_size_1 = np.empty(len(x), dtype = int)\n",
    "    sample_size_2 = np.empty(len(y), dtype = int)\n",
    "    sample_size_1.fill(1)\n",
    "    sample_size_2.fill(0)\n",
    "    y = np.append(x,y)\n",
    "    x = np.append(sample_size_1,sample_size_2)\n",
    "\n",
    "    return x,y\n",
    "\n",
    "\n",
    "# This is a function that converts Z_scores into pvalues\n",
    "def calculate_p_value_from_z_score(score):\n",
    "    p_value = st.t.sf((abs(score)), 100000) * 2\n",
    "    return min(float(p_value), 0.99999)\n",
    "\n",
    "# This is a function that converts t_scores into pvalues\n",
    "def calculate_p_value_from_t_score(t_score, df):\n",
    "    p_value = st.t.sf((abs(t_score)), df) * 2\n",
    "    return min(float(p_value), 0.99999)\n",
    "\n",
    "# This is a function that converts chi_square values into pvalues\n",
    "def calculate_p_value_from_chi_score(chi_score, df):\n",
    "    p_value = st.chi2.sf((abs(chi_score)), df)\n",
    "    return min(float(p_value), 0.99999)\n",
    "\n",
    "\n",
    "# This is a function that calculates the Confidence Intervals of the Effect size in One Sample Z_score test (or two dependent samples)\n",
    "#Note that since the effect size in the population and its standart deviation are unknown we can estimate it based on the sample. For the one sample case we will use the Hedges and Olkin 1985 Formula to estimate the standart deviation of the effect size\n",
    "def calculate_central_ci_from_cohens_d_one_sample(cohens_d, sample_size, confidence_level):\n",
    "    standard_error_es = np.sqrt((1 / sample_size) + ((cohens_d**2 / (2 * sample_size))))\n",
    "    z_critical_value = st.norm.ppf(confidence_level + ((1 - confidence_level) / 2))\n",
    "    ci_lower = cohens_d - standard_error_es * z_critical_value\n",
    "    ci_upper = cohens_d + standard_error_es * z_critical_value\n",
    "    return ci_lower, ci_upper, standard_error_es\n",
    "\n",
    "def calculate_central_ci_from_cohens_d_two_samples(cohens_d, sample_size_1, sample_size_2, confidence_level):\n",
    "    standard_error_es = np.sqrt(((sample_size_1 + sample_size_2)/(sample_size_1 * sample_size_2)) + ((cohens_d ** 2 / (2 * (sample_size_1 + sample_size_2)))))\n",
    "    z_critical_value = st.norm.ppf(confidence_level + ((1-confidence_level)/2))\n",
    "    ci_lower = cohens_d - standard_error_es * z_critical_value\n",
    "    ci_upper = cohens_d + standard_error_es * z_critical_value\n",
    "    return ci_lower, ci_upper, standard_error_es\n",
    "\n",
    "def calculate_cohens_d_from_z_score(z_score, sample_size):\n",
    "    return abs(z_score / np.sqrt(sample_size))\n",
    "\n",
    "#This is the Correction factor for Hedges g\n",
    "def correction_factor(df):\n",
    "    return math.exp(math.lgamma(df/2) - math.log(math.sqrt(df/2)) - math.lgamma((df-1)/2))\n",
    "\n",
    "\n",
    "#This is a function for the non-central pivotal CI's\n",
    "\n",
    "def pivotal_ci_t(t_Score, df, sample_size, confidence_level):\n",
    "    is_negative = False\n",
    "    \n",
    "    if t_Score < 0:\n",
    "        is_negative = True\n",
    "        t_Score = abs(t_Score)\n",
    "    \n",
    "    upper_limit = 1 - (1 - confidence_level) / 2\n",
    "    lower_limit = (1 - confidence_level) / 2\n",
    "\n",
    "    lower_criterion = [-t_Score, t_Score / 2, t_Score]\n",
    "    upper_criterion = [t_Score, 2 * t_Score, 3 * t_Score]\n",
    "\n",
    "    while nct.cdf(t_Score, df, lower_criterion[0]) < upper_limit:\n",
    "        lower_criterion = [lower_criterion[0] - t_Score, lower_criterion[0], lower_criterion[2]]\n",
    "    \n",
    "    while nct.cdf(t_Score, df, upper_criterion[0]) < lower_limit:\n",
    "        if nct.cdf(t_Score, df) < lower_limit:\n",
    "            lower_ci = [0, nct.cdf(t_Score, df)]\n",
    "            upper_criterion = [upper_criterion[0] / 4, upper_criterion[0], upper_criterion[2]]\n",
    "    \n",
    "    while nct.cdf(t_Score, df, upper_criterion[2]) > lower_limit:\n",
    "        upper_criterion = [upper_criterion[0], upper_criterion[2], upper_criterion[2] + t_Score]\n",
    "\n",
    "    # Finding lower limit for the ncp\n",
    "    lower_ci = 0.0\n",
    "    diff_lower = 1\n",
    "    while diff_lower > 0.00001:\n",
    "        if nct.cdf(t_Score, df, lower_criterion[1]) < upper_limit:\n",
    "            lower_criterion = [lower_criterion[0], (lower_criterion[0] + lower_criterion[1]) / 2, lower_criterion[1]]\n",
    "        else:\n",
    "            lower_criterion = [lower_criterion[1], (lower_criterion[1] + lower_criterion[2]) / 2, lower_criterion[2]]\n",
    "        diff_lower = abs(nct.cdf(t_Score, df, lower_criterion[1]) - upper_limit)\n",
    "        lower_ci = lower_criterion[1] / (np.sqrt(sample_size))\n",
    "\n",
    "    # Finding upper limit for the ncp\n",
    "    upper_ci = 0.0\n",
    "    diff_upper = 1\n",
    "    while diff_upper > 0.00001:\n",
    "        if nct.cdf(t_Score, df, upper_criterion[1]) < lower_limit:\n",
    "            upper_criterion = [upper_criterion[0], (upper_criterion[0] + upper_criterion[1]) / 2, upper_criterion[1]]\n",
    "        else:\n",
    "            upper_criterion = [upper_criterion[1], (upper_criterion[1] + upper_criterion[2]) / 2, upper_criterion[2]]\n",
    "        diff_upper = abs(nct.cdf(t_Score, df, upper_criterion[1]) - lower_limit)\n",
    "        upper_ci = upper_criterion[1] / (np.sqrt(sample_size))\n",
    "\n",
    "    if is_negative:\n",
    "        return -upper_ci, -lower_ci\n",
    "    else:\n",
    "        return lower_ci, upper_ci\n",
    "    \n",
    "\n",
    "## These Functions are for the Robust_Effect_Size\n",
    "def density(x):\n",
    "    x = np.array(x)\n",
    "    return x**2 * norm.pdf(x)\n",
    "\n",
    "def pbvar(x, beta=0.2):\n",
    "    w = np.abs(x - np.median(x))\n",
    "    omega = np.sort(w)[int(np.floor((1 - beta) * len(x)))]\n",
    "    y = (x - np.median(x)) / omega\n",
    "    z = np.minimum(1, np.maximum(-1, y))\n",
    "    pbvar = len(x) * omega**2 * np.sum(z**2) / len(x[np.abs(y) < 1])**2\n",
    "    return pbvar\n",
    "\n",
    "def area_under_function(f, a, b, *args, function_a=None, function_b=None, limit=10, eps=1e-5):\n",
    "    if function_a is None:\n",
    "        function_a = f(a, *args)\n",
    "    if function_b is None:\n",
    "        function_b = f(b, *args)\n",
    "    midpoint = (a + b) / 2\n",
    "    f_midpoint = f(midpoint, *args)\n",
    "    area_trapezoidal = ((function_a + function_b) * (b - a)) / 2\n",
    "    area_simpson = ((function_a + 4 * f_midpoint + function_b) * (b - a)) / 6\n",
    "    if abs(area_trapezoidal - area_simpson) < eps or limit == 0:\n",
    "        return area_simpson\n",
    "    return area_under_function(f, a, midpoint, *args, function_a=function_a, function_b=f_midpoint, limit=limit-1, eps=eps) + area_under_function(f, midpoint, b, *args, function_a=f_midpoint, function_b=function_b, limit=limit-1, eps=eps)\n",
    "\n",
    "def WinsorizedVariance(x, trimming_level=0.2):\n",
    "    y = np.sort(x)\n",
    "    n = len(x)\n",
    "    ibot = int(np.floor(trimming_level * n)) + 1\n",
    "    itop = n - ibot + 1\n",
    "    xbot = y[ibot-1] \n",
    "    xtop = y[itop-1]\n",
    "    y = np.where(y <= xbot, xbot, y)\n",
    "    y = np.where(y >= xtop, xtop, y)\n",
    "    winvar = np.std(y, ddof=1)**2\n",
    "    return winvar\n",
    "\n",
    "def WinsorizedCorrelation(x, y, trimming_level=0.2):\n",
    "    sample_size = len(x)\n",
    "    x_sorted = np.sort(x)\n",
    "    y_sorted = np.sort(y)\n",
    "    trimming_size = int(np.floor(trimming_level * sample_size)) + 1\n",
    "    x_lower = x_sorted[trimming_size - 1]\n",
    "    x_upper = x_sorted[sample_size - trimming_size]\n",
    "    y_lower = y_sorted[trimming_size - 1]\n",
    "    y_upper = y_sorted[sample_size - trimming_size]\n",
    "    x_winzsorized = np.clip(x, x_lower, x_upper)\n",
    "    y_winzsorized = np.clip(y, y_lower, y_upper)\n",
    "    winsorized_correlation = np.corrcoef(x_winzsorized, y_winzsorized)[0, 1]\n",
    "    winsorized_covariance = np.cov(x_winzsorized, y_winzsorized)[0, 1]\n",
    "    test_statistic = winsorized_correlation * np.sqrt((sample_size - 2) / (1 - winsorized_correlation**2))\n",
    "    Number_of_trimmed_values = int(np.floor(trimming_level * sample_size))\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(test_statistic), sample_size - 2*Number_of_trimmed_values - 2))\n",
    "    return {'cor': winsorized_correlation, 'cov': winsorized_covariance, 'p.value': p_value, 'n': sample_size, 'test_statistic': test_statistic}\n",
    "\n",
    "\n",
    "#Yuens_Test Tyuen statistic with the relvant effect size - note that for the independent variable a bootstrapping is used (with 100 repetitions) if n1 and n2 are not equal which might yield slightly different results\n",
    "#This function caculates the Yuen Statistics for the Trimmed Means\n",
    "def Yuens_Test_Trimmed_Means_Dependent(x, y, trimming_level=0.2, confidence_level = 0.95):\n",
    "    sample_size = len(x)\n",
    "    non_winsorized_sample_size = len(x) - 2 * np.floor(trimming_level * len(x))\n",
    "    trimmed_mean_1 = stats.trim_mean(x, trimming_level)\n",
    "    trimmed_mean_2 = stats.trim_mean(y, trimming_level)\n",
    "    sort_values = np.concatenate((x, y))\n",
    "    Variance_Trimmed_Means = (np.std(np.array([trimmed_mean_1, trimmed_mean_2]), ddof=1))**2\n",
    "    correction = area_under_function(density, norm.ppf(trimming_level), norm.ppf(1 - trimming_level)) + 2 * (norm.ppf(trimming_level)**2) * trimming_level\n",
    "    Winzorized_Variance = WinsorizedVariance(sort_values, trimming_level = trimming_level)\n",
    "    Explained_variance = Variance_Trimmed_Means/(Winzorized_Variance/correction)\n",
    "    Robust_Explonatory_Effect_Size = np.sqrt(Explained_variance)\n",
    "    if (Robust_Explonatory_Effect_Size > 1):\n",
    "        x0 = np.concatenate((np.ones(len(x)), np.ones(len(y)) * 2))\n",
    "        y0 = np.concatenate((x, y))\n",
    "        Robust_Explonatory_Effect_Size = np.sqrt(WinsorizedCorrelation(x0, y0, trimming_level = trimming_level)['cor']**2)\n",
    "    winsorized_variance_1 = WinsorizedVariance(x, trimming_level) \n",
    "    winsorized_variance_2 = WinsorizedVariance(y, trimming_level)\n",
    "    winsorized_correlation = WinsorizedCorrelation(x, y, trimming_level)['cor']\n",
    "    winsorized_correlation_pvalue = WinsorizedCorrelation(x, y, trimming_level)['p.value']\n",
    "    q1 = (len(x) - 1) * winsorized_variance_1\n",
    "    q2 = (len(y) - 1) * winsorized_variance_2\n",
    "    q3 = (len(x) - 1) * WinsorizedCorrelation(x, y, trimming_level)['cov']\n",
    "    df = non_winsorized_sample_size - 1\n",
    "    Yuen_Standrd_Error = np.sqrt((q1 + q2 - 2 * q3) / (non_winsorized_sample_size * (non_winsorized_sample_size - 1)))    \n",
    "    difference_trimmed_means = trimmed_mean_1 - trimmed_mean_2\n",
    "    Yuen_Statistic = difference_trimmed_means / Yuen_Standrd_Error\n",
    "    Yuen_p_value = 2 * (1 - t.cdf(np.abs(Yuen_Statistic), df))\n",
    "    return np.array([trimmed_mean_1, trimmed_mean_2, winsorized_variance_1, winsorized_variance_2, sample_size, Yuen_Statistic, df, Yuen_Standrd_Error, Yuen_p_value, \\\n",
    "           difference_trimmed_means, winsorized_correlation, winsorized_correlation_pvalue, Explained_variance, Robust_Explonatory_Effect_Size])\n",
    "\n",
    "#Build CI for the yuens statistics using the Akp 2005 method with using the yuens and its df's and multiplying by the factor \n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import t, norm\n",
    "\n",
    "def Yuens_Test_Trimmed_Means_Independent(x, y, trimming_level=0.2, confidence_level=0.95, PB=False):\n",
    "    sample_size_1 = len(x)\n",
    "    sample_size_2 = len(y)\n",
    "    h1 = sample_size_1 - 2 * np.floor(trimming_level * sample_size_1)\n",
    "    h2 = sample_size_2 - 2 * np.floor(trimming_level * sample_size_2)\n",
    "    trimmed_mean_1 = stats.trim_mean(x, trimming_level)\n",
    "    trimmed_mean_2 = stats.trim_mean(y, trimming_level)\n",
    "    difference_trimmed_means = trimmed_mean_1 - trimmed_mean_2\n",
    "    winsorized_variance_1 = WinsorizedVariance(x, trimming_level)\n",
    "    winsorized_variance_2 = WinsorizedVariance(y, trimming_level)\n",
    "    correction = area_under_function(density, norm.ppf(trimming_level), norm.ppf(1 - trimming_level)) + 2 * (norm.ppf(trimming_level)**2) * trimming_level\n",
    "    q1 = (sample_size_1 - 1) * winsorized_variance_1 / (h1 * (h1 - 1))\n",
    "    q2 = (sample_size_2 - 1) * winsorized_variance_2 / (h2 * (h2 - 1))\n",
    "    df = (q1 + q2)**2 / ((q1**2 / (h1 - 1)) + (q2**2 / (h2 - 1)))\n",
    "    Yuen_Standard_Error = np.sqrt(q1 + q2)\n",
    "    Yuen_Statistic = np.abs(difference_trimmed_means / Yuen_Standard_Error)\n",
    "    Yuen_p_value = 2 * (1 - t.cdf(abs(Yuen_Statistic), df))\n",
    "    sort_values = np.concatenate((x, y))\n",
    "    Variance_Trimmed_Means = np.var(np.concatenate(([trimmed_mean_1], [trimmed_mean_2])))\n",
    "    Winzorized_Variance = WinsorizedVariance(sort_values, trimming_level=trimming_level) / correction if not PB else pbvar(sort_values) / 1.06\n",
    "    \n",
    "    # Check if winsorized variance is equal to 1 and set explanatory robust effect size and explained variance to 0\n",
    "    if Winzorized_Variance == 0:\n",
    "        Explained_Variance = 0\n",
    "        Robust_Explonatory_Effect_Size = 0\n",
    "    else:\n",
    "        Explained_Variance = Variance_Trimmed_Means / Winzorized_Variance\n",
    "        Robust_Explonatory_Effect_Size = np.sqrt(Explained_Variance)\n",
    "\n",
    "    standardizer_Robust_Cohens_d = np.sqrt(((sample_size_1 - 1) * winsorized_variance_1 + (sample_size_1 - 2) * winsorized_variance_2) / (sample_size_1 + sample_size_2 - 2))\n",
    "    Robust_Cohens_d = np.sqrt(correction) * difference_trimmed_means / standardizer_Robust_Cohens_d\n",
    "    Robust_Cohens_d_corrected_1 = np.sqrt(correction) * difference_trimmed_means / np.sqrt(winsorized_variance_1)\n",
    "    Robust_Cohens_d_corrected_2 = np.sqrt(correction) * difference_trimmed_means / np.sqrt(winsorized_variance_2)\n",
    "    \n",
    "    if Explained_Variance > 1:\n",
    "        x0 = np.concatenate((np.repeat(1, sample_size_1), np.repeat(2, sample_size_2)))\n",
    "        y0 = np.concatenate((x, y))\n",
    "        Explained_Variance = WinsorizedCorrelation(x0, y0, trimming_level=trimming_level)['cor']**2\n",
    "        Robust_Explonatory_Effect_Size = np.sqrt(Explained_Variance)\n",
    "    \n",
    "    if sample_size_1 != sample_size_2:\n",
    "        N = min(sample_size_1, sample_size_2)\n",
    "        vals = np.zeros(100)\n",
    "        for i in range(100):\n",
    "            sample_x = np.random.choice(x, size=N, replace=False)\n",
    "            sample_y = np.random.choice(y, size=N, replace=False) \n",
    "            m1_sample = stats.trim_mean(sample_x, trimming_level)\n",
    "            m2_sample = stats.trim_mean(sample_y, trimming_level)\n",
    "            Variance_Trimmed_Means = np.var(np.concatenate(([m1_sample], [m2_sample])), ddof=1) if sample_y is not None else np.var([m1_sample])\n",
    "            correction = area_under_function(density, norm.ppf(trimming_level), norm.ppf(1 - trimming_level)) + 2 * (norm.ppf(trimming_level)**2) * trimming_level\n",
    "            Winzorized_Variance = WinsorizedVariance(np.concatenate((sample_x, sample_y)), trimming_level=trimming_level) / correction if not PB else pbvar(np.concatenate((sample_x, sample_y))) / 1.06\n",
    "            vals[i] = Variance_Trimmed_Means / Winzorized_Variance\n",
    "        Explained_Variance = np.median(vals)\n",
    "        Robust_Explonatory_Effect_Size = np.sqrt(Explained_Variance)\n",
    "\n",
    "    return np.array([trimmed_mean_1, trimmed_mean_2, winsorized_variance_1, winsorized_variance_2, sample_size_1, sample_size_2, Yuen_Statistic, df, Yuen_Standard_Error, Yuen_p_value, difference_trimmed_means, Explained_Variance, Robust_Explonatory_Effect_Size, Robust_Cohens_d_corrected_2, Robust_Cohens_d_corrected_1, Robust_Cohens_d, standardizer_Robust_Cohens_d, h1, h2, correction])\n",
    "\n",
    "#Add a note as if one of the winsorized samples has no variabilty - the explanotry robust effect is equal to zero in such a case\n",
    "#Also dont forget to add the robust correlation for unequal variances based on sample size of one group (seperate effect sizes)\n",
    "#Add bootstrapping\n",
    "\n",
    "def sign_test_wilcoxon_method (x,y,confidence_level=0.95, population_median=0): #this is the original method by wilcoxon that ignores ties\n",
    "    if y is None:\n",
    "        difference = x - population_median\n",
    "    else:\n",
    "        difference = x - y\n",
    "    sample_size = len(x)\n",
    "    sign = np.where(difference < 0, -1, 1)\n",
    "    difference = difference[difference != 0]\n",
    "    difference_abs = np.abs(difference)\n",
    "    total_n = difference.shape[0]\n",
    "    positive_n = difference[difference > 0].shape[0]\n",
    "    negative_n = difference[difference < 0].shape[0]\n",
    "    zero_n = difference[difference == 0].shape[0]\n",
    "    ranked = scipy.stats.rankdata(difference_abs)\n",
    "    sign = np.where(difference < 0, -1, 1)\n",
    "    ranked_sign = (sign * ranked)\n",
    "    total_sum_ranks = ranked.sum()\n",
    "    positive_sum_ranks = ranked[difference > 0].sum()\n",
    "    negative_sum_ranks = ranked[difference < 0].sum()\n",
    "    mean_positive_ranks = positive_sum_ranks / positive_n\n",
    "    mean_negative_ranks = negative_sum_ranks / negative_n\n",
    "    zero_sum_ranks = ranked[difference == 0].sum()\n",
    "    sign2 = np.where(difference == 0, 0, sign)\n",
    "    ranked2 = sign2 * ranked\n",
    "    ranked2 = np.where(difference == 0, 0, ranked2)\n",
    "    var_adj_T = (ranked2 * ranked2).sum()\n",
    "    Adjusted_Var = (1/4) * var_adj_T\n",
    "    Unadjusted_Var = ((total_n * (total_n + 1)) * (2 * total_n + 1)) /24\n",
    "    var_zero_adj_T_pos = -1 * ((zero_n * (zero_n + 1)) * (2 * zero_n + 1)) /24\n",
    "    var_ties_adj = Adjusted_Var - Unadjusted_Var - var_zero_adj_T_pos\n",
    "    MeanW = ((positive_n + negative_n) * (positive_n + negative_n + 1)) / 4\n",
    "    RBCwilcoxon = (positive_sum_ranks - negative_sum_ranks) / (positive_sum_ranks + negative_sum_ranks) #This is the match paired rank biserial correlation (Kerby, 2014)\n",
    "\n",
    "    #Calculate The Z score wilcox\n",
    "    Z_adjusted = (positive_sum_ranks - MeanW) /  np.sqrt(Adjusted_Var)\n",
    "    Z_Unadjusted = (positive_sum_ranks - MeanW) /  np.sqrt(Unadjusted_Var)\n",
    "    Z_adjustedCorrected = (positive_sum_ranks - MeanW - 0.5 * np.sign(positive_sum_ranks - MeanW)) /  np.sqrt(Adjusted_Var)\n",
    "    Z_Unadjusted_Corrected = (positive_sum_ranks - MeanW - 0.5 * np.sign(positive_sum_ranks - MeanW)) /  np.sqrt(Unadjusted_Var)\n",
    "    p_value_adjusted = norm.cdf(Z_adjusted)*2\n",
    "    p_value_unadjusted = norm.cdf(Z_Unadjusted)*2\n",
    "    p_value_adjusted_corrected = norm.cdf(Z_adjustedCorrected)*2\n",
    "    p_value_unadjusted_corrected = norm.cdf(Z_Unadjusted_Corrected)*2\n",
    "    RBC_adjusted = Z_adjusted / np.sqrt(sample_size)\n",
    "    RBC_unadjusted = Z_Unadjusted / np.sqrt(sample_size)\n",
    "    RBC_unajusted_corrected = Z_adjustedCorrected / np.sqrt(sample_size)\n",
    "    RBC_adjusted_corrected = Z_Unadjusted_Corrected / np.sqrt(sample_size)\n",
    "\n",
    "    return np.array([positive_n, negative_n, total_n, positive_sum_ranks, negative_sum_ranks, total_sum_ranks, \\\n",
    "                     mean_positive_ranks,  mean_negative_ranks, MeanW, Adjusted_Var, Unadjusted_Var, RBCwilcoxon, \\\n",
    "                     Z_adjusted, Z_Unadjusted, Z_adjustedCorrected, Z_Unadjusted_Corrected, \\\n",
    "                     p_value_adjusted, p_value_unadjusted, p_value_adjusted_corrected, p_value_unadjusted_corrected, \\\n",
    "                     RBC_adjusted, RBC_unadjusted, RBC_unajusted_corrected, RBC_adjusted_corrected])\n",
    "\n",
    "def sign_test_pratt_method (x,y, confidence_level): #this is the original method by pratt that consider ties in the data\n",
    "    difference = x-y\n",
    "    sample_size = len(difference)\n",
    "    sign = np.where(difference < 0, -1, 1)\n",
    "    difference_abs = np.abs(difference)\n",
    "    total_n = difference.shape[0]\n",
    "    positive_n = difference[difference > 0].shape[0]\n",
    "    negative_n = difference[difference < 0].shape[0]\n",
    "    zero_n = difference[difference == 0].shape[0]\n",
    "    ranked = scipy.stats.rankdata(difference_abs)\n",
    "    sign = np.where(difference < 0, -1, 1)\n",
    "    ranked_sign = (sign * ranked)\n",
    "    total_sum_ranks = ranked.sum()\n",
    "    positive_sum_ranks = ranked[difference > 0].sum()\n",
    "    negative_sum_ranks = ranked[difference < 0].sum()\n",
    "    mean_positive_ranks = positive_sum_ranks / positive_n\n",
    "    mean_negative_ranks = negative_sum_ranks / negative_n\n",
    "    zero_sum_ranks = ranked[difference == 0].sum()\n",
    "    sign2 = np.where(difference == 0, 0, sign)\n",
    "    ranked2 = sign2 * ranked\n",
    "    ranked2 = np.where(difference == 0, 0, ranked2)\n",
    "    var_adj_T = (ranked2 * ranked2).sum()\n",
    "    Adjusted_Var = (1/4) * var_adj_T\n",
    "    Unadjusted_Var = ((total_n * (total_n + 1)) * (2 * total_n + 1)) /24\n",
    "    var_zero_adj_T_pos = -1 * ((zero_n * (zero_n + 1)) * (2 * zero_n + 1)) /24\n",
    "    var_ties_adj = Adjusted_Var - Unadjusted_Var - var_zero_adj_T_pos\n",
    "    MeanW = (positive_sum_ranks+negative_sum_ranks)/2\n",
    "    RBCpratt = (positive_sum_ranks - negative_sum_ranks) / (positive_sum_ranks + negative_sum_ranks + zero_sum_ranks) #This is the matched pairs rank biserial correlation\n",
    "\n",
    "\n",
    "    #Calculate The Z score pratt\n",
    "    Z_adjusted = (positive_sum_ranks - MeanW) /  np.sqrt(Adjusted_Var)\n",
    "    Z_Unadjusted = (positive_sum_ranks - MeanW) /  np.sqrt(Unadjusted_Var)\n",
    "    Z_adjusted_Corrected = (positive_sum_ranks - MeanW - 0.5 * np.sign(positive_sum_ranks - MeanW)) /  np.sqrt(Adjusted_Var)\n",
    "    Z_Unadjusted_Corrected = (positive_sum_ranks - MeanW - 0.5 * np.sign(positive_sum_ranks - MeanW)) /  np.sqrt(Unadjusted_Var)\n",
    "    p_value_adjusted = norm.cdf(Z_adjusted)*2\n",
    "    p_value_unadjusted = norm.cdf(Z_Unadjusted)*2\n",
    "    p_value_adjusted_corrected = norm.cdf(Z_adjusted_Corrected)*2\n",
    "    p_value_unadjusted_corrected = norm.cdf(Z_Unadjusted_Corrected)*2\n",
    "    RBC_adjusted = Z_adjusted / np.sqrt(sample_size)\n",
    "    RBC_unadjusted = Z_Unadjusted / np.sqrt(sample_size)\n",
    "    RBC_unajusted_corrected = Z_Unadjusted_Corrected / np.sqrt(sample_size)\n",
    "    RBC_adjusted_corrected = Z_adjusted_Corrected / np.sqrt(sample_size)\n",
    "\n",
    "    return np.array([positive_n, negative_n, total_n, positive_sum_ranks, negative_sum_ranks, total_sum_ranks, \\\n",
    "                     mean_positive_ranks,  mean_negative_ranks, MeanW, Adjusted_Var, Unadjusted_Var, RBCpratt, \\\n",
    "                     Z_adjusted, Z_Unadjusted, Z_adjusted_Corrected, Z_Unadjusted_Corrected, \\\n",
    "                     p_value_adjusted, p_value_unadjusted, p_value_adjusted_corrected, p_value_unadjusted_corrected, \\\n",
    "                     RBC_adjusted, RBC_unadjusted, RBC_unajusted_corrected, RBC_adjusted_corrected])\n",
    "\n",
    "def Common_Language_Effect_Sizes_Dependent(x,y,confidence_level):\n",
    "    difference = (x- y)\n",
    "    sample_size = len(x)\n",
    "    mean_difference = np.mean(difference)\n",
    "    cohens_dz = mean_difference/np.std(difference, ddof = 1)\n",
    "    \n",
    "    #Mcgraw and Wang common language Effect size (calculate dz)\n",
    "    cles = norm.cdf(cohens_dz) * 100\n",
    "\n",
    "    #Probabilty of superiority PSdep\n",
    "    superiority_counts = np.where(x > y, 1, np.where(x < y, 0, 0.5)) #This line  gives 1 for superior values, 0 to inferior values and 0.5 to ties. \n",
    "    totalp = np.sum(np.where(superiority_counts == 0.5, 0, superiority_counts))\n",
    "    PSdep = totalp/sample_size\n",
    "    \n",
    "    #Vargha and Delany Axy, 2000 (the dependent groups version)\n",
    "    VDA_xy = sum(superiority_counts)/sample_size\n",
    "\n",
    "    #Cliffs Delta dependent version (Feng ordinal delta, 2007)\n",
    "    cliffs_delta = (sum(np.where(x>y,1,0)) - sum(np.where(y>x,1,0)))/sample_size\n",
    "\n",
    "    return np.array([PSdep, cles, VDA_xy, cliffs_delta])\n",
    "\n",
    "\n",
    "\n",
    "#######CI's for proportions depndent \n",
    "\n",
    "########### These functions are for the schilon_doi CI's\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "\n",
    "def PSdep_dependent_CI(x,y,confidence_level):\n",
    "    sample_size = len(x)\n",
    "    difference = x - y\n",
    "    superiority_counts = np.where(x > y, 1, np.where(x < y, 0, 0.5)) #This line  gives 1 for superior values, 0 to inferior values and 0.5 to ties. \n",
    "    totalp = np.sum(np.where(superiority_counts == 0.5, 0, superiority_counts))\n",
    "\n",
    "###Agrest_Cull CI's\n",
    "    p = totalp/sample_size\n",
    "    q = 1 - p\n",
    "    n_tag = sample_size + norm.ppf(1 - confidence_level)**2\n",
    "    w_tag = totalp + norm.ppf(1 - confidence_level)**2 / 2\n",
    "    ptag = w_tag/n_tag\n",
    "    qtag = 1 - ptag\n",
    "    agresti_cull_upper = ptag + norm.ppf(1 - confidence_level) *(np.sqrt((ptag*qtag)/n_tag))\n",
    "    agresti_cull_lower = ptag - norm.ppf(1 - confidence_level) *(np.sqrt((ptag*qtag)/n_tag))\n",
    "    AgrestiCull_lower = agresti_cull_lower\n",
    "    AgrestiCull_upper = agresti_cull_upper\n",
    "\n",
    "    if AgrestiCull_lower < 0:\n",
    "        AgrestiCull_lower = 0\n",
    "    if AgrestiCull_upper > 1:\n",
    "        AgrestiCull_upper = 1\n",
    "\n",
    "    ###Wilson_CI's\n",
    "    ctagsquare = norm.ppf(1 - confidence_level)**2\n",
    "    ctag = norm.ppf(1 - confidence_level)\n",
    "    element1 = ctag*np.sqrt(((totalp*(sample_size-totalp))/sample_size**3)+ctagsquare/(4*sample_size**2))\n",
    "    lowerwilson = (sample_size/(sample_size+ctagsquare)) *(p+ctagsquare/(2*sample_size) - element1)\n",
    "    upperwilson = (sample_size/(sample_size+ctagsquare)) *(p+ctagsquare/(2*sample_size) + element1)\n",
    "    \n",
    "    if lowerwilson < 0:\n",
    "        lowerwilson = 0\n",
    "    if upperwilson > 1:\n",
    "        upperwilson = 1\n",
    "\n",
    "    ###Wald_CI's\n",
    "    term1 = ctag * np.sqrt(p*q)/np.sqrt(sample_size)\n",
    "    lowerwald = p - term1\n",
    "    upperwald = p + term1\n",
    "    \n",
    "    if lowerwald < 0:\n",
    "        lowerwald = 0\n",
    "    if upperwald > 1:\n",
    "        upperwald = 1\n",
    "\n",
    "    ###jeffreys CI's\n",
    "    lowerjeffreys = beta.ppf(confidence_level, totalp+0.5, sample_size-totalp+0.5)\n",
    "    upperjeffreys = beta.ppf(1-confidence_level, totalp+0.5, sample_size-totalp+0.5)\n",
    "\n",
    "    if lowerjeffreys < 0:\n",
    "        lowerjeffreys = 0\n",
    "    if upperjeffreys > 1:\n",
    "        upperjeffreys = 1\n",
    "\n",
    "###clopper-pearson CI's\n",
    "    lowerCP = beta.ppf(confidence_level, totalp, sample_size-totalp+1)\n",
    "    upperCP= beta.ppf(1-confidence_level, totalp+1, sample_size-totalp)\n",
    "\n",
    "    if lowerCP < 0:\n",
    "        lowerCP = 0\n",
    "    if upperCP > 1:\n",
    "        upperCP = 1\n",
    "\n",
    "###arcsine CI's 1 (Kulynskaya)\n",
    "    ptilde = (totalp + 0.375)/(sample_size + 0.75)\n",
    "    arcsinelower = math.sin(math.asin(np.sqrt(ptilde)) - 0.5*ctag/np.sqrt(sample_size))**2\n",
    "    arcsineupper = math.sin(math.asin(np.sqrt(ptilde)) + 0.5*ctag/np.sqrt(sample_size))**2\n",
    "\n",
    "    if arcsinelower < 0:\n",
    "        arcsinelower = 0\n",
    "    if arcsineupper > 1:\n",
    "        arcsineupper = 1\n",
    "\n",
    "    ### logit CI's\n",
    "    lambdahat = math.log(totalp/(sample_size-totalp))\n",
    "    term1 = sample_size/(totalp*(sample_size-totalp))\n",
    "    lambdalow = lambdahat - ctag*np.sqrt(term1)\n",
    "    lambdaupper = lambdahat + ctag*np.sqrt(term1)\n",
    "    logitlower = math.exp(lambdalow)/(1 + math.exp(lambdalow))\n",
    "    logitupper = math.exp(lambdaupper)/(1 + math.exp(lambdaupper))\n",
    "\n",
    "    if logitupper > 1:\n",
    "        logitupper = 1\n",
    "    \n",
    "    if totalp == 0:\n",
    "        logitlower = \"**Error**\"\n",
    "        logitupper = \"Cant calculte logit CI's when p is equal to zero\"\n",
    "    \n",
    "    \n",
    "    ### Pratt CI's\n",
    "    A = ((totalp + 1)/(sample_size-totalp))**2\n",
    "    B = 81 * (totalp+1) * (sample_size-totalp) - 9*sample_size - 8\n",
    "    C = -3 * ctag * np.sqrt(9*(totalp+1)*(sample_size - totalp) * (9*sample_size + 5 - ctagsquare) + sample_size + 1)\n",
    "    D = 81 * (totalp + 1)**2 - 9 *(totalp+1)* (2+ctagsquare) + 1\n",
    "    E = 1 + A * ((B+C)/D)**3\n",
    "    A2 = (totalp/ (sample_size-totalp-1)) **2\n",
    "    B2 = 81 * (totalp) * (sample_size-totalp-1) - 9*sample_size - 8\n",
    "    C2 = 3 * ctag * np.sqrt(9*totalp*(sample_size-totalp-1) * (9*sample_size + 5 - ctagsquare) + sample_size + 1)\n",
    "    D2 = 81 * totalp**2 - (9 *totalp) * (2+ctagsquare) + 1\n",
    "    E2 = 1 + A2 * ((B2+C2)/D2)**3\n",
    "\n",
    "    upperPratt = 1/E    \n",
    "    lowerPratt = 1/E2\n",
    "\n",
    "    if totalp == 1:\n",
    "        lowerPratt = 1 - (1-confidence_level)**(1/sample_size)\n",
    "        upperPratt = 1 - (confidence_level)**(1/sample_size)\n",
    "\n",
    "    if totalp == 0:\n",
    "        lowerpratt = 0\n",
    "        upperPratt = beta.ppf(1-confidence_level, totalp+1, sample_size-totalp)\n",
    "\n",
    "    if totalp == sample_size-1:\n",
    "        lowerpratt = (confidence_level)**(1/sample_size)\n",
    "        upperPratt = (1-confidence_level)**(1/sample_size)\n",
    "    \n",
    "    if totalp == sample_size:\n",
    "        lowerpratt = (confidence_level*2)**(1/sample_size)\n",
    "        upperPratt = 1\n",
    "\n",
    "    if lowerPratt < 0:\n",
    "        lowerPratt = 0\n",
    "    if upperPratt > 1:\n",
    "        upperPratt = 1\n",
    "    \n",
    "\n",
    "#Goodman-Kruskal Gamma function\n",
    "def goodman(x, y):\n",
    "    x,y =Wide_to_Long_2_coloumns(x,y)\n",
    "    Rx = np.sign(np.subtract.outer(x, x))\n",
    "    Ry = np.sign(np.subtract.outer(y, y))\n",
    "    S1 = np.multiply(Rx, Ry)\n",
    "    return np.sum(S1) / np.sum(np.abs(S1))\n",
    "\n",
    "\n",
    "def CLES_Aparametric_Independent_Samples (x,y,confidence_level, bootstrap=False):\n",
    "    sample_size_1 = len(x)\n",
    "    sample_size_2 = len(y)\n",
    "    sample_size = sample_size_1 + sample_size_2\n",
    "    Number_of_comparisons_between_x_and_y = sample_size_1 * sample_size_2\n",
    "    Matrix = np.subtract.outer(x, y)\n",
    "    Poistive_Compariosons = Matrix > 0\n",
    "    nonties_comparisons = Matrix != 0 \n",
    "    Superioirty_num = np.count_nonzero(Poistive_Compariosons) ## Proportions of cases group1 is larger\n",
    "    Number_Of_Ties = (Number_of_comparisons_between_x_and_y - np.count_nonzero(nonties_comparisons)) # Amount of ties in the data\n",
    "    proportion_of_ties = Number_Of_Ties / Number_of_comparisons_between_x_and_y\n",
    "    Inferioirtiy_num = Number_of_comparisons_between_x_and_y - Superioirty_num - Number_Of_Ties # Proportions of cases group2 is larger\n",
    "\n",
    "    # Applying the Haldane-Anscombe correction only if bootstrap is True and Inferioirtiy_num is zero\n",
    "    if bootstrap and Inferioirtiy_num == 0:\n",
    "        Inferioirtiy_num = 0.05\n",
    "\n",
    "    # Applying the Haldane-Anscombe correction only if bootstrap is True and Superioirty_num is zero\n",
    "    if bootstrap and Superioirty_num == 0:\n",
    "        Superioirty_num = 0.05\n",
    "    \n",
    "    ##Calculating the effect sizes of POS = Probability_Of_Superiority\n",
    "    POS_Grissom_Kim = Superioirty_num / Number_of_comparisons_between_x_and_y\n",
    "    POS_Vargha_Delaney = (Superioirty_num + Number_Of_Ties *0.5) / Number_of_comparisons_between_x_and_y\n",
    "    POS_Gamma_Based_Metsamuuronen = 1 - goodman(x,y) * 0.5 + 0.5\n",
    "\n",
    "    ##Calculating the odds ratio effect sizes\n",
    "    WMW_agresti = Superioirty_num/Inferioirtiy_num\n",
    "\n",
    "    #Add anotification of the meanings - text that will describe the meaning\n",
    "    WMW_Churilov = ((Superioirty_num  / Number_of_comparisons_between_x_and_y) + 0.5 * proportion_of_ties) /((Inferioirtiy_num / Number_of_comparisons_between_x_and_y) + 0.5 * proportion_of_ties)\n",
    "\n",
    "    #Cliffs delta is a transformation of the probabilty of superiority:\n",
    "    cliffs_delta = 1 - 2 * POS_Grissom_Kim\n",
    "\n",
    "    if bootstrap: #i need to return that as an array for the bootstrap\n",
    "        return POS_Grissom_Kim, POS_Vargha_Delaney, POS_Gamma_Based_Metsamuuronen, WMW_agresti, WMW_Churilov, cliffs_delta\n",
    "\n",
    "    else: \n",
    "    \n",
    "        # Central Confidence Intervals\n",
    "        ##############################\n",
    "        #The t_value_for the CI's\n",
    "        Critical_t_value = scipy.stats.t.ppf(1 - ((1-confidence_level)/2) , sample_size-2)\n",
    "\n",
    "        #1. Probabilty of Superiority\n",
    "        #A. Ruscio CI's:\n",
    "        Standard_Error_Ruscio = np.sqrt((1/ sample_size_1+ 1/sample_size_2 +1/(sample_size_1*sample_size_2))/12)\n",
    "\n",
    "        # Grissom & Kim PS CI\n",
    "        ci_lower_Grissom_Kim = POS_Grissom_Kim - (Standard_Error_Ruscio * Critical_t_value)\n",
    "        ci_upper_Grissom_Kim = POS_Grissom_Kim + (Standard_Error_Ruscio * Critical_t_value)\n",
    "        ci_lower_central_Grissom_Kim = max(ci_lower_Grissom_Kim, 0)\n",
    "        ci_upper_central_Grissom_Kim = min(ci_upper_Grissom_Kim, 1)\n",
    "\n",
    "        # Probability of Superiority CI (Vargha-Delaney)\n",
    "        ci_lower_Vargha_Delaney = POS_Vargha_Delaney - (Standard_Error_Ruscio * Critical_t_value)\n",
    "        ci_upper_Vargha_Delaney = POS_Vargha_Delaney + (Standard_Error_Ruscio * Critical_t_value)\n",
    "        ci_lower_central_Vargha_Delaney = max(ci_lower_Vargha_Delaney, 0)\n",
    "        ci_upper_central_Vargha_Delaney = min(ci_upper_Vargha_Delaney, 1)\n",
    "\n",
    "        # PS Gamma Rubia CI\n",
    "        ci_lower_Gamma = POS_Gamma_Based_Metsamuuronen - (Standard_Error_Ruscio * Critical_t_value)\n",
    "        ci_upper_Gamma = POS_Gamma_Based_Metsamuuronen + (Standard_Error_Ruscio * Critical_t_value)\n",
    "        ci_lower_central_Gamma = max(ci_lower_Gamma, 0)\n",
    "        ci_upper_central_Gamma = min(ci_upper_Gamma, 1)\n",
    "\n",
    "        #B Metsammuuronen/Rubia\n",
    "        gamma = goodman(x,y)\n",
    "\n",
    "        sample_size_1 = len(x)\n",
    "        sample_size_2 = len(y)\n",
    "        sample_size = sample_size_1 + sample_size_2\n",
    "\n",
    "        x_combined = np.append(x, y)\n",
    "        freq = Counter(x_combined)\n",
    "        frequencies = list(freq.values())\n",
    "        tiescorrection = [(f ** 3 - f) for f in frequencies]\n",
    "        multiplicity_factor = sum(tiescorrection)\n",
    "\n",
    "        data_dict = {'X': x, 'Y': y}\n",
    "        group_freq = {group: Counter(data_dict[group]) for group in data_dict.keys()}\n",
    "        group_x_df = pd.DataFrame.from_dict(group_freq['X'], orient='index', columns=['Group1'])\n",
    "        group_y_df = pd.DataFrame.from_dict(group_freq['Y'], orient='index', columns=['Group2'])\n",
    "        cont_table = pd.concat([group_x_df, group_y_df], axis=1).fillna(0)\n",
    "\n",
    "        data_long = Wide_to_Long_2_coloumns(x,y)\n",
    "        data_ASE = pd.DataFrame({'Score': data_long[1], 'Group': data_long[0]})\n",
    "        data_ASE['nj'] = data_ASE.groupby('Score')['Score'].transform('count')\n",
    "        data_ASE['ni'] = data_ASE.groupby('Group')['Group'].transform('count')\n",
    "        count_x_greater = [sum([y_val < x_val for y_val in y]) for x_val in x]\n",
    "        count_y_greater = [sum([x_val < y_val for x_val in x]) for y_val in y]\n",
    "        count_x_smaller = [sum([y_val > x_val for y_val in y]) for x_val in x]\n",
    "        count_y_smaller = [sum([x_val > y_val for x_val in x]) for y_val in y]\n",
    "\n",
    "        data_ASE['Concordant'] = count_x_greater + count_y_greater\n",
    "        data_ASE['Disoncordant'] = count_x_smaller + count_y_smaller\n",
    "\n",
    "        group_0_rows = data_ASE.loc[data_ASE['Group'] == 0, ['Concordant', 'Disoncordant']]\n",
    "        group_0_rows[['Concordant', 'Disoncordant']] = group_0_rows[['Disoncordant', 'Concordant']].values\n",
    "        data_ASE.loc[data_ASE['Group'] == 0, ['Concordant', 'Disoncordant']] = group_0_rows\n",
    "\n",
    "        data_ASE['C-D'] = data_ASE['Concordant'] - data_ASE['Disoncordant']\n",
    "        Q_concordant = sum(data_ASE['Disoncordant'])\n",
    "        P_disconcordant = sum(data_ASE['Concordant'])\n",
    "\n",
    "        ni_squared = sum(cont_table.sum(axis=1) ** 2)\n",
    "        nj_squared = sum(cont_table.sum(axis=0) ** 2)\n",
    "        Dx = sample_size ** 2 - ni_squared\n",
    "        Dg = sample_size ** 2 - nj_squared\n",
    "\n",
    "        data_ASE['N-nj'] = sample_size - data_ASE['nj']\n",
    "        data_ASE['N-ni'] = sample_size - data_ASE['ni']\n",
    "        data_ASE['ASE1formula'] = ((Dx * data_ASE['C-D']) - (data_ASE['N-nj'] * (P_disconcordant - Q_concordant))) ** 2\n",
    "        data_ASE['ASE2formula'] = ((Dg * data_ASE['C-D']) - (data_ASE['N-ni'] * (P_disconcordant - Q_concordant))) ** 2\n",
    "        data_ASE['C*Q - P*D'] = (data_ASE['Concordant'] * Q_concordant - data_ASE['Disoncordant'] * P_disconcordant) ** 2\n",
    "\n",
    "        #Standart Errors of the Gamma and Delta correlations\n",
    "        Standart_Error_Gamma_Metsamuuronen = ((2 / (Q_concordant + P_disconcordant))) * (np.sqrt((sum(data_ASE['C-D'] ** 2)) - ((1 / sample_size) * ((P_disconcordant - Q_concordant) ** 2))))\n",
    "        Standart_Error_Delta_Metsamuuronen = ((2 / Dx)) * (np.sqrt((sum(data_ASE['C-D'] ** 2)) - ((1 / sample_size) * ((P_disconcordant - Q_concordant) ** 2))))\n",
    "\n",
    "        #Extra - Standart Error for the H0 Hypothesis\n",
    "        Standard_Error_1_Delta_H0 = (2 / Dx ** 2) * np.sqrt(sum(data_ASE['ASE1formula']))\n",
    "        Standard_Error_2_Delta_H0 = ((2 / Dg)) * (np.sqrt((sum(data_ASE['C-D'] ** 2)) - ((1 / sample_size) * ((P_disconcordant - Q_concordant) ** 2))))\n",
    "        Standart_Error_Gamma_H0 = (4 / (Q_concordant + P_disconcordant) ** 2) * (np.sqrt(sum(data_ASE['C*Q - P*D'])))\n",
    "\n",
    "        # PS Gamma Metsamuuronen CI\n",
    "        ci_lower_Gamma_Metsamuuronen = POS_Gamma_Based_Metsamuuronen - ((Standart_Error_Gamma_Metsamuuronen*0.5+0.5) * Critical_t_value)\n",
    "        ci_upper_Gamma_Metsamuuronen = POS_Gamma_Based_Metsamuuronen + ((Standart_Error_Gamma_Metsamuuronen*0.5+0.5) * Critical_t_value)\n",
    "        ci_lower_Gamma_Metsamuuronen = max(ci_lower_Gamma_Metsamuuronen, 0)\n",
    "        ci_upper_Gamma_Metsamuuronen = min(ci_upper_Gamma_Metsamuuronen, 1)\n",
    "\n",
    "        # Somers_Delta Metsamuuronen\n",
    "        ci_lower_Grissom_Kim_Metsamuuronen = POS_Grissom_Kim - ((Standart_Error_Delta_Metsamuuronen*0.5+0.5) * Critical_t_value)\n",
    "        ci_upper_Grissom_Kim_Metsamuuronen = POS_Grissom_Kim + ((Standart_Error_Delta_Metsamuuronen*0.5+0.5) * Critical_t_value)\n",
    "        ci_lower_Grissom_Kim_Metsamuuronen = max(ci_lower_Grissom_Kim_Metsamuuronen, 0)\n",
    "        ci_upper_Grissom_Kim_Metsamuuronen = min(ci_upper_Grissom_Kim_Metsamuuronen, 1)\n",
    "\n",
    "        return POS_Grissom_Kim, POS_Vargha_Delaney, POS_Gamma_Based_Metsamuuronen, WMW_agresti, WMW_Churilov, cliffs_delta, \\\n",
    "                ci_lower_central_Grissom_Kim, ci_upper_central_Grissom_Kim,ci_lower_central_Vargha_Delaney, ci_upper_central_Vargha_Delaney,  \\\n",
    "                ci_lower_central_Gamma,ci_upper_central_Gamma, ci_lower_Gamma_Metsamuuronen,  ci_upper_Gamma_Metsamuuronen, \\\n",
    "                ci_lower_Grissom_Kim_Metsamuuronen, ci_upper_Grissom_Kim_Metsamuuronen\n",
    "\n",
    "\n",
    "#Cohensd based effect sizes\n",
    "def CLES_Cohens_based_Independent_samples(sample_mean_1, sample_mean_2, population_diff, sample_size_1, sample_size_2, sample_sd_1,sample_sd_2, df, confidence_level, bootstrap=False):\n",
    "    sample_size = sample_size_1 + sample_size_2\n",
    "    cohens_ds = (sample_mean_1 - sample_mean_2 - population_diff)  / (np.sqrt((((sample_size_1-1)*(sample_sd_1**2)) + ((sample_size_2-1)*(sample_sd_2**2))) / df))\n",
    "    cohens_dpop = cohens_ds / np.sqrt(df/(df+2))\n",
    "    t_score = cohens_ds / (np.sqrt(1/sample_size_1 + 1/sample_size_2))\n",
    "\n",
    "    cohens_U2_dpop = (norm.cdf(abs(cohens_dpop)/2))\n",
    "    cohens_U2_ds = (norm.cdf(abs(cohens_ds)/2))\n",
    "    cohens_U1_dpop = (2 * abs(cohens_U2_dpop) - 1) / (cohens_U2_dpop)\n",
    "    cohens_U1_ds =  (2*abs(cohens_U2_ds) - 1) / cohens_U2_ds\n",
    "    cohens_U3_dpop = norm.cdf(abs(cohens_dpop))\n",
    "    cohens_U3_ds = norm.cdf(abs(cohens_ds))\n",
    "\n",
    "    probabilty_of_superiority_dpop = norm.cdf(cohens_dpop/np.sqrt(2))        \n",
    "    probabilty_of_superiority_ds = norm.cdf(cohens_ds/np.sqrt(2))\n",
    "\n",
    "    proportion_of_overlap_dpop = 2 * norm.cdf(-abs(cohens_dpop) / 2)\n",
    "    proportion_of_overlap_ds = 2 * norm.cdf(-abs(cohens_ds) / 2)\n",
    "\n",
    "    Mcgraw_Wong_CLES = norm.cdf((sample_mean_1 - sample_mean_2 - population_diff) - np.sqrt(sample_sd_1**2 + sample_sd_2**2))\n",
    "\n",
    "    if bootstrap: #i need to return that as an array for the bootstrap\n",
    "        return proportion_of_overlap_ds, proportion_of_overlap_dpop, probabilty_of_superiority_ds, probabilty_of_superiority_dpop, cohens_U3_ds, cohens_U3_dpop, cohens_U1_ds, cohens_U1_dpop, cohens_U2_ds, cohens_U2_dpop\n",
    "\n",
    "    else: \n",
    "        # 1. Central CI's \n",
    "        Standart_Error_dpop =  np.sqrt(((sample_size_1+sample_size_2)/(sample_size_1*sample_size_2)) + ((cohens_dpop**2/(2*(sample_size_1+sample_size_2)))))\n",
    "        Standart_Error_ds =  np.sqrt(((sample_size_1+sample_size_2)/(sample_size_1*sample_size_2)) + ((cohens_ds**2/(2*(sample_size_2+sample_size_1)))))\n",
    "        tcrit = scipy.stats.t.ppf(1-(1-confidence_level)/2, df) #this is the critical value for calculating the confidence interval for t score\n",
    "        upper_central_ci_dpop = abs(cohens_dpop + tcrit * Standart_Error_dpop)\n",
    "        upper_central_ci_ds = abs(cohens_ds + tcrit * Standart_Error_ds)\n",
    "        lower_central_ci_dpop = abs(cohens_dpop - tcrit * Standart_Error_dpop)\n",
    "        lower_central_ci_ds = abs(cohens_ds - tcrit * Standart_Error_ds)\n",
    "\n",
    "        #Convert the upper limits to the effect sizes based on d\n",
    "        upper_central_ci_cohens_U2_dpop =  (norm.cdf(upper_central_ci_dpop/2))\n",
    "        upper_central_ci_cohens_U2_ds = (norm.cdf(upper_central_ci_ds/2))\n",
    "        upper_central_ci_cohens_U1_dpop = ((2 * upper_central_ci_cohens_U2_dpop - 1) / upper_central_ci_cohens_U2_dpop)\n",
    "        upper_central_ci_cohens_U1_ds =   ((2 * upper_central_ci_cohens_U2_ds - 1)   / upper_central_ci_cohens_U2_ds)\n",
    "        upper_central_ci_cohens_U3_dpop = norm.cdf(upper_central_ci_dpop)\n",
    "        upper_central_ci_cohens_U3_ds = norm.cdf(upper_central_ci_ds)\n",
    "        upper_central_ci_POS_dpop = norm.cdf(upper_central_ci_dpop/np.sqrt(2))\n",
    "        upper_central_ci_POS_ds = norm.cdf(upper_central_ci_ds/np.sqrt(2))\n",
    "        upper_central_ci_POO_dpop = 2 * norm.cdf(-abs(upper_central_ci_dpop) / 2)\n",
    "        upper_central_ci_POO_ds = 2 * norm.cdf(-abs(upper_central_ci_ds) / 2)\n",
    "        if upper_central_ci_cohens_U2_dpop > 100:\n",
    "                upper_central_ci_cohens_U2_dpop = 100\n",
    "        if upper_central_ci_cohens_U2_ds > 100:\n",
    "                upper_central_ci_cohens_U2_ds = 100\n",
    "        if upper_central_ci_cohens_U1_dpop > 100:\n",
    "                upper_central_ci_cohens_U1_dpop = 100\n",
    "        if upper_central_ci_cohens_U1_ds > 100:\n",
    "                upper_central_ci_cohens_U1_ds = 100\n",
    "        if upper_central_ci_cohens_U3_dpop > 100:\n",
    "                upper_central_ci_cohens_U3_dpop = 100\n",
    "        if upper_central_ci_cohens_U3_ds > 100:\n",
    "                upper_central_ci_cohens_U3_ds = 100\n",
    "        if upper_central_ci_POS_dpop > 1:\n",
    "                upper_central_ci_POS_dpop = 1\n",
    "        if upper_central_ci_POS_ds > 1:\n",
    "                upper_central_ci_POS_ds = 1\n",
    "        if upper_central_ci_POO_dpop > 1:\n",
    "                upper_central_ci_POO_dpop = 1\n",
    "        if upper_central_ci_POO_ds > 1:\n",
    "                upper_central_ci_POO_ds = 1\n",
    "        \n",
    "        #Convert the lower limits to the effect sizes based on d\n",
    "        lower_central_ci_cohens_U2_dpop = (norm.cdf(lower_central_ci_dpop/2))\n",
    "        lower_central_ci_cohens_U2_ds = (norm.cdf(lower_central_ci_ds/2))\n",
    "        lower_central_ci_cohens_U1_dpop = ((2 * (lower_central_ci_cohens_U2_dpop) - 1) / (lower_central_ci_cohens_U2_dpop))\n",
    "        lower_central_ci_cohens_U1_ds = ((2 * (lower_central_ci_cohens_U2_ds) - 1) / (lower_central_ci_cohens_U2_ds))\n",
    "        lower_central_ci_cohens_U3_dpop = norm.cdf(lower_central_ci_dpop)\n",
    "        lower_central_ci_cohens_U3_ds = norm.cdf(lower_central_ci_ds)\n",
    "        lower_central_ci_POS_dpop = norm.cdf(lower_central_ci_dpop/np.sqrt(2))        \n",
    "        lower_central_ci_POS_ds = norm.cdf(lower_central_ci_ds/np.sqrt(2))\n",
    "        lower_central_ci_POO_dpop = 2 * norm.cdf(-abs(lower_central_ci_dpop) / 2)\n",
    "        lower_central_ci_POO_ds = 2 * norm.cdf(-abs(lower_central_ci_ds) / 2)\n",
    "        if lower_central_ci_cohens_U2_dpop < 0:\n",
    "                lower_central_ci_cohens_U2_dpop = 0\n",
    "        if lower_central_ci_cohens_U2_ds < 0:\n",
    "                lower_central_ci_cohens_U2_ds = 0\n",
    "        if lower_central_ci_cohens_U1_dpop < 0:\n",
    "                lower_central_ci_cohens_U1_dpop = 0\n",
    "        if lower_central_ci_cohens_U1_ds < 0:\n",
    "                lower_central_ci_cohens_U1_ds = 0\n",
    "        if lower_central_ci_cohens_U3_dpop < 0:\n",
    "                lower_central_ci_cohens_U3_dpop = 0\n",
    "        if lower_central_ci_cohens_U3_ds < 0:\n",
    "                lower_central_ci_cohens_U3_ds = 0\n",
    "        if lower_central_ci_POS_dpop < 0:\n",
    "                lower_central_ci_POS_dpop = 0\n",
    "        if lower_central_ci_POS_ds  < 0:\n",
    "                lower_central_ci_POS_ds = 0\n",
    "        if lower_central_ci_POO_dpop < 0:\n",
    "                lower_central_ci_POO_dpop = 0\n",
    "        if lower_central_ci_POO_ds < 0:\n",
    "                lower_central_ci_POO_ds = 0\n",
    "\n",
    "        #2 Pivotal_CI's\n",
    "        lower_pivotal_ci_ds, upper_pivotal_ci_ds = pivotal_ci_t(abs(t_score), df, sample_size, confidence_level)\n",
    "        lower_pivotal_ci_ds = (lower_pivotal_ci_ds * np.sqrt(sample_size)) / np.sqrt(sample_size/4)\n",
    "        upper_pivotal_ci_ds = (upper_pivotal_ci_ds * np.sqrt(sample_size)) / np.sqrt(sample_size/4)\n",
    "\n",
    "        lower_pivotal_ci_dpop = lower_pivotal_ci_ds / np.sqrt(df/(df+2)) \n",
    "        upper_pivotal_ci_dpop = upper_pivotal_ci_ds / np.sqrt(df/(df+2)) \n",
    "        \n",
    "        upper_pivotal_ci_cohens_U2_ds = (norm.cdf(upper_pivotal_ci_ds/2))\n",
    "        upper_pivotal_ci_cohens_U1_ds = ((2 * (upper_pivotal_ci_cohens_U2_ds) - 1) / (upper_pivotal_ci_cohens_U2_ds))\n",
    "        upper_pivotal_ci_cohens_U3_ds = norm.cdf(upper_pivotal_ci_ds)\n",
    "        upper_pivotal_ci_POS_ds = norm.cdf(upper_pivotal_ci_ds/np.sqrt(2))\n",
    "        upper_pivotal_ci_POO_ds = 2 * norm.cdf(-abs(upper_pivotal_ci_ds) / 2)\n",
    "        if upper_pivotal_ci_cohens_U2_ds > 100:\n",
    "                upper_pivotal_ci_cohens_U2_ds = 100\n",
    "        if upper_pivotal_ci_cohens_U1_ds > 100:\n",
    "                upper_pivotal_ci_cohens_U1_ds = 100\n",
    "        if upper_pivotal_ci_cohens_U3_ds > 100:\n",
    "                upper_pivotal_ci_cohens_U3_ds = 100\n",
    "        if upper_pivotal_ci_POS_ds > 1:\n",
    "                upper_pivotal_ci_POS_ds = 1\n",
    "        if upper_pivotal_ci_POO_ds > 1:\n",
    "                upper_pivotal_ci_POO_ds = 1\n",
    "        \n",
    "        #Convert the lower limits to the effect sizes based on d\n",
    "        lower_pivotal_ci_cohens_U2_ds = (norm.cdf(lower_pivotal_ci_ds/2))\n",
    "        lower_pivotal_ci_cohens_U1_ds = ((2 * (lower_pivotal_ci_ds) - 1) / (lower_pivotal_ci_cohens_U2_ds))\n",
    "        lower_pivotal_ci_cohens_U3_ds = norm.cdf(lower_pivotal_ci_ds)\n",
    "        lower_pivotal_ci_POS_ds = norm.cdf(lower_pivotal_ci_ds/np.sqrt(2))        \n",
    "        lower_pivotal_ci_POO_ds = 2 * norm.cdf(-abs(lower_pivotal_ci_ds) / 2)\n",
    "        if lower_pivotal_ci_cohens_U2_ds < 0:\n",
    "                lower_pivotal_ci_cohens_U2_ds = 0\n",
    "        if lower_pivotal_ci_cohens_U1_ds < 0:\n",
    "                lower_pivotal_ci_cohens_U1_ds = 0\n",
    "        if lower_pivotal_ci_cohens_U3_ds < 0:\n",
    "                lower_pivotal_ci_cohens_U3_ds = 0\n",
    "        if lower_pivotal_ci_POS_ds < 0:\n",
    "                lower_pivotal_ci_POS_ds = 0\n",
    "        if lower_pivotal_ci_POO_ds < 0:\n",
    "                lower_pivotal_ci_POO_ds = 0\n",
    "\n",
    "\n",
    "        upper_pivotal_ci_cohens_U2_dpop = (norm.cdf(upper_pivotal_ci_dpop/2))\n",
    "        upper_pivotal_ci_cohens_U1_dpop = ((2 * (upper_pivotal_ci_cohens_U2_dpop) - 1) / (upper_pivotal_ci_cohens_U2_dpop))\n",
    "        upper_pivotal_ci_cohens_U3_dpop = norm.cdf(upper_pivotal_ci_dpop)\n",
    "        upper_pivotal_ci_POS_dpop = norm.cdf(upper_pivotal_ci_dpop/np.sqrt(2))\n",
    "        upper_pivotal_ci_POO_dpop = 2 * norm.cdf(-abs(upper_pivotal_ci_dpop) / 2)\n",
    "\n",
    "        if upper_pivotal_ci_cohens_U2_dpop > 100:\n",
    "            upper_pivotal_ci_cohens_U2_dpop = 100\n",
    "        if upper_pivotal_ci_cohens_U1_dpop > 100:\n",
    "            upper_pivotal_ci_cohens_U1_dpop = 100\n",
    "        if upper_pivotal_ci_cohens_U3_dpop > 100:\n",
    "            upper_pivotal_ci_cohens_U3_dpop = 100\n",
    "        if upper_pivotal_ci_POS_dpop > 1:\n",
    "            upper_pivotal_ci_POS_dpop = 1\n",
    "        if upper_pivotal_ci_POO_dpop > 1:\n",
    "            upper_pivotal_ci_POO_dpop = 1\n",
    "\n",
    "        # Convert the lower limits to the effect sizes based on dpop\n",
    "        lower_pivotal_ci_cohens_U2_dpop = (norm.cdf(lower_pivotal_ci_dpop/2))\n",
    "        lower_pivotal_ci_cohens_U1_dpop = ((2 * (lower_pivotal_ci_cohens_U2_dpop) - 1) / (lower_pivotal_ci_cohens_U2_dpop))\n",
    "        lower_pivotal_ci_cohens_U3_dpop = norm.cdf(lower_pivotal_ci_dpop)\n",
    "        lower_pivotal_ci_POS_dpop = norm.cdf(lower_pivotal_ci_dpop/np.sqrt(2))        \n",
    "        lower_pivotal_ci_POO_dpop = 2 * norm.cdf(-abs(lower_pivotal_ci_dpop) / 2)\n",
    "\n",
    "        if lower_pivotal_ci_cohens_U2_dpop < 0:\n",
    "            lower_pivotal_ci_cohens_U2_dpop = 0\n",
    "        if lower_pivotal_ci_cohens_U1_dpop < 0:\n",
    "            lower_pivotal_ci_cohens_U1_dpop = 0\n",
    "        if lower_pivotal_ci_cohens_U3_dpop < 0:\n",
    "            lower_pivotal_ci_cohens_U3_dpop = 0\n",
    "        if lower_pivotal_ci_POS_dpop < 0:\n",
    "            lower_pivotal_ci_POS_dpop = 0\n",
    "        if lower_pivotal_ci_POO_dpop < 0:\n",
    "            lower_pivotal_ci_POO_dpop = 0\n",
    "\n",
    "        return upper_pivotal_ci_ds, lower_pivotal_ci_ds, t_score, cohens_ds, cohens_dpop, \\\n",
    "               Mcgraw_Wong_CLES, cohens_U1_ds, cohens_U2_ds, cohens_U3_ds, probabilty_of_superiority_ds, probabilty_of_superiority_dpop, proportion_of_overlap_ds, \\\n",
    "               cohens_U1_dpop, cohens_U2_dpop, cohens_U3_dpop, probabilty_of_superiority_dpop, proportion_of_overlap_dpop, \\\n",
    "               \\\n",
    "               lower_central_ci_cohens_U2_ds, upper_central_ci_cohens_U2_ds, lower_central_ci_cohens_U1_ds, upper_pivotal_ci_cohens_U1_ds, \\\n",
    "               lower_central_ci_cohens_U3_ds, upper_central_ci_cohens_U3_ds, lower_central_ci_POS_ds, upper_central_ci_POS_ds, \\\n",
    "               lower_central_ci_POO_ds, upper_central_ci_POO_ds, lower_central_ci_cohens_U2_dpop, upper_central_ci_cohens_U2_dpop, \\\n",
    "               lower_central_ci_cohens_U1_dpop, upper_central_ci_cohens_U1_dpop, lower_central_ci_cohens_U3_dpop, upper_central_ci_cohens_U1_ds, \\\n",
    "               lower_central_ci_POS_dpop, upper_central_ci_POS_dpop, lower_central_ci_POO_dpop, upper_central_ci_POO_dpop,\\\n",
    "               \\\n",
    "               lower_pivotal_ci_cohens_U2_ds, upper_pivotal_ci_cohens_U2_ds, lower_pivotal_ci_cohens_U1_ds, upper_pivotal_ci_cohens_U1_ds, \\\n",
    "               lower_pivotal_ci_cohens_U3_ds, upper_pivotal_ci_cohens_U3_ds, lower_pivotal_ci_POS_ds, upper_pivotal_ci_POS_ds, \\\n",
    "               lower_pivotal_ci_POO_ds, upper_pivotal_ci_POO_ds, lower_pivotal_ci_cohens_U2_dpop, upper_pivotal_ci_cohens_U2_dpop, \\\n",
    "               lower_pivotal_ci_cohens_U1_dpop, upper_pivotal_ci_cohens_U1_dpop, lower_pivotal_ci_cohens_U3_dpop, upper_pivotal_ci_cohens_U1_ds, \\\n",
    "               lower_pivotal_ci_POS_dpop, upper_pivotal_ci_POS_dpop, lower_pivotal_ci_POO_dpop, upper_pivotal_ci_POO_dpop\n",
    "         \n",
    "\n",
    "def Other_CLES_dependent(x, y, confidence_level):\n",
    "    num1 = len(x)\n",
    "    num2 = len(y)\n",
    "\n",
    "    # Calculate bandwidth h\n",
    "    h_1 = (1.2 * (np.percentile(x, 75) - np.percentile(x, 25))) / (num1 ** (1 / 5))\n",
    "    h_1 = max(h_1, 0.05)\n",
    "\n",
    "    h_2 = (1.2 * (np.percentile(y, 75) - np.percentile(y, 25))) / (num2 ** (1 / 5))\n",
    "    h_2 = max(h_2, 0.05)\n",
    "\n",
    "    # Calculate F and eta for Wilcox & Musaka Q\n",
    "    eta_1 = 0\n",
    "    eta_2 = 0\n",
    "\n",
    "    for test_x in x:\n",
    "        count1 = np.sum(x <= (test_x + h_1))\n",
    "        count2 = np.sum(x < (test_x - h_1))\n",
    "        f_x1 = (count1 - count2) / (2 * num1 * h_1)\n",
    "\n",
    "        count1 = np.sum(y <= (test_x + h_2))\n",
    "        count2 = np.sum(y < (test_x - h_2))\n",
    "        f_x2 = (count1 - count2) / (2 * num2 * h_2)\n",
    "\n",
    "        if f_x1 > f_x2:\n",
    "            eta_1 += 1\n",
    "\n",
    "    for test_x in y:\n",
    "        count1 = np.sum(x <= (test_x + h_1))\n",
    "        count2 = np.sum(x < (test_x - h_1))\n",
    "        f_x1 = (count1 - count2) / (2 * num1 * h_1)\n",
    "\n",
    "        count1 = np.sum(y <= (test_x + h_2))\n",
    "        count2 = np.sum(y < (test_x - h_2))\n",
    "        f_x2 = (count1 - count2) / (2 * num2 * h_2)\n",
    "\n",
    "        if f_x2 > f_x1:\n",
    "            eta_2 += 1\n",
    "\n",
    "    Wilcox_Q = (eta_1 + eta_2) / (num1 + num2)\n",
    "\n",
    "    # Calculate CLES\n",
    "    med_x = np.median(x)\n",
    "    med_y = np.median(y)\n",
    "\n",
    "    # Compare each value in x to the median of y\n",
    "    greater_x = sum(1 for val in x if val > med_y)\n",
    "    smaller_x = sum(1 for val in x if val < med_y)\n",
    "    equal_x = len(x) - greater_x - smaller_x\n",
    "\n",
    "    # Compare each value in y to the median of x\n",
    "    greater_y = sum(1 for val in y if val > med_x)\n",
    "    smaller_y = sum(1 for val in y if val < med_x)\n",
    "    equal_y = len(y) - greater_y - smaller_y\n",
    "\n",
    "    # Avoid division by zero by dividing it in the total cases instead\n",
    "    if greater_x == 0:\n",
    "        greater_x = 1\n",
    "\n",
    "    totaln = greater_x + smaller_x + equal_x\n",
    "\n",
    "    q1_ka = greater_x / totaln\n",
    "    q1_HS = (greater_x + equal_x * 0.5) / totaln\n",
    "\n",
    "    Kramers_Andrew_Gamma = norm.ppf(q1_ka)\n",
    "    Hentschke_Stttgen_Gamma = norm.ppf(q1_HS)\n",
    "\n",
    "    return Wilcox_Q, Hentschke_Stttgen_Gamma, Kramers_Andrew_Gamma\n",
    "\n",
    "    \n",
    "# Example usage:\n",
    "x = np.array([1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "y = np.array([1,1,1,1,1,1,1,2,2,2,3,3,3,3,4,4,4,5,5])\n",
    "\n",
    "def Confidence_Interval_Central_Dependent (sample_mean_1, sample_mean_2, sample_sd_1,sample_sd_2, sample_size, pearson_correlation, population_mean_diff, confidence_level):\n",
    "    df1 = sample_size - 1 #degrees of freedom for within subject design\n",
    "    df2 = sample_size*2 - 2 #degrees of freedom for between subject design\n",
    "    convertor = (2 * (1 - pearson_correlation) / sample_size) #this one converts between effect sizes\n",
    "    reversed_convertor = sample_size / (2 * (1 - pearson_correlation)) \n",
    "    correction = correction_factor(df1)    \n",
    "\n",
    "    standardizer_dz = np.sqrt(sample_sd_1**2 + sample_sd_2**2 - 2*pearson_correlation * sample_sd_1 * sample_sd_2)\n",
    "    standardizer_dav = np.sqrt((sample_sd_1**2 + sample_sd_2**2)/2)\n",
    "    standardizer_drm = np.sqrt(sample_sd_1**2 + sample_sd_2**2 - 2*pearson_correlation * sample_sd_1 * sample_sd_2) / np.sqrt(2*(1-pearson_correlation))\n",
    "    standardizer_ds = np.sqrt(((((sample_size-1)*sample_sd_1**2)) + ((sample_size-1)*sample_sd_2**2)) / (2*sample_size-2))\n",
    "        \n",
    "    cohens_dz = (((sample_mean_1-sample_mean_2) - population_mean_diff )/standardizer_dz) #This is the effect size for one sample t-test Cohen's d\n",
    "    correction = correction_factor(df1)\n",
    "    hedges_gz = cohens_dz*correction #This is the actual corrected effect size\n",
    "    cohens_dav = abs(((sample_mean_1-sample_mean_2) - population_mean_diff )/standardizer_dav)\n",
    "    hedges_gav = cohens_dav*correction\n",
    "    cohens_drm = (sample_mean_1 - sample_mean_2 - population_mean_diff) / standardizer_drm\n",
    "    hedges_grm = cohens_drm * correction\n",
    "    cohens_ds = (((sample_mean_1-sample_mean_2) - population_mean_diff )/standardizer_ds)\n",
    "    tcrit = scipy.stats.t.ppf(1-(1-confidence_level)/2, df1) #this is the critical value for calculating the confidence interval for t score\n",
    "\n",
    "    #True Formula (Hedges 1981a):\n",
    "    True_SE_dz =  np.sqrt ((df1/(df1-2)) * (convertor) *     (1 + cohens_ds**2 * reversed_convertor) - (cohens_ds**2 / (correction_factor(df1))**2))\n",
    "    True_SE_dav = np.sqrt ((df2/(df2-2)) * (2/sample_size) * (1 + cohens_ds**2 * (sample_size/2))    - (cohens_ds**2 / (correction_factor(df2))**2)) \n",
    "    \n",
    "    lower_central_true_ci_cohens_dz = cohens_dz - tcrit * True_SE_dz\n",
    "    upper_central_true_ci_cohens_dz = cohens_dz + tcrit * True_SE_dz\n",
    "    lower_central_true_ci_cohens_drm = cohens_drm - tcrit * True_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    upper_central_true_ci_cohens_drm = cohens_drm + tcrit * True_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    lower_central_true_ci_cohens_dav = cohens_dav - tcrit * True_SE_dav  \n",
    "    upper_central_true_ci_cohens_dav = cohens_dav + tcrit * True_SE_dav\n",
    "    \n",
    "    lower_central_true_ci_cohens_gz =  hedges_gz - tcrit * True_SE_dz * correction\n",
    "    upper_central_true_ci_cohens_gz =  hedges_gz + tcrit * True_SE_dz  * correction\n",
    "    lower_central_true_ci_cohens_grm = hedges_grm - tcrit * True_SE_dz * (np.sqrt(2*(1-pearson_correlation)))  * correction\n",
    "    upper_central_true_ci_cohens_grm = hedges_grm + tcrit * True_SE_dz * (np.sqrt(2*(1-pearson_correlation))) * correction\n",
    "    lower_central_true_ci_cohens_gav = hedges_gav - tcrit * True_SE_dav   * correction\n",
    "    upper_central_true_ci_cohens_gav = hedges_gav + tcrit * True_SE_dav * correction\n",
    "\n",
    "    # Hedges Approximation (Hedges 1981b)\n",
    "    Hedges_SE_dz  = np.sqrt(convertor        *  (1 + cohens_ds**2 * reversed_convertor / (2 * df1))) \n",
    "    Hedges_SE_dav = np.sqrt((2/sample_size)  *  (1 + cohens_ds**2 * (sample_size/2)    / (2 * df2))) \n",
    "    \n",
    "    lower_central_Hedges_ci_cohens_dz =  cohens_dz - tcrit * Hedges_SE_dz \n",
    "    upper_central_Hedges_ci_cohens_dz =  cohens_dz + tcrit * Hedges_SE_dz\n",
    "    lower_central_Hedges_ci_cohens_drm = cohens_drm - tcrit * Hedges_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    upper_central_Hedges_ci_cohens_drm = cohens_drm + tcrit * Hedges_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    lower_central_Hedges_ci_cohens_dav = cohens_dav - tcrit * Hedges_SE_dav  \n",
    "    upper_central_Hedges_ci_cohens_dav = cohens_dav + tcrit * Hedges_SE_dav\n",
    "    \n",
    "    lower_central_Hedges_ci_cohens_gz =  hedges_gz - tcrit * Hedges_SE_dz * correction\n",
    "    upper_central_Hedges_ci_cohens_gz =  hedges_gz + tcrit * Hedges_SE_dz * correction\n",
    "    lower_central_Hedges_ci_cohens_grm = hedges_grm - tcrit * Hedges_SE_dz * (np.sqrt(2*(1-pearson_correlation))) * correction\n",
    "    upper_central_Hedges_ci_cohens_grm = hedges_grm + tcrit * Hedges_SE_dz * (np.sqrt(2*(1-pearson_correlation))) * correction\n",
    "    lower_central_Hedges_ci_cohens_gav = hedges_gav - tcrit * Hedges_SE_dav   * correction\n",
    "    upper_central_Hedges_ci_cohens_gav = hedges_gav + tcrit * Hedges_SE_dav * correction\n",
    "\n",
    "    #Hedges & Olkin Approximation\n",
    "    HO_Approximate_SE_dz = np.sqrt(convertor *      (1 + cohens_ds**2 * reversed_convertor / (2 * sample_size)))   \n",
    "    HO_Approximate_SE_dav = np.sqrt(2/sample_size * (1 + cohens_ds**2 * (sample_size/2)    / (2 * sample_size*2))) \n",
    "    \n",
    "    lower_central_HO_approx_ci_cohens_dz =  cohens_dz - tcrit * HO_Approximate_SE_dz\n",
    "    upper_central_HO_approx_ci_cohens_dz =  cohens_dz + tcrit * HO_Approximate_SE_dz\n",
    "    lower_central_HO_approx_ci_cohens_drm = cohens_drm - tcrit * HO_Approximate_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    upper_central_HO_approx_ci_cohens_drm = cohens_drm + tcrit * HO_Approximate_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    lower_central_HO_approx_ci_cohens_dav = cohens_dav - tcrit * HO_Approximate_SE_dav  \n",
    "    upper_central_HO_approx_ci_cohens_dav = cohens_dav + tcrit * HO_Approximate_SE_dav\n",
    "    \n",
    "    lower_central_HO_approx_ci_cohens_gz =  hedges_gz - tcrit * HO_Approximate_SE_dz  * correction\n",
    "    upper_central_HO_approx_ci_cohens_gz =  hedges_gz + tcrit * HO_Approximate_SE_dz  * correction\n",
    "    lower_central_HO_approx_ci_cohens_grm = hedges_grm - tcrit * HO_Approximate_SE_dz * (np.sqrt(2*(1-pearson_correlation)))  * correction\n",
    "    upper_central_HO_approx_ci_cohens_grm = hedges_grm + tcrit * HO_Approximate_SE_dz * (np.sqrt(2*(1-pearson_correlation)))  * correction\n",
    "    lower_central_HO_approx_ci_cohens_gav = hedges_gav - tcrit * HO_Approximate_SE_dav   * correction\n",
    "    upper_central_HO_approx_ci_cohens_gav = hedges_gav + tcrit * HO_Approximate_SE_dav  * correction\n",
    "\n",
    "    #MLE Approximation\n",
    "    MLE_SE_dz =  np.sqrt(((df1 + 2) / df1) * convertor *       (1 + cohens_ds**2 * reversed_convertor / (2*df1))) \n",
    "    MLE_SE_dav = np.sqrt(((df2 + 2) / df2) * (2/sample_size) * (1 + cohens_ds**2 * (sample_size/2)   /  (2*df2))) \n",
    "    \n",
    "    lower_central_MLE_ci_cohens_dz =  cohens_dz - tcrit * MLE_SE_dz\n",
    "    upper_central_MLE_ci_cohens_dz =  cohens_dz + tcrit * MLE_SE_dz\n",
    "    lower_central_MLE_ci_cohens_drm = cohens_drm - tcrit * MLE_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    upper_central_MLE_ci_cohens_drm = cohens_drm + tcrit * MLE_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    lower_central_MLE_ci_cohens_dav = cohens_dav - tcrit * MLE_SE_dav  \n",
    "    upper_central_MLE_ci_cohens_dav = cohens_dav + tcrit * MLE_SE_dav\n",
    "    \n",
    "    lower_central_MLE_ci_cohens_gz =  hedges_gz - tcrit * MLE_SE_dz  * correction\n",
    "    upper_central_MLE_ci_cohens_gz =  hedges_gz + tcrit * MLE_SE_dz * correction\n",
    "    lower_central_MLE_ci_cohens_grm = hedges_grm - tcrit * MLE_SE_dz * (np.sqrt(2*(1-pearson_correlation))) * correction\n",
    "    upper_central_MLE_ci_cohens_grm = hedges_grm + tcrit * MLE_SE_dz * (np.sqrt(2*(1-pearson_correlation))) * correction\n",
    "    lower_central_MLE_ci_cohens_gav = hedges_gav - tcrit * MLE_SE_dav   * correction\n",
    "    upper_central_MLE_ci_cohens_gav = hedges_gav + tcrit * MLE_SE_dav * correction\n",
    "\n",
    "    #Large_n\n",
    "    Large_n_SE_dz  = np.sqrt( convertor *        (1 + cohens_ds**2 / 8 ))\n",
    "    Large_n_SE_dav = np.sqrt((2/sample_size) *   (1 + cohens_ds**2 / 8 )) \n",
    "    \n",
    "    lower_central_Large_n_ci_cohens_dz = cohens_dz - tcrit * Large_n_SE_dz\n",
    "    upper_central_Large_n_ci_cohens_dz = cohens_dz + tcrit * Large_n_SE_dz\n",
    "    lower_central_Large_n_ci_cohens_drm = cohens_drm - tcrit * Large_n_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    upper_central_Large_n_ci_cohens_drm = cohens_drm + tcrit * Large_n_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    lower_central_Large_n_ci_cohens_dav = cohens_dav - tcrit * Large_n_SE_dav  \n",
    "    upper_central_Large_n_ci_cohens_dav = cohens_dav + tcrit * Large_n_SE_dav\n",
    "    \n",
    "    lower_central_Large_n_ci_cohens_gz =  hedges_gz - tcrit * Large_n_SE_dz * correction\n",
    "    upper_central_Large_n_ci_cohens_gz =  hedges_gz + tcrit * Large_n_SE_dz * correction\n",
    "    lower_central_Large_n_ci_cohens_grm = hedges_grm - tcrit * Large_n_SE_dz * (np.sqrt(2*(1-pearson_correlation))) * correction\n",
    "    upper_central_Large_n_ci_cohens_grm = hedges_grm + tcrit * Large_n_SE_dz * (np.sqrt(2*(1-pearson_correlation))) * correction\n",
    "    lower_central_Large_n_ci_cohens_gav = hedges_gav - tcrit * Large_n_SE_dav   * correction\n",
    "    upper_central_Large_n_ci_cohens_gav = hedges_gav + tcrit * Large_n_SE_dav * correction\n",
    "\n",
    "\n",
    "    #Correction for small n\n",
    "    Small_n_SE_dz =  np.sqrt(((df1+1)/(df1-1)) * convertor *       (1 + cohens_ds**2/8)) \n",
    "    Small_n_SE_dav = np.sqrt(((df2+1)/(df2-1)) * (2/sample_size) * (1 + cohens_ds**2/8))\n",
    "    \n",
    "    lower_central_Small_n_ci_cohens_dz = cohens_dz - tcrit *   Small_n_SE_dz\n",
    "    upper_central_Small_n_ci_cohens_dz = cohens_dz + tcrit *   Small_n_SE_dz\n",
    "    lower_central_Small_n_ci_cohens_drm = cohens_drm - tcrit * Small_n_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    upper_central_Small_n_ci_cohens_drm = cohens_drm + tcrit * Small_n_SE_dz * (np.sqrt(2*(1-pearson_correlation)))\n",
    "    lower_central_Small_n_ci_cohens_dav = cohens_dav - tcrit * Small_n_SE_dav  \n",
    "    upper_central_Small_n_ci_cohens_dav = cohens_dav + tcrit * Small_n_SE_dav\n",
    "    \n",
    "    lower_central_Small_n_ci_cohens_gz =  hedges_gz - tcrit *   Small_n_SE_dz * correction\n",
    "    upper_central_Small_n_ci_cohens_gz =  hedges_gz + tcrit *   Small_n_SE_dz * correction\n",
    "    lower_central_Small_n_ci_cohens_grm = hedges_grm - tcrit * Small_n_SE_dz * (np.sqrt(2*(1-pearson_correlation))) * correction\n",
    "    upper_central_Small_n_ci_cohens_grm = hedges_grm + tcrit * Small_n_SE_dz * (np.sqrt(2*(1-pearson_correlation))) * correction\n",
    "    lower_central_Small_n_ci_cohens_gav = hedges_gav - tcrit * Small_n_SE_dav   * correction\n",
    "    upper_central_Small_n_ci_cohens_gav = hedges_gav + tcrit * Small_n_SE_dav * correction\n",
    "\n",
    "\n",
    "\n",
    "    return cohens_dz,cohens_dav,cohens_drm,                                     \\\n",
    "            lower_central_true_ci_cohens_dz,    upper_central_true_ci_cohens_dz, \\\n",
    "            lower_central_true_ci_cohens_drm,   upper_central_true_ci_cohens_drm, \\\n",
    "            lower_central_true_ci_cohens_dav,   upper_central_true_ci_cohens_dav,  \\\n",
    "            lower_central_true_ci_cohens_gz,    upper_central_true_ci_cohens_gz, \\\n",
    "            lower_central_true_ci_cohens_grm,   upper_central_true_ci_cohens_grm, \\\n",
    "            lower_central_true_ci_cohens_gav,   upper_central_true_ci_cohens_gav,  \\\n",
    "                                                                                 \\\n",
    "            lower_central_Hedges_ci_cohens_dz,    upper_central_Hedges_ci_cohens_dz, \\\n",
    "            lower_central_Hedges_ci_cohens_drm,   upper_central_Hedges_ci_cohens_drm, \\\n",
    "            lower_central_Hedges_ci_cohens_dav,   upper_central_Hedges_ci_cohens_dav,  \\\n",
    "            lower_central_Hedges_ci_cohens_gz,    upper_central_Hedges_ci_cohens_gz, \\\n",
    "            lower_central_Hedges_ci_cohens_grm,   upper_central_Hedges_ci_cohens_grm, \\\n",
    "            lower_central_Hedges_ci_cohens_gav,   upper_central_Hedges_ci_cohens_gav,  \\\n",
    "                                                                                 \\\n",
    "            lower_central_HO_approx_ci_cohens_dz,    upper_central_HO_approx_ci_cohens_dz, \\\n",
    "            lower_central_HO_approx_ci_cohens_drm,   upper_central_HO_approx_ci_cohens_drm, \\\n",
    "            lower_central_HO_approx_ci_cohens_dav,   upper_central_HO_approx_ci_cohens_dav,  \\\n",
    "            lower_central_HO_approx_ci_cohens_gz,    upper_central_HO_approx_ci_cohens_gz, \\\n",
    "            lower_central_HO_approx_ci_cohens_grm,   upper_central_HO_approx_ci_cohens_grm, \\\n",
    "            lower_central_HO_approx_ci_cohens_gav,   upper_central_HO_approx_ci_cohens_gav,  \\\n",
    "                                                                                 \\\n",
    "            lower_central_MLE_ci_cohens_dz,    upper_central_MLE_ci_cohens_dz, \\\n",
    "            lower_central_MLE_ci_cohens_drm,   upper_central_MLE_ci_cohens_drm, \\\n",
    "            lower_central_MLE_ci_cohens_dav,   upper_central_MLE_ci_cohens_dav,  \\\n",
    "            lower_central_MLE_ci_cohens_gz,    upper_central_MLE_ci_cohens_gz, \\\n",
    "            lower_central_MLE_ci_cohens_grm,   upper_central_MLE_ci_cohens_grm, \\\n",
    "            lower_central_MLE_ci_cohens_gav,   upper_central_MLE_ci_cohens_gav,  \\\n",
    "                                                                                 \\\n",
    "            lower_central_Large_n_ci_cohens_dz,    upper_central_Large_n_ci_cohens_dz, \\\n",
    "            lower_central_Large_n_ci_cohens_drm,   upper_central_Large_n_ci_cohens_drm, \\\n",
    "            lower_central_Large_n_ci_cohens_dav,   upper_central_Large_n_ci_cohens_dav,  \\\n",
    "            lower_central_Large_n_ci_cohens_gz,    upper_central_Large_n_ci_cohens_gz, \\\n",
    "            lower_central_Large_n_ci_cohens_grm,   upper_central_Large_n_ci_cohens_grm, \\\n",
    "            lower_central_Large_n_ci_cohens_gav,   upper_central_Large_n_ci_cohens_gav,  \\\n",
    "                                                                                 \\\n",
    "            lower_central_Small_n_ci_cohens_dz,    upper_central_Small_n_ci_cohens_dz, \\\n",
    "            lower_central_Small_n_ci_cohens_drm,   upper_central_Small_n_ci_cohens_drm, \\\n",
    "            lower_central_Small_n_ci_cohens_dav,   upper_central_Small_n_ci_cohens_dav,  \\\n",
    "            lower_central_Small_n_ci_cohens_gz,    upper_central_Small_n_ci_cohens_gz, \\\n",
    "            lower_central_Small_n_ci_cohens_grm,   upper_central_Small_n_ci_cohens_grm, \\\n",
    "            lower_central_Small_n_ci_cohens_gav,   upper_central_Small_n_ci_cohens_gav\n",
    "            \n",
    "def Confidence_Interval_Central_Independent (sample_mean_1, sample_mean_2, sample_sd_1,sample_sd_2, sample_size_1, sample_size_2, pearson_correlation, population_mean_diff, confidence_level):\n",
    "    sample_size = sample_size_1 + sample_size_2\n",
    "    df = sample_size - 2 #degrees of freedom for between subject design\n",
    "    correction = correction_factor(df)\n",
    "    standardizer_ds = np.sqrt(((((sample_size_1-1)*sample_sd_1**2)) + ((sample_size_2-1)*sample_sd_2**2)) / (sample_size-2))\n",
    "    h1 = (sample_size_1 + sample_size_2) / (sample_size_1 * sample_size_2) #2/(harmonic_mean of n1,n2)\n",
    "    h2 = (sample_size_1 + sample_size_2) / (sample_size_1 * sample_size_2) # (harmonic_mean of n1,n2)/2\n",
    "   \n",
    "    cohens_ds = (((sample_mean_1-sample_mean_2) - population_mean_diff )/standardizer_ds)\n",
    "    cohens_dpop = cohens_ds / np.sqrt((sample_size-2)/sample_size)\n",
    "    hedges_gs = cohens_ds*correction\n",
    "    tcrit = scipy.stats.t.ppf(1-(1-confidence_level)/2, df) #this is the critical value for calculating the confidence interval for t score\n",
    "\n",
    "    #True Formula (Hedges 1981a):\n",
    "    True_SE_ds =  np.sqrt ((df/(df-2)) * (h1) *     (1 + cohens_ds**2 * h2) - (cohens_ds**2 / (correction_factor(df))**2))\n",
    "    \n",
    "    lower_central_true_ci_cohens_ds =  cohens_ds - tcrit * True_SE_ds\n",
    "    upper_central_true_ci_cohens_ds =  cohens_ds + tcrit * True_SE_ds\n",
    "    lower_central_true_ci_cohens_dpop = cohens_dpop - tcrit * True_SE_ds / np.sqrt((sample_size-2)/sample_size)\n",
    "    upper_central_true_ci_cohens_dpop = cohens_dpop + tcrit * True_SE_ds / np.sqrt((sample_size-2)/sample_size)\n",
    "    lower_central_true_ci_cohens_gs =  hedges_gs - tcrit * True_SE_ds * correction\n",
    "    upper_central_true_ci_cohens_gs =  hedges_gs + tcrit * True_SE_ds  * correction\n",
    "    \n",
    "    # Hedges Approximation (Hedges 1981b)\n",
    "    Hedges_SE_ds  = np.sqrt(h1        *  (1 + cohens_ds**2 * h2 / (2 * df))) \n",
    "    \n",
    "    lower_central_Hedges_ci_cohens_ds =  cohens_ds - tcrit * Hedges_SE_ds \n",
    "    upper_central_Hedges_ci_cohens_ds =  cohens_ds + tcrit * Hedges_SE_ds\n",
    "    lower_central_Hedges_ci_cohens_dpop =  cohens_dpop - tcrit * Hedges_SE_ds / np.sqrt((sample_size-2)/sample_size)\n",
    "    upper_central_Hedges_ci_cohens_dpop =  cohens_dpop + tcrit * Hedges_SE_ds / np.sqrt((sample_size-2)/sample_size)\n",
    "    lower_central_Hedges_ci_cohens_gs =  hedges_gs - tcrit * Hedges_SE_ds * correction\n",
    "    upper_central_Hedges_ci_cohens_gs =  hedges_gs + tcrit * Hedges_SE_ds * correction\n",
    "   \n",
    "    #Hedges & Olkin Approximation\n",
    "    HO_Approximate_SE_ds = np.sqrt(h1 *      (1 + cohens_ds**2 * h2 / (2 * sample_size)))   \n",
    "    \n",
    "    lower_central_HO_approx_ci_cohens_ds =  cohens_ds - tcrit *  HO_Approximate_SE_ds\n",
    "    upper_central_HO_approx_ci_cohens_ds =  cohens_ds + tcrit *  HO_Approximate_SE_ds\n",
    "    lower_central_HO_approx_ci_cohens_dpop = cohens_dpop  - tcrit * HO_Approximate_SE_ds / np.sqrt((sample_size-2)/sample_size)\n",
    "    upper_central_HO_approx_ci_cohens_dpop = cohens_dpop  + tcrit * HO_Approximate_SE_ds / np.sqrt((sample_size-2)/sample_size)    \n",
    "    lower_central_HO_approx_ci_cohens_gs =  hedges_gs - tcrit *  HO_Approximate_SE_ds * correction\n",
    "    upper_central_HO_approx_ci_cohens_gs =  hedges_gs + tcrit *  HO_Approximate_SE_ds * correction\n",
    "\n",
    "    #MLE Approximation\n",
    "    MLE_SE_ds =  np.sqrt(((df + 2) / df) * h1 *       (1 + cohens_ds**2 * h2 / (2*df))) \n",
    "    \n",
    "    lower_central_MLE_ci_cohens_ds =  cohens_ds - tcrit * MLE_SE_ds\n",
    "    upper_central_MLE_ci_cohens_ds =  cohens_ds + tcrit * MLE_SE_ds\n",
    "    lower_central_MLE_ci_cohens_dpop = cohens_dpop  - tcrit * MLE_SE_ds / np.sqrt((sample_size-2)/sample_size)\n",
    "    upper_central_MLE_ci_cohens_dpop = cohens_dpop  + tcrit * MLE_SE_ds / np.sqrt((sample_size-2)/sample_size)    \n",
    "    lower_central_MLE_ci_cohens_gs =  hedges_gs - tcrit * MLE_SE_ds  * correction\n",
    "    upper_central_MLE_ci_cohens_gs =  hedges_gs + tcrit * MLE_SE_ds * correction\n",
    "\n",
    "    #Large_n\n",
    "    Large_n_SE_ds  = np.sqrt( h1 *        (1 + cohens_ds**2 / 8 ))\n",
    "    \n",
    "    lower_central_Large_n_ci_cohens_ds =  cohens_ds - tcrit * Large_n_SE_ds\n",
    "    upper_central_Large_n_ci_cohens_ds =  cohens_ds + tcrit * Large_n_SE_ds\n",
    "    lower_central_Large_n_ci_cohens_dpop = cohens_dpop  - tcrit * Large_n_SE_ds / np.sqrt((sample_size-2)/sample_size)\n",
    "    upper_central_Large_n_ci_cohens_dpop = cohens_dpop  + tcrit * Large_n_SE_ds / np.sqrt((sample_size-2)/sample_size) \n",
    "    lower_central_Large_n_ci_cohens_gs =  hedges_gs - tcrit * Large_n_SE_ds * correction\n",
    "    upper_central_Large_n_ci_cohens_g =  hedges_gs + tcrit * Large_n_SE_ds * correction\n",
    "\n",
    "\n",
    "    #Correction for small n\n",
    "    Small_n_SE_ds =  np.sqrt(((df+1)/(df-1)) * h1 *       (1 + cohens_ds**2/8)) \n",
    "    \n",
    "    lower_central_Small_n_ci_cohens_ds =  cohens_ds - tcrit *   Small_n_SE_ds\n",
    "    upper_central_Small_n_ci_cohens_ds =  cohens_ds + tcrit *   Small_n_SE_ds\n",
    "    lower_central_Small_n_ci_cohens_dpop = cohens_dpop  - tcrit * Small_n_SE_ds / np.sqrt((sample_size-2)/sample_size)\n",
    "    upper_central_Small_n_ci_cohens_dpop = cohens_dpop  + tcrit * Small_n_SE_ds / np.sqrt((sample_size-2)/sample_size)    \n",
    "    lower_central_Small_n_ci_cohens_gs =  hedges_gs - tcrit *   Small_n_SE_ds * correction\n",
    "    upper_central_Small_n_ci_cohens_gs =  hedges_gs + tcrit *   Small_n_SE_ds * correction\n",
    "\n",
    "\n",
    "    return cohens_ds,cohens_dpop, hedges_gs,                                     \\\n",
    "            lower_central_true_ci_cohens_ds,    upper_central_true_ci_cohens_ds, \\\n",
    "            lower_central_true_ci_cohens_dpop,   upper_central_true_ci_cohens_dpop, \\\n",
    "            lower_central_true_ci_cohens_gs,   upper_central_true_ci_cohens_gs,  \\\n",
    "                                                                       \\\n",
    "            lower_central_Hedges_ci_cohens_ds,    upper_central_Hedges_ci_cohens_ds, \\\n",
    "            lower_central_Hedges_ci_cohens_dpop,   upper_central_Hedges_ci_cohens_dpop, \\\n",
    "            lower_central_Hedges_ci_cohens_gs,   upper_central_Hedges_ci_cohens_gs,  \\\n",
    "                                                                                 \\\n",
    "            lower_central_HO_approx_ci_cohens_ds,    upper_central_HO_approx_ci_cohens_ds, \\\n",
    "            lower_central_HO_approx_ci_cohens_dpop,   upper_central_HO_approx_ci_cohens_dpop, \\\n",
    "            lower_central_HO_approx_ci_cohens_gs,   upper_central_HO_approx_ci_cohens_gs,  \\\n",
    "                                                                  \\\n",
    "            lower_central_MLE_ci_cohens_ds,    upper_central_MLE_ci_cohens_ds, \\\n",
    "            lower_central_MLE_ci_cohens_dpop,   upper_central_MLE_ci_cohens_dpop, \\\n",
    "            lower_central_MLE_ci_cohens_gs,   upper_central_MLE_ci_cohens_gs,  \\\n",
    "                                                                    \\\n",
    "            lower_central_Large_n_ci_cohens_ds,    upper_central_Large_n_ci_cohens_ds, \\\n",
    "            lower_central_Large_n_ci_cohens_dpop,   upper_central_Large_n_ci_cohens_dpop, \\\n",
    "            lower_central_Large_n_ci_cohens_gs,   upper_central_Large_n_ci_cohens_g,  \\\n",
    "                                                                     \\\n",
    "            lower_central_Small_n_ci_cohens_ds,    upper_central_Small_n_ci_cohens_ds, \\\n",
    "            lower_central_Small_n_ci_cohens_dpop,   upper_central_Small_n_ci_cohens_dpop, \\\n",
    "            lower_central_Small_n_ci_cohens_gs,   upper_central_Small_n_ci_cohens_gs\n",
    "\n",
    "\n",
    "###############################################\n",
    "##### Relevant Functions For Correlations!!####\n",
    "###############################################\n",
    "\n",
    "### 1. Goodman and Kruskal Lambda Coefficeint\n",
    "def goodman_kruskal_lamda_correlation(matrix, confidence_level):\n",
    "    \n",
    "        # Calcluation of the Matrix follows Hartwig, 1976\n",
    "        #################################################\n",
    "\n",
    "        # Please Note that SPSS and DescTools have a different method for calculating the SRK in the symmetric method - they are both yields the same results \n",
    "\n",
    "\n",
    "        # Marginal total Frequencies\n",
    "        csum = np.sum(matrix, axis=0)\n",
    "        rsum = np.sum(matrix, axis=1)\n",
    "        \n",
    "        n = np.sum(matrix) # Sample Size\n",
    "        Nrc = np.sum(np.max(matrix, axis=1)) # Sum of the largest values in each Row\n",
    "        Nkc = np.sum(np.max(matrix, axis=0)) # Sum of the largest values in each Coloumn\n",
    "        Nrm = np.max(rsum) # This is the largest value in rows\n",
    "        Nkm = np.max(csum) # This is the largest value in coloumns\n",
    "        Um = Nrm + Nkm #For later calculations shortcuts\n",
    "        Uc = Nrc + Nkc #for later calculations shortcuts\n",
    "        \n",
    "        # Calculation of Nr_tag and Nk_tag - In case of duplicates one need to store all the combinations of Nr_tag and Nktag\n",
    "\n",
    "        # Find row(s) with the largest row sum and return a vector for the Nr_tag\n",
    "        row_sums = np.sum(matrix, axis=1)\n",
    "        rows_with_largest_rsum = np.where(row_sums == Nrm)[0]\n",
    "        largest_row_values_vector = [] # Create a vector to store the largest values from the rows with the largest rsum\n",
    "\n",
    "        # Iterate through the rows with the largest rsum and find the largest value in each row\n",
    "        for row_idx in rows_with_largest_rsum:\n",
    "            largest_value_in_row = np.max(matrix[row_idx, :])\n",
    "            largest_row_values_vector.append(largest_value_in_row)\n",
    "        largest_rows_vector = np.array(largest_row_values_vector)\n",
    "\n",
    "        # Find row(s) with the largest coloumn sum and return a vector for the Nk_tag\n",
    "        column_sums = np.sum(matrix, axis=0)\n",
    "        columns_with_largest_csum = np.where(column_sums == Nkm)[0]\n",
    "        largest_col_values_vector = []\n",
    "\n",
    "        # Iterate through the columns with the largest csum and find the largest value in each column\n",
    "        for col_idx in columns_with_largest_csum:\n",
    "            largest_value_in_column = np.max(matrix[:, col_idx])\n",
    "            largest_col_values_vector.append(largest_value_in_column)\n",
    "        largest_col_vector = np.array(largest_col_values_vector)\n",
    "\n",
    "        #Now calculate all the possible combination of Nks_tag and Nr_tag and return them as seperate vectors\n",
    "        combinations_nks_nrs = list(product(largest_rows_vector, largest_col_vector))\n",
    "        Nk_tag = [item[0] for item in combinations_nks_nrs]\n",
    "        Nr_tag = [item[1] for item in combinations_nks_nrs]\n",
    "\n",
    "        # Calculation of Ntag that can also have multiple values in case of marginal totals frequencies ties\n",
    "        #Locate the row\n",
    "        rows_with_most_frequencies = np.where(rsum == Nrm)[0]  \n",
    "        cols_with_most_frequencies = np.where(csum == Nkm)[0]\n",
    "\n",
    "        N_tags_combinations = list(product(rows_with_most_frequencies, cols_with_most_frequencies))\n",
    "        N_tags = []\n",
    "\n",
    "        #Ntags values - test which rows\\coloumns in the table have \n",
    "        for row_idx, col_idx in N_tags_combinations:\n",
    "            N_tags.append(matrix[row_idx, col_idx])\n",
    "\n",
    "        #Skcr and Srck - we need to search for the rows and coloums with the largest marginal rows and \n",
    "        \n",
    "        #Skcr\n",
    "        # Create a vector to store the largest values from the rows with the largest rsum\n",
    "        row_sums = np.sum(matrix, axis=1)\n",
    "        rows_with_largest_rsum = np.where(row_sums == Nrm)[0]\n",
    "        largest_row_values_vector = []  # Create a vector to store the largest values from the rows with the largest rsum\n",
    "\n",
    "        # Iterate through the rows with the largest rsum and find the largest value in each row\n",
    "        for row_idx in rows_with_largest_rsum:\n",
    "            largest_value_in_row = np.max(matrix[row_idx, :])\n",
    "            largest_row_values_vector.append(largest_value_in_row)\n",
    "\n",
    "        # Sum the values in each row that are the maximum in their respective columns\n",
    "        sum_of_highest_values_rows = []\n",
    "        for row_idx in rows_with_largest_rsum:\n",
    "            max_values_in_cols = [matrix[row_idx, col_idx] for col_idx in range(matrix.shape[1]) if matrix[row_idx, col_idx] == np.max(matrix[:, col_idx])]\n",
    "            row_sum_of_highest_values = sum(max_values_in_cols)\n",
    "            sum_of_highest_values_rows.append(row_sum_of_highest_values)\n",
    "\n",
    "        sum_of_highest_values_row_vector = np.array(sum_of_highest_values_rows)\n",
    "\n",
    "        #Srck \n",
    "        col_sums = np.sum(matrix, axis=0)\n",
    "        cols_with_largest_csum = np.where(col_sums == Nkm)[0]\n",
    "        largest_col_values_vector = []  # Create a vector to store the largest values from the rows with the largest rsum\n",
    "\n",
    "        # Iterate through the rows with the largest rsum and find the largest value in each row\n",
    "        for col_idx in cols_with_largest_csum:\n",
    "            largest_value_in_col = np.max(matrix[:, col_idx])\n",
    "            largest_col_values_vector.append(largest_value_in_col)\n",
    "\n",
    "        # Sum the values in each col that are the maximum in their respective row\n",
    "        sum_of_highest_values_col = []\n",
    "        for col_idx in cols_with_largest_csum:\n",
    "            max_values_in_rows = [matrix[row_idx, col_idx] for row_idx in range(matrix.shape[0]) if matrix[row_idx,col_idx] == np.max(matrix[row_idx,: ])]\n",
    "            col_sum_of_highest_values = sum(max_values_in_rows)\n",
    "            sum_of_highest_values_col.append(col_sum_of_highest_values)\n",
    "\n",
    "        sum_of_highest_values_col_vector = np.array(sum_of_highest_values_col)\n",
    "\n",
    "        #Now calculate all the possible combination of Nks_tag and Nr_tag and return them as a seperate vectors\n",
    "        combinations_Skcr_Srck = list(product(sum_of_highest_values_row_vector, sum_of_highest_values_col_vector))\n",
    "        Skcr = [item[0] for item in combinations_Skcr_Srck] #This is basically the sum of all largest values in each coloumn seperatly for each row\n",
    "        Srck = [item[1] for item in combinations_Skcr_Srck] #This is basically the sum of all largest values in each row seperatly for each coloumn\n",
    "\n",
    "        #Calculate Srk - sum of all  largest values in both their row and coloumn\n",
    "        largest_both_values = []\n",
    "        \n",
    "        for row_idx in range(matrix.shape[0]):\n",
    "            for col_idx in range(matrix.shape[1]):\n",
    "                value = matrix[row_idx, col_idx]\n",
    "                if value == np.max(matrix[row_idx, :]) and value == np.max(matrix[:, col_idx]):\n",
    "                    largest_both_values.append(value)\n",
    "        \n",
    "        Srk = np.sum(largest_both_values)\n",
    "\n",
    "        #Now the Utag for shortening calcualtions\n",
    "        Utag = []\n",
    "        for i in range(len(Skcr)):\n",
    "            sum_value = Skcr[i] + Srck[i] + Nr_tag[i] + Nk_tag[i]\n",
    "            Utag.append(sum_value)\n",
    "\n",
    "\n",
    "        # Final Parameters - Note that SAS has a wrong calcualation\n",
    "\n",
    "    \n",
    "        #Standard Errors\n",
    "        Standard_Error_Rows = []\n",
    "        for srck_value in Srck:\n",
    "            standard_error_row = 1 / np.sqrt(((n - Nkm) ** 3) / ((n - Nrc) * (Nrc + Nkm - 2 * srck_value)))\n",
    "            Standard_Error_Rows.append(standard_error_row)\n",
    "\n",
    "        # Calculate Standard Errors for Columns\n",
    "        Standard_Error_Coloumns = []\n",
    "        for skcr_value in Skcr:\n",
    "            standard_error_col = 1 / np.sqrt(((n - Nrm) ** 3) / ((n - Nkc) * (Nkc + Nrm - 2 * skcr_value)))\n",
    "            Standard_Error_Coloumns.append(standard_error_col)\n",
    "    \n",
    "    # Calculate Symmetric Standard Errors \n",
    "        Standard_Error_Symmetric = []\n",
    "        for Utag_value, Ntags_Value in zip(Utag, N_tags):\n",
    "            standard_error_sym  = 1/ ((2*n - Um)**4 / ((2*n - Um) * (2*n - Uc) * (Um + Uc + 4*n - 2 *Utag_value) - 2*(2*n - Um)**2 * (n-Srk) - 2*(2*n - Uc)**2 * (n-Ntags_Value)))**0.5\n",
    "            Standard_Error_Symmetric.append(standard_error_sym)\n",
    "\n",
    "        #Final Parameters for Output\n",
    "        \n",
    "        # Lambda Values\n",
    "        Lambda_row = (Nrc - Nkm) / (n - Nkm)\n",
    "        Lambda_col = (Nkc - Nrm) / (n - Nrm)\n",
    "        Lambda_symmetric = (Nrc + Nkc - Nrm - Nkm) / (2 * n - Nrm - Nkm)\n",
    "\n",
    "        # Standard Errors Methods CI's and inferential statistics\n",
    "        z_critical_value = st.norm.ppf(confidence_level + ((1 - confidence_level) / 2))\n",
    "\n",
    "        # 1. Random Method - other software use the first values in matrix - here is the random method\n",
    "        Standard_Error_Rows_random = random.choice(Standard_Error_Rows)\n",
    "        Standard_Error_Coloumns_random = random.choice(Standard_Error_Coloumns)\n",
    "        Standard_Error_Symmetric_random = random.choice(Standard_Error_Symmetric)\n",
    "        Z_row_rand = Lambda_row / Standard_Error_Rows_random\n",
    "        Z_col_rand = Lambda_col / Standard_Error_Coloumns_random\n",
    "        Z_symmetric_rand = Lambda_symmetric / Standard_Error_Symmetric_random\n",
    "        pval_row_rand = calculate_p_value_from_z_score(Z_row_rand)\n",
    "        pval_col_rand = calculate_p_value_from_z_score(Z_col_rand)\n",
    "        pval_symmetric_rand = calculate_p_value_from_z_score(Z_symmetric_rand)\n",
    "        lower_ci_lambda_rows_rand = max(Lambda_row - z_critical_value * Standard_Error_Rows_random,0)\n",
    "        upper_ci_lambda_rows_rand = min(Lambda_row + z_critical_value * Standard_Error_Rows_random,1)\n",
    "        lower_ci_lambda_cols_rand = max(Lambda_col - z_critical_value * Standard_Error_Coloumns_random,0)\n",
    "        upper_ci_lambda_cols_rand = min(Lambda_col + z_critical_value * Standard_Error_Coloumns_random,1)\n",
    "        lower_ci_lambda_symmetric_rand = max(Lambda_symmetric - z_critical_value * Standard_Error_Symmetric_random,0)\n",
    "        upper_ci_lambda_symmetric_rand = min(Lambda_symmetric + z_critical_value * Standard_Error_Symmetric_random,1)\n",
    "        \n",
    "        # 2. Maximum Method - Strict Method - Uses the Largest value\n",
    "        Standard_Error_Rows_max = np.max(Standard_Error_Rows)\n",
    "        Standard_Error_Coloumns_max = np.max(Standard_Error_Coloumns)\n",
    "        Standard_Error_Symmetric_max = np.max(Standard_Error_Symmetric)\n",
    "        Z_row_max = Lambda_row / Standard_Error_Rows_max\n",
    "        Z_col_max = Lambda_col / Standard_Error_Coloumns_max\n",
    "        Z_symmetric_max = Lambda_symmetric / Standard_Error_Symmetric_max\n",
    "        pval_row_max = calculate_p_value_from_z_score(Z_row_max)\n",
    "        pval_col_max = calculate_p_value_from_z_score(Z_col_max)\n",
    "        pval_symmetric_max = calculate_p_value_from_z_score(Z_symmetric_max)\n",
    "        lower_ci_lambda_rows_max = max(Lambda_row - z_critical_value * Standard_Error_Rows_max,0)\n",
    "        upper_ci_lambda_rows_max = min(Lambda_row + z_critical_value * Standard_Error_Rows_max,1)\n",
    "        lower_ci_lambda_cols_max = max(Lambda_col - z_critical_value * Standard_Error_Coloumns_max,0)\n",
    "        upper_ci_lambda_cols_max = min(Lambda_col + z_critical_value * Standard_Error_Coloumns_max,1)\n",
    "        lower_ci_lambda_symmetric_max = max(Lambda_symmetric - z_critical_value * Standard_Error_Symmetric_max,0)\n",
    "        upper_ci_lambda_symmetric_max = min(Lambda_symmetric + z_critical_value * Standard_Error_Symmetric_max,1)\n",
    "\n",
    "        # 3. Mean Method - Average of all standard Error\n",
    "        Standard_Error_Rows_mean = np.mean(Standard_Error_Rows)\n",
    "        Standard_Error_Coloumns_mean = np.mean(Standard_Error_Coloumns)\n",
    "        Standard_Error_Symmetric_mean = np.mean(Standard_Error_Symmetric)\n",
    "        Z_row_mean = Lambda_row / Standard_Error_Rows_mean\n",
    "        Z_col_mean = Lambda_col / Standard_Error_Coloumns_mean\n",
    "        Z_symmetric_mean = Lambda_symmetric / Standard_Error_Symmetric_mean\n",
    "        pval_row_mean = calculate_p_value_from_z_score(Z_row_mean)\n",
    "        pval_col_mean = calculate_p_value_from_z_score(Z_col_mean)\n",
    "        pval_symmetric_mean = calculate_p_value_from_z_score(Z_symmetric_mean)\n",
    "        lower_ci_lambda_rows_mean = max(Lambda_row - z_critical_value * Standard_Error_Rows_mean,0)\n",
    "        upper_ci_lambda_rows_mean = min(Lambda_row + z_critical_value * Standard_Error_Rows_mean,1)\n",
    "        lower_ci_lambda_cols_mean = max(Lambda_col - z_critical_value * Standard_Error_Coloumns_mean,0)\n",
    "        upper_ci_lambda_cols_mean = min(Lambda_col + z_critical_value * Standard_Error_Coloumns_mean,1)\n",
    "        lower_ci_lambda_symmetric_mean = max(Lambda_symmetric - z_critical_value * Standard_Error_Symmetric_mean,0)\n",
    "        upper_ci_lambda_symmetric_mean = min(Lambda_symmetric + z_critical_value * Standard_Error_Symmetric_mean,1)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Lambda Values\n",
    "        results[\"Lambda Rows\"] = round(Lambda_row, 7)\n",
    "        results[\"Lambda Columns\"] = round(Lambda_col, 7)\n",
    "        results[\"Lambda Symmetric\"] = round(Lambda_symmetric, 7)\n",
    "\n",
    "        # Random Method\n",
    "        results[\"Random Method - P-Value Rows\"] = pval_row_rand\n",
    "        results[\"Random Method - P-Value Columns\"] = pval_col_rand\n",
    "        results[\"Random Method - P-Value Symmetric\"] = pval_symmetric_rand\n",
    "        results[\"Random Method - Confidence Interval Rows\"] = [round(lower_ci_lambda_rows_rand, 7), round(upper_ci_lambda_rows_rand, 7)]\n",
    "        results[\"Random Method - Confidence Interval Columns\"] = [round(lower_ci_lambda_cols_rand, 7), round(upper_ci_lambda_cols_rand, 7)]\n",
    "        results[\"Random Method - Confidence Interval Symmetric\"] = [round(lower_ci_lambda_symmetric_rand, 7), round(upper_ci_lambda_symmetric_rand, 4)]\n",
    "        results[\"Random Method - Standard Error Rows\"] = round(Standard_Error_Rows_random, 7)\n",
    "        results[\"Random Method - Standard Error Columns\"] = round(Standard_Error_Coloumns_random, 7)\n",
    "        results[\"Random Method - Standard Error Symmetric\"] = round(Standard_Error_Symmetric_random, 7)\n",
    "\n",
    "        # Maximum Method\n",
    "        results[\"Maximum Method - P-Value Rows\"] = pval_row_max\n",
    "        results[\"Maximum Method - P-Value Columns\"] = pval_col_max\n",
    "        results[\"Maximum Method - P-Value Symmetric\"] = pval_symmetric_max\n",
    "        results[\"Maximum Method - Confidence Interval Rows\"] = [round(lower_ci_lambda_rows_max, 7), round(upper_ci_lambda_rows_max, 7)]\n",
    "        results[\"Maximum Method - Confidence Interval Columns\"] = [round(lower_ci_lambda_cols_max, 7), round(upper_ci_lambda_cols_max, 7)]\n",
    "        results[\"Maximum Method - Confidence Interval Symmetric\"] = [round(lower_ci_lambda_symmetric_max, 7), round(upper_ci_lambda_symmetric_max, 4)]\n",
    "        results[\"Maximum Method - Standard Error Rows\"] = round(Standard_Error_Rows_max, 7)\n",
    "        results[\"Maximum Method - Standard Error Columns\"] = round(Standard_Error_Coloumns_max, 7)\n",
    "        results[\"Maximum Method - Standard Error Symmetric\"] = round(Standard_Error_Symmetric_max, 7)\n",
    "\n",
    "        # Mean Method\n",
    "        results[\"Mean Method - P-Value Rows\"] = pval_row_mean\n",
    "        results[\"Mean Method - P-Value Columns\"] = pval_col_mean\n",
    "        results[\"Mean Method - P-Value Symmetric\"] = pval_symmetric_mean\n",
    "        results[\"Mean Method - Confidence Interval Rows\"] = [round(lower_ci_lambda_rows_mean, 7), round(upper_ci_lambda_rows_mean, 7)]\n",
    "        results[\"Mean Method - Confidence Interval Columns\"] = [round(lower_ci_lambda_cols_mean, 7), round(upper_ci_lambda_cols_mean, 7)]\n",
    "        results[\"Mean Method - Confidence Interval Symmetric\"] = [round(lower_ci_lambda_symmetric_mean, 7), round(upper_ci_lambda_symmetric_mean, 7)]\n",
    "        results[\"Mean Method - Standard Error Rows\"] = round(Standard_Error_Rows_mean, 7)\n",
    "        results[\"Mean Method - Standard Error Columns\"] = round(Standard_Error_Coloumns_mean, 7)\n",
    "        results[\"Mean Method - Standard Error Symmetric\"] = round(Standard_Error_Symmetric_mean, 7)\n",
    "\n",
    "        result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "        return result_str\n",
    "\n",
    "###2. Goodman and Kruskal Tau (Data (x variable) needs to be in a matrix that rperesents contingency Table)\n",
    "\n",
    "def Goodman_Kruskal_Tau(x, confidence_level=0.95):\n",
    "    #This function uses the Method presented in Libertau 1983 to calcualte the Standard Error\n",
    "\n",
    "    # Global Variables\n",
    "    sample_size = np.sum(x)\n",
    "    Sum_Of_The_Rows = np.sum(x, axis=1)\n",
    "    Sum_Of_The_Columns = np.sum(x, axis=0)\n",
    "    Conditional_Errors_Coloumns = sample_size ** 2 - np.sum(Sum_Of_The_Columns ** 2)\n",
    "    Conditional_Errors_Rows = sample_size ** 2 - np.sum(Sum_Of_The_Rows ** 2)\n",
    "    Mean_Rows = Conditional_Errors_Rows / (sample_size ** 2)\n",
    "    Mean_Coloumns = Conditional_Errors_Coloumns / (sample_size ** 2)\n",
    "\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "\n",
    "    # Calculate Tau_r for the Rows\n",
    "    Unconditonal_Error_Rows = (sample_size ** 2) - sample_size * np.sum((x[:, np.newaxis] ** 2) / Sum_Of_The_Columns[np.newaxis])\n",
    "    Tau_Rows = 1 - (Unconditonal_Error_Rows / Conditional_Errors_Rows)\n",
    "    v = Unconditonal_Error_Rows / (sample_size ** 2)\n",
    "    ASE_Rows = np.sqrt(np.sum((x * (-2 * v * (Sum_Of_The_Rows[:, np.newaxis] / sample_size) + Mean_Rows * ((2 * x / Sum_Of_The_Columns) - np.sum((x / Sum_Of_The_Columns) ** 2, axis=0)) - (Mean_Rows * (v + 1) - 2 * v)) ** 2) / (sample_size ** 2 * Mean_Rows ** 4)))\n",
    "    Confidence_Intervals_Rows = zcrit * ASE_Rows * np.array([-1, 1]) + Tau_Rows\n",
    "\n",
    "    # Calculate Tau_c for the Coloumns\n",
    "    Unconditional_Error_Coloumns = (sample_size ** 2) - sample_size * np.sum((x ** 2) / Sum_Of_The_Rows[:, np.newaxis])\n",
    "    Tau_Coloumns = 1 - (Unconditional_Error_Coloumns / Conditional_Errors_Coloumns)\n",
    "    v2 = Unconditional_Error_Coloumns / (sample_size ** 2)\n",
    "\n",
    "    ASE_Columns = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            term = x[i, j] * (-2 * v2 * (Sum_Of_The_Columns[j] / sample_size) + Mean_Coloumns * ((2 * x[i, j] / Sum_Of_The_Rows[i]) - np.sum((x[i, :] / Sum_Of_The_Rows[i]) ** 2)) - (Mean_Coloumns * (v2 + 1) - 2 * v2)) ** 2 / (sample_size ** 2 * Mean_Coloumns ** 4)\n",
    "            ASE_Columns = ASE_Columns + term\n",
    "    ASE_Columns = np.sqrt(ASE_Columns)\n",
    "    Confidence_Intervals_Coloumns = zcrit * ASE_Columns * np.array([-1, 1]) + Tau_Coloumns\n",
    "\n",
    "    # Symmetric Tau\n",
    "    alpha = ((sample_size ** 2) - np.sum(Sum_Of_The_Rows**2)) / ((2* (sample_size ** 2)) - np.sum(Sum_Of_The_Rows**2) - np.sum(Sum_Of_The_Columns**2))\n",
    "    Tau_Symmetric = (Tau_Rows*alpha) + (1-alpha)*Tau_Coloumns\n",
    "    ASE_Symmetric = (ASE_Rows*alpha) + (1-alpha)*ASE_Columns\n",
    "    Confidence_Intervals_Symmetric = zcrit * ASE_Symmetric * np.array([-1, 1]) + Tau_Symmetric\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Goodman Kruskal Tau (Rows):\"]= Tau_Rows\n",
    "    results[\"Standard Error Rows\"] = ASE_Rows\n",
    "    results[\"Goodman-Kruskal CI's (Rows)\"] = Confidence_Intervals_Rows\n",
    "    results[\"Goodman Kruskal Tau (Columns)\"]= Tau_Coloumns\n",
    "    results[\"Standard Error Coloumns\"]= ASE_Columns\n",
    "    results[\"Goodman-Kruskal CI's (Coloumns)\"]= Confidence_Intervals_Coloumns\n",
    "    results[\"Goodman Kruskal Tau (Symmetric)\"]= Tau_Symmetric\n",
    "    results[\"Standard Error Symmetric\"]= ASE_Symmetric\n",
    "    results[\"Goodman-Kruskal CI's (Symmetric)\"]= Confidence_Intervals_Symmetric\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "###3. Uncertainty Coefficient - Needs to be in a Contingency Table\n",
    "\n",
    "def uncertainty_coefficient(contingency_table, confidence_level = 0.95):\n",
    "    Sum_Of_Rows = np.sum(contingency_table, axis=1)\n",
    "    Sum_Of_Columns = np.sum(contingency_table, axis=0)\n",
    "    sample_size = np.sum(contingency_table)\n",
    "    HX = -np.sum((Sum_Of_Rows * np.log(Sum_Of_Rows / sample_size)) / sample_size)\n",
    "    HY = -np.sum((Sum_Of_Columns * np.log(Sum_Of_Columns / sample_size)) / sample_size)\n",
    "    HXY = -np.sum(contingency_table * np.log(contingency_table / sample_size)) / sample_size\n",
    "    \n",
    "    # Calculate Effect Size UC \n",
    "    Uncertainty_Coefficient_Symmetric = 2 * (HX + HY - HXY) / (HX + HY)\n",
    "    Uncertainty_Coefficient_Rows = (HX + HY - HXY) / HX\n",
    "    Uncertainty_Coefficient_Columns = (HX + HY - HXY) / HY\n",
    "\n",
    "    # Calculate the Asymptotic Standard Errors (Standard Errors)\n",
    "    Standard_Error_Symmetric = np.sqrt((4 * np.sum(contingency_table * (HXY * np.log(np.outer(Sum_Of_Rows, Sum_Of_Columns) / sample_size ** 2) - (HX + HY) * np.log(contingency_table / sample_size)) ** 2) / (sample_size ** 2 * (HX + HY) ** 4)))\n",
    "    Standard_Error_Rows = np.sqrt(np.sum(contingency_table * (HX * np.log(contingency_table / Sum_Of_Columns) + (HY - HXY) * np.log(Sum_Of_Rows / sample_size)[:, np.newaxis]) ** 2) / (sample_size ** 2 * HX ** 4))\n",
    "    Standard_Error_Columns = np.sqrt(np.sum(contingency_table * (HY * np.log(contingency_table / Sum_Of_Rows[:, np.newaxis]) + (HX - HXY) * np.log(Sum_Of_Columns / sample_size)) ** 2) / (sample_size ** 2 * HY ** 4))\n",
    "\n",
    "    # Calculate p_values        \n",
    "    Z_value_Symmetric = Uncertainty_Coefficient_Symmetric / Standard_Error_Symmetric\n",
    "    Z_value_Rows = Uncertainty_Coefficient_Rows / Standard_Error_Rows\n",
    "    Z_value_Columns = Uncertainty_Coefficient_Columns / Standard_Error_Columns\n",
    "\n",
    "    # Confidence Intervals\n",
    "    Zcrit = 1 - (1 - confidence_level) / 2\n",
    "    Symmetric_CIs = Zcrit * np.sqrt(Standard_Error_Symmetric) * np.array([-1, 1]) + Uncertainty_Coefficient_Symmetric\n",
    "    Rows_CIs = Zcrit * np.sqrt(Standard_Error_Rows) * np.array([-1, 1]) + Uncertainty_Coefficient_Rows\n",
    "    Columns_CIs = Zcrit * np.sqrt(Standard_Error_Columns) * np.array([-1, 1]) + Uncertainty_Coefficient_Columns\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Uncertainty Coefficient (Symmetric)\"] = Uncertainty_Coefficient_Symmetric\n",
    "    results[\"Uncertainty Coefficient (Rows)\"] = Uncertainty_Coefficient_Rows\n",
    "    results[\"Uncertainty Coefficient (Columns)\"] = Uncertainty_Coefficient_Columns\n",
    "    results[\"Uncertainty Coefficient Standard Error (Symmetric)\"] = Standard_Error_Symmetric\n",
    "    results[\"Uncertainty Coefficient Standard Error (Rows)\"] = Standard_Error_Rows\n",
    "    results[\"Uncertainty Coefficient Standard Error (Columns)\"] = Standard_Error_Columns\n",
    "    results[\"Uncertainty Coefficient Z-value (Symmetric)\"] = Z_value_Symmetric\n",
    "    results[\"Uncertainty Coefficient Z-value (Rows)\"] = Z_value_Rows\n",
    "    results[\"Uncertainty Coefficient Z-value (Columns)\"] = Z_value_Columns\n",
    "    results[\"Uncertainty Coefficient Confidence Intervals (Symmetric)\"] = Symmetric_CIs\n",
    "    results[\"Uncertainty Coefficient Confidence Intervals (Rows)\"] = Rows_CIs\n",
    "    results[\"Uncertainty Coefficient Confidence Intervals (Columns)\"] = Columns_CIs\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "### 4. likelihood_ratio\n",
    "\n",
    "def likelihood_ratio(contingency_table):\n",
    "        Observed = contingency_table\n",
    "        Sum_Product = np.sum(Observed)\n",
    "        Expected = np.outer(np.sum(Observed, axis=1), np.sum(Observed, axis=0)) / Sum_Product\n",
    "        likelihood_ratio = 0\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                likelihood_ratio += Observed[i, j] * np.log(Observed[i, j] / Expected[i, j])\n",
    "        likelihood_ratio = 2 * likelihood_ratio\n",
    "        p_value = 1 - chi2.cdf(likelihood_ratio, 1)\n",
    "        return [likelihood_ratio, p_value]\n",
    "\n",
    "\n",
    "\n",
    "### 5. Berry_Mielke_Maximum_Corrected_Cramer_V\n",
    "def Berry_Mielke_Maximum_Corrected_Cramer_V(matrix):\n",
    "    Observed_Chi_Square = chi2_contingency(matrix).statistic\n",
    "    r, c = matrix.shape \n",
    "    sum_of_rows_vector = matrix.sum(axis=1)  \n",
    "    sum_of_cols_vector = matrix.sum(axis=0)  \n",
    "    NR = sum_of_rows_vector.sum()  \n",
    "    NC = sum_of_cols_vector.sum() \n",
    "\n",
    "    matrix = np.zeros((r, c)) \n",
    "    x = np.where(np.isin(sum_of_cols_vector, sum_of_rows_vector), np.argmax(np.isin(sum_of_rows_vector, sum_of_cols_vector), axis=0) + 1, np.nan)\n",
    "    y = np.where(np.isin(sum_of_rows_vector, sum_of_cols_vector), np.argmax(np.isin(sum_of_cols_vector, sum_of_rows_vector), axis=0) + 1, np.nan)\n",
    "    x = x[~np.isnan(x)].astype(int) - 1  \n",
    "    y = y[~np.isnan(y)].astype(int) - 1  \n",
    "\n",
    "    matrix[x,y] = sum_of_rows_vector[x]\n",
    "    sum_of_rows_vector[x] = sum_of_rows_vector[x]-sum_of_rows_vector[x] \n",
    "    sum_of_cols_vector[y] = sum_of_cols_vector[y]-sum_of_cols_vector[y] \n",
    "\n",
    "    while NR > 0 and NC > 0:\n",
    "        NR = np.sum(sum_of_rows_vector)  \n",
    "        NC = np.sum(sum_of_cols_vector)  \n",
    "        x = np.argmax(sum_of_rows_vector)  \n",
    "        y = np.argmax(sum_of_cols_vector)  \n",
    "        z = min(sum_of_rows_vector[x], sum_of_cols_vector[y]) \n",
    "        matrix[x, y] = z\n",
    "        sum_of_rows_vector[x] -= z\n",
    "        sum_of_cols_vector[y] -= z\n",
    "\n",
    "    Maximum_Chi_Square = chi2_contingency(matrix).statistic\n",
    "    Maximum_Corrected_Cramers_v = Observed_Chi_Square/ Maximum_Chi_Square\n",
    "\n",
    "    return Maximum_Corrected_Cramers_v\n",
    "\n",
    "\n",
    "### 6. Freemans_Theta_for_Nominal_by_Ordinal correlation\n",
    "def Fremmans_Theta(Contingency_Table):\n",
    "    contingency_table = pd.DataFrame(Contingency_Table)\n",
    "    row_names = contingency_table.index\n",
    "    vectors = {}\n",
    "    contrasts = {}\n",
    "\n",
    "    for i in range(len(row_names)):\n",
    "        for j in range(i + 1, len(row_names)):\n",
    "            row1 = contingency_table.loc[row_names[i]].values\n",
    "            row2 = contingency_table.loc[row_names[j]].values\n",
    "            vector = np.multiply.outer(row1, row2)\n",
    "            key = f'{row_names[i]}_{row_names[j]}'\n",
    "            vectors[key] = vector\n",
    "\n",
    "    for i in range(len(row_names)):\n",
    "        for j in range(i + 1, len(row_names)):\n",
    "            vector = vectors[f'{row_names[i]}_{row_names[j]}']\n",
    "            contrast = np.sum(np.triu(vector)) - np.sum(np.tril(vector))\n",
    "            key = f'{row_names[i]}_{row_names[j]}'\n",
    "            contrasts[key] = contrast\n",
    "\n",
    "    Delta = np.sum(np.abs(list(contrasts.values())))\n",
    "\n",
    "    row_sums = contingency_table.sum(axis=1).values\n",
    "    T2 = np.sum(np.triu(np.outer(row_sums, row_sums), k=1))\n",
    "    Theta = Delta / T2\n",
    "\n",
    "    return Theta\n",
    "\n",
    "#### 7. Kendall's Tau versions\n",
    "def Kendall_Tau_Family(contingency_table, confidence_level = 0.95): #inspired by Desctools function\n",
    "    number_of_rows, number_of_columns = contingency_table.shape\n",
    "    concordant_table = np.zeros((number_of_rows, number_of_columns))\n",
    "    disconcordant_table = np.zeros((number_of_rows, number_of_columns))\n",
    "    for i in range(number_of_rows):\n",
    "        for j in range(number_of_columns):\n",
    "            concordant_table[i, j] = np.sum(contingency_table[:i, :j]) + np.sum(contingency_table[i+1:, j+1:])\n",
    "            disconcordant_table[i, j] = np.sum(contingency_table[:i, j+1:]) + np.sum(contingency_table[i+1:, :j])\n",
    "    Concordant_pairs, Disconcordant_pairs = np.sum(concordant_table * contingency_table) / 2, np.sum(disconcordant_table * contingency_table) / 2\n",
    "    sample_size = np.sum(contingency_table)\n",
    "    number_of_total_pairs = sample_size * (sample_size-1)/2  \n",
    "    sum_of_the_rows = contingency_table.sum(axis=1)\n",
    "    sum_of_the_cols = contingency_table.sum(axis=0)\n",
    "    Number_of_ties_rows = np.sum(sum_of_the_rows * (sum_of_the_rows-1)/2)\n",
    "    Number_of_ties_columns = np.sum(sum_of_the_cols * (sum_of_the_cols-1)/2)\n",
    "    low_rc = (min(number_of_rows, number_of_columns)) # choose whethehr there are less row or coloumns\n",
    "\n",
    "    Kendall_tau_a = (Concordant_pairs - Disconcordant_pairs) / number_of_total_pairs\n",
    "    Kendall_tau_b = (Concordant_pairs - Disconcordant_pairs) / np.sqrt((number_of_total_pairs-Number_of_ties_rows) * (number_of_total_pairs-Number_of_ties_columns))\n",
    "    Kendall_tau_c = 2 * (Concordant_pairs - Disconcordant_pairs) / (sample_size**2 *  ((low_rc - 1) / low_rc)) \n",
    "\n",
    "    # Standard Errors of tau\n",
    "    Term1 = (concordant_table - disconcordant_table) * (contingency_table !=0)\n",
    "    Term2 = np.sum(Term1[Term1 != 0]) / sample_size\n",
    "    Standard_Error_Tau_a = np.sqrt(2 / (sample_size * (sample_size - 1)) * ((2 * (sample_size - 2)) / (sample_size * (sample_size - 1)**2) * np.sum((Term1 - Term2)**2) + 1 - Kendall_tau_a**2))\n",
    "    Proportions_Tables = contingency_table / sample_size\n",
    "    Proportions_Difference = (concordant_table - disconcordant_table) / sample_size\n",
    "    sum_of_proportions_rows = np.sum(Proportions_Tables, axis=1)\n",
    "    sum_of_proportions_columns = np.sum(Proportions_Tables, axis=0)\n",
    "    proportions_matrix_row = (np.tile(sum_of_proportions_rows, (contingency_table.shape[1], 1)).T) * (2 * (Concordant_pairs - Disconcordant_pairs) / sample_size**2)\n",
    "    proportions_matrix_columns = np.tile(sum_of_proportions_columns, (contingency_table.shape[0], 1)) * (2 * (Concordant_pairs - Disconcordant_pairs) / sample_size**2)\n",
    "    delta1 = np.sqrt(1 - np.sum(sum_of_proportions_rows**2))\n",
    "    delta2 = np.sqrt(1 - np.sum(sum_of_proportions_columns**2))\n",
    "    Term3 = (2 * Proportions_Difference + proportions_matrix_columns) * delta2 * delta1 + (proportions_matrix_row * delta2) / delta1\n",
    "    Standard_Error_Tau_b = np.sqrt(((np.sum(Proportions_Tables * Term3**2) - np.sum(Proportions_Tables * Term3)**2) / (delta1 * delta2)**4) / sample_size)\n",
    "    Standard_Error_Tau_c = np.sqrt(4 * low_rc**2/((low_rc - 1)**2 * sample_size**4) * (np.sum(contingency_table * (concordant_table - disconcordant_table)**2) - 4 * (Concordant_pairs - Disconcordant_pairs)**2/sample_size))\n",
    "\n",
    "    # Approximate Confidence Intervals Kendall's Tau\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    Confidence_Interval_Tau_a = (Kendall_tau_a - zcrit * Standard_Error_Tau_a,Kendall_tau_a + zcrit * Standard_Error_Tau_a)\n",
    "    Confidence_Interval_Tau_b = (Kendall_tau_b - zcrit * Standard_Error_Tau_b,Kendall_tau_b + zcrit * Standard_Error_Tau_b)\n",
    "    Confidence_Interval_Tau_c = (Kendall_tau_c - zcrit * Standard_Error_Tau_c,Kendall_tau_c + zcrit * Standard_Error_Tau_c)\n",
    "\n",
    "    Z_Score_Tau_a = Kendall_tau_a / Standard_Error_Tau_a\n",
    "    Z_Score_Tau_b = Kendall_tau_b / Standard_Error_Tau_b\n",
    "    Z_Score_Tau_c = Kendall_tau_c / Standard_Error_Tau_c\n",
    "\n",
    "    p_value_Tau_a = scipy.stats.t.sf((abs(Z_Score_Tau_a)), 100000) * 2\n",
    "    p_value_Tau_b = scipy.stats.t.sf((abs(Z_Score_Tau_b)), 100000) * 2\n",
    "    p_value_Tau_c = scipy.stats.t.sf((abs(Z_Score_Tau_c)), 100000) * 2\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Kendalls Tau-a:\"]= Kendall_tau_a\n",
    "    results[\"Kendalls Tau-a Standard Error\"] = Standard_Error_Tau_a\n",
    "    results[\"Kendalls Tau-a Z-score\"] = Z_Score_Tau_a\n",
    "    results[\"Kendalls Tau-a p-value\"]= p_value_Tau_a\n",
    "    results[\"Kendalls Tau-a Confidence Intervals\"]= Confidence_Interval_Tau_a\n",
    "    results[\"Kendalls Tau-b:\"]= Kendall_tau_b\n",
    "    results[\"Kendalls Tau-b Standard Error\"] = Standard_Error_Tau_b\n",
    "    results[\"Kendalls Tau-b Z-score\"] = Z_Score_Tau_b\n",
    "    results[\"Kendalls Tau-b p-value\"]= p_value_Tau_b\n",
    "    results[\"Kendalls Tau-b. Confidence Intervals\"]= Confidence_Interval_Tau_b\n",
    "    results[\"Kendalls Tau-c\"]= Kendall_tau_c\n",
    "    results[\"Kendalls Tau-c Standard Error\"] = Standard_Error_Tau_c\n",
    "    results[\"Kendalls Tau-c Z-score\"] = Z_Score_Tau_c\n",
    "    results[\"Kendalls Tau-c p-value\"]= p_value_Tau_c\n",
    "    results[\"Kendalls Tau-c Confidence Intervals\"]= Confidence_Interval_Tau_c\n",
    "\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "# 8. Rsquare Estimation\n",
    "def Rsquare_Estimation(Rsquare, sample_size, Number_Of_Predictors):\n",
    "    dftotal = sample_size-1\n",
    "    df_residual = sample_size - Number_Of_Predictors - 1\n",
    "    df = sample_size - Number_Of_Predictors\n",
    "\n",
    "    # Adjusted R square\n",
    "    term1 = (sample_size - 3) * (1 - Rsquare) / df_residual\n",
    "    Smith = 1 - (sample_size / (df)) * (1 - Rsquare) # Smith, 1929\n",
    "    Ezekiel = 1 - (dftotal / df_residual) * (1 - Rsquare) #Ezekiel, 1930\n",
    "    Wherry = 1 - (dftotal / df) * (1 - Rsquare) #Wherry, 1931\n",
    "    olkin_pratt = 1 - term1 * special.hyp2f1(1, 1, (df + 1) / 2, 1 - Rsquare) #Olkin & Pratt, 1958\n",
    "    Cattin = 1 - term1 * ((1 + (2 * (1 - Rsquare)) / df_residual) + ((8 * (1 - Rsquare) ** 2) / (df_residual * (df + 3)))) # Cattin, 1980 (Approximation to Olkin and Pratt)\n",
    "    Pratt = 1 - (((sample_size - 3) * (1 - Rsquare)) / df_residual) * (1 + (2 * (1 - Rsquare)) / (df - 2.3)) # Pratt, 1964\n",
    "    Herzberg = 1 - (((sample_size - 3) * (1 - Rsquare)) / df_residual) * (1 + (2 * (1 - Rsquare)) / (df+1)) # Herzberg, 1969\n",
    "    Claudy =   1 - (((sample_size - 4) * (1 - Rsquare)) / df_residual) * (1 + (2 * (1 - Rsquare)) / (df+1)) # Herzberg, 1978\n",
    "    def Alf_Graf_MLE(Rsquare, sample_size, Number_Of_Predictors):     # Alf and Graf 2002, MLE\n",
    "        return opt.minimize_scalar(lambda rho: (1 - rho) ** (sample_size / 2) * (special.hyp2f1(0.5 * sample_size, 0.5 * sample_size, 0.5 * Number_Of_Predictors, rho * Rsquare)) * -1,bounds=(0, 1),method='bounded').x\n",
    "    AlfGraf = Alf_Graf_MLE(Rsquare, sample_size, Number_Of_Predictors)\n",
    "\n",
    "    # Squared Cross-Validity Coefficient\n",
    "    Lord = 1 - (sample_size+Number_Of_Predictors+1) / (sample_size-Number_Of_Predictors-1) * (1-Rsquare) #Uhl & Eisenberg, 1970, also known as the Lord formula (it is most cited by this name)\n",
    "    Lord_Nicholson = 1 - ((sample_size+Number_Of_Predictors +1) / sample_size) * (dftotal/df_residual) * (1-Rsquare) \n",
    "    Darlington_Stein = 1 - ((sample_size + 1) / sample_size) * (dftotal/df_residual) * ((sample_size-2) / (df-2)) * (1-Rsquare)\n",
    "    Burket = (sample_size*Rsquare - Number_Of_Predictors) / (np.sqrt(Rsquare)*df) \n",
    "    Brown_Large_Sample = (((df - 3) * Ezekiel**2 +     Ezekiel)     / ((sample_size-2*Number_Of_Predictors - 2) * Ezekiel + Number_Of_Predictors))\n",
    "    Brown_small_Sample = (((df - 3) * olkin_pratt**2 + olkin_pratt) / ((sample_size-2*Number_Of_Predictors - 2) * olkin_pratt + Number_Of_Predictors))\n",
    "    Rozeboom = 1 - ((sample_size+Number_Of_Predictors) / df) * (1 - Rsquare) #Rozeboom, 1978\n",
    "    Rozeboom2_Large_Sample = Ezekiel * ((1 + (Number_Of_Predictors/ (df-2)) * ((1-Ezekiel)/Ezekiel))**-1) #Rozeboom, 1981 \n",
    "    Rozeboom2_small_Sample = olkin_pratt * ((1 + (Number_Of_Predictors/ (df-2)) * ((1-olkin_pratt)/olkin_pratt))**-1) #Rozeboom, 1981\n",
    "    Claudy1_Large_Sample = (2* Ezekiel - (Rsquare))\n",
    "    Claudy1_Small_Sample = (2* olkin_pratt - (Rsquare))\n",
    "\n",
    "    results = {\n",
    "        \"Smith (1928)\": Smith,\n",
    "        \"Ezekiel (1930)\": Ezekiel,\n",
    "        \"Wherry (1931)\": Wherry,\n",
    "        \"Olkin & Pratt (1958)\": olkin_pratt,\n",
    "        \"Olkin & Pratt, Pratt's Approximation (1964)\": Pratt,\n",
    "        \"Olkin & Pratt, Herzberg's Approximation (1968)\": Herzberg,\n",
    "        \"Olkin & Pratt, Claudy's Approximation (1978)\": Claudy,\n",
    "        \"Olkin & Pratt, Cattin's Approximation (1980)\": Cattin,\n",
    "        \"Alf and Graf MLE (2002)\": AlfGraf,\n",
    "        \"Lord(1950)\" \n",
    "        \"Lord_Nicholson(1960)\": Lord_Nicholson,\n",
    "        \"Darlington/Stein (1967/1960)\": Darlington_Stein,\n",
    "        \"Burket (1964)\": Burket,\n",
    "        \"Brown_Large_Samples (1975)\": Brown_Large_Sample,\n",
    "        \"Brown_Small_Samples (1975)\": Brown_small_Sample,\n",
    "        \"Rozeboom I (1978)\": Rozeboom,\n",
    "        \"Rozeboom II Large Samples- (1978)\": Rozeboom2_Large_Sample,\n",
    "        \"Rozeboom II Small Samples (1978)\": Rozeboom2_small_Sample, \n",
    "        \"Claudy-I, Large Samples (1978)\": Claudy1_Large_Sample,\n",
    "        \"Claudy-I, Small Samples (1978)\": Claudy1_Small_Sample,   \n",
    "    }\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "# 9. Coefficeint Eta (Correlation Ratio)\n",
    "def Eta_Correlation_Ratio(x,y,confidence_level):\n",
    "    ss_between_x_independent = np.sum([(len(y[x == value]) * (np.mean(y[x == value]) - np.mean(y)) ** 2) for value in np.unique(x)])\n",
    "    ss_total_x_indpednent = np.sum((y - np.mean(y)) ** 2)\n",
    "    eta_x_independent = np.sqrt(ss_between_x_independent/ss_total_x_indpednent)\n",
    "    ss_between_y_independent = np.sum([(len(x[y == value]) * (np.mean(x[y == value]) - np.mean(x)) ** 2) for value in np.unique(y)])\n",
    "    ss_total_y_indpednent = np.sum((x - np.mean(x)) ** 2)\n",
    "    eta_y_independent = np.sqrt(ss_between_y_independent/ss_total_y_indpednent)\n",
    "\n",
    "    # Attenuated Corrected Eta by Metsamuronen (2023):\n",
    "    x_sorted = x[np.argsort(x)]\n",
    "    y_sorted = y[np.argsort(y)]\n",
    "    ss_between_x_max = np.sum([(len(y_sorted[x_sorted == value]) * (np.mean(y_sorted[x_sorted == value]) - np.mean(y_sorted)) ** 2) for value in np.unique(x_sorted)])\n",
    "    eta_max_x_indpendent = np.sqrt(ss_between_x_max/ss_total_x_indpednent)\n",
    "\n",
    "    ss_between_y_max = np.sum([(len(x_sorted[y_sorted == value]) * (np.mean(x_sorted[y_sorted == value]) - np.mean(x_sorted)) ** 2) for value in np.unique(y_sorted)])\n",
    "    eta_max_y_indpendent = np.sqrt(ss_between_y_max/ss_total_y_indpednent)\n",
    "\n",
    "    Attenuated_Corrected_Eta_X_Independent = (eta_x_independent/eta_max_x_indpendent)\n",
    "    Attenuated_Corrected_Eta_Y_Independent = (eta_y_independent/eta_max_y_indpendent)\n",
    "\n",
    "    results = {\n",
    "        \"Eta (Variable X is Indpendent)\": eta_x_independent,\n",
    "        \"Eta (Variable y is Indpendent)\": eta_y_independent,\n",
    "        \"Attenuated Correct Eta (Variable X is Indpendent)\": Attenuated_Corrected_Eta_X_Independent,\n",
    "        \"Attenuated Correct Eta (Variable y is Indpendent)\": Attenuated_Corrected_Eta_Y_Independent\n",
    "    }\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "# 10. Pearson Correlation\n",
    "def pearson_correlation(x,y,confidence_level = 0.95):\n",
    "    Pearson_Correlation, Pearson_pvalue =  scipy.stats.pearsonr(x,y)\n",
    "\n",
    "    sample_size = len(x)\n",
    "    Sample_1_Mean = np.mean(x)    \n",
    "    Sample_2_Mean = np.mean(y)\n",
    "    Sample_1_Standard_Deviation = np.std(x, ddof = 1)    \n",
    "    Sample_2_Standard_Deviation = np.std(y, ddof = 1)\n",
    "    Slope = (Sample_2_Standard_Deviation/Sample_1_Standard_Deviation) * Pearson_Correlation\n",
    "    constant = -(Slope * Sample_1_Mean - Sample_2_Mean)\n",
    "    Predicted_Values = Slope*np.array(x) + constant\n",
    "    Sum_of_square_deviations_from_mean_y = np.sum((y - Sample_2_Mean)**2) ##Sstotal\n",
    "    Sum_of_square_deviations_from_mean_x = np.sum((x - Sample_1_Mean)**2) ##Ssx\n",
    "    Sum_of_residuals = np.sum((y-Predicted_Values)**2) #SSres\n",
    "    sum_of_regression = Sum_of_square_deviations_from_mean_y - Sum_of_residuals #SSreg\n",
    "    Standard_error_slope = np.sqrt((1/(sample_size-2)) * (Sum_of_residuals/Sum_of_square_deviations_from_mean_x)) \n",
    "    t_score_of_the_slope = Slope / Standard_error_slope\n",
    "    Rsquare = (sum_of_regression/Sum_of_square_deviations_from_mean_y)\n",
    "    Approximated_r = Pearson_Correlation + (Pearson_Correlation*(1-Rsquare)) / (2*(sample_size-3))\n",
    "\n",
    "    # Approximated Standrd Errors - Gnambs, 2023\n",
    "    Fisher_SE = (1-Rsquare) / np.sqrt(sample_size*(1+Rsquare))  #Fisher, 1896\n",
    "    Fisher_Filon_SE = (1-Rsquare) / np.sqrt(sample_size)  #Fisher & Filon, 1898\n",
    "    Soper_1_SE = ((1-Rsquare) / np.sqrt(sample_size-1))  # Soper, 1913, Large Sample\n",
    "    Soper_2_SE = ((1-Rsquare)/np.sqrt(sample_size)) * ( 1 + (1+5.5*Rsquare) / (2*sample_size))  # Soper, 1913\n",
    "    Soper_3_SE = ((1-Rsquare)/np.sqrt(sample_size-1)) * (1 + (11*Rsquare) / (4*(sample_size-1)))  # Soper, 1913\n",
    "    Hoteling = ((1-Rsquare)/np.sqrt(sample_size-1)) * (1 + ((11*Rsquare) / (4*(sample_size-1))) + ((-192*Rsquare+479*Rsquare**2)/ (32*(sample_size-1)**2)))  # Hoteling, 1953\n",
    "    Ghosh_SE = np.sqrt((1 - ((sample_size - 2) * (1 - Rsquare) / (sample_size - 1) * hyp2f1(1, 1, (sample_size + 1) / 2, Rsquare)) - ((2 / (sample_size - 1) * (math.gamma(sample_size / 2) / math.gamma((sample_size - 1) / 2))**2) * np.sqrt(Rsquare) * hyp2f1(0.5, 0.5, (sample_size + 1) / 2, Rsquare))**2))\n",
    "    Hedges_SE = np.sqrt((Pearson_Correlation * scipy.special.hyp2f1(0.5, 0.5, (sample_size - 2) / 2, 1 - Rsquare))**2 - (1 - (sample_size - 3) * (1 - Rsquare) * scipy.special.hyp2f1(1, 1, sample_size / 2, 1 - Rsquare) / (sample_size - 2)))\n",
    "    Bonett_SE = (1-Rsquare) / np.sqrt(sample_size - 3)\n",
    "    Regression_SE = np.sqrt((1-Rsquare) / (sample_size - 2))\n",
    " \n",
    "    # Confidence Intervals:\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    tcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, sample_size-2)\n",
    "\n",
    "    # 1. Fisher (1921)\n",
    "    Zr = 0.5 * np.log((1+Pearson_Correlation) / (1-Pearson_Correlation)) # Fisher's Transformation\n",
    "    Standard_Error_ZR =  1 / (np.sqrt(sample_size-3)) #Fisher (1921) Approximation of the Variance\n",
    "    Z_Lower = Zr - zcrit * Standard_Error_ZR\n",
    "    Z_Upper = Zr + zcrit * Standard_Error_ZR\n",
    "    Pearosn_Fisher_CI_lower = (np.exp(2*Z_Lower) - 1) / (np.exp(2*Z_Lower) + 1)\n",
    "    Pearosn_Fisher_CI_upper = (np.exp(2*Z_Upper) - 1) / (np.exp(2*Z_Upper) + 1)\n",
    "\n",
    "    # 2. Olivoto et al., 2018 - Half Widthe Confidecne Interval\n",
    "    Half_Width = (0.45304**abs(Pearson_Correlation)) * 2.25152 * (sample_size**-0.50089)\n",
    "    Lower_Olivoto = Pearson_Correlation - Half_Width\n",
    "    Upperr_Olivoto = Pearson_Correlation + Half_Width\n",
    "\n",
    "    # 3. Olkin & Fin, 1995\n",
    "    Standard_Error_R_Sqaure = np.sqrt((4 * Rsquare * (1- Rsquare)**2*sample_size**2) / ((sample_size**2 -1)*(sample_size+3)))# Olkin & Finn, 1995\n",
    "    Pearson_CI_Olkin_Fin_Lower = Rsquare - zcrit * Standard_Error_R_Sqaure\n",
    "    Pearson_CI_Olkin_Fin_Upper = Rsquare + zcrit * Standard_Error_R_Sqaure\n",
    "\n",
    "    # Common Language Effect Sizes and more\n",
    "    Common_Language_Effect_Size_Dunlap = np.arcsin(Pearson_Correlation) / math.pi + 0.5\n",
    "    Counter_Null_EffectSize = np.sqrt(4*Rsquare/(1+3*Rsquare))    \n",
    "    Absolute_R_Square = 1 - (sum_of_regression / (np.sum((y - np.median(y))**2))) # Bertsimas et al., 2008\n",
    " \n",
    "    results = {}\n",
    "    results[\"Pearson Correlation\"] = (Pearson_Correlation)\n",
    "    results[\"t score\"] = round(t_score_of_the_slope, 4)\n",
    "    results[\"Degrees of Freedom\"] = round(sample_size-2, 4)\n",
    "    results[\"Pearson Correlation P-value\"] = round(Pearson_pvalue, 4)\n",
    "    results[\"Standrd Error of the Slope\"] = round(Standard_error_slope, 4)\n",
    "    results[\"Constant\"] = round(constant, 4)\n",
    "    results[\"Slope\"] = round(Slope, 4)\n",
    "\n",
    "    results[\"Standard Error Fisher \"] = round(Fisher_SE, 4)\n",
    "    results[\"Standard Error Fisher & Filon\"] = round(Fisher_Filon_SE, 4)\n",
    "    results[\"Standard Error Soper-I\"] = round(Soper_1_SE, 4)\n",
    "    results[\"Standard Error Soper-II\"] = round(Soper_2_SE, 4)\n",
    "    results[\"Standard Error Soper-III\"] = round(Soper_3_SE, 4)\n",
    "    results[\"Standard Error Hoteling\"] = round(Hoteling, 4)\n",
    "    results[\"Standard Error Ghosh\"] = round(Ghosh_SE, 4)\n",
    "    results[\"Standard Error Hedges\"] = round(Hedges_SE, 4)\n",
    "    results[\"Standard Error Bonett\"] = round(Bonett_SE, 4)    \n",
    "    results[\"Standard Error From Regression\"] = round(Regression_SE, 4)\n",
    "\n",
    "    results[\"Fisher's Zr\"] = (Zr)\n",
    "    results[\"Standard Error of Zr\"] = (Standard_Error_ZR)\n",
    "    results[\"Confidence Intervals Fisher (1921)\"] = f\"({round(Pearosn_Fisher_CI_lower, 4)}, {round(Pearosn_Fisher_CI_upper, 4)})\"\n",
    "    results[\"Confidence Intervals Olivoto\"] = f\"({round(Lower_Olivoto, 4)}, {round(Upperr_Olivoto, 4)})\"\n",
    "    results[\"Confidence Intervals for R-square (Olkin & Fin)\"] = f\"({round(Pearson_CI_Olkin_Fin_Lower, 4)}, {round(Pearson_CI_Olkin_Fin_Upper, 4)})\"\n",
    "\n",
    "\n",
    "    results[\"Common Language Effect Size (Dunlap, 1994)\"] = (Common_Language_Effect_Size_Dunlap)\n",
    "    results[\"Approximated r (Hedges & Olkin, 1985)\"] = (Approximated_r)\n",
    "    results[\"Counter_Null_EffectSize\"] = (Counter_Null_EffectSize)\n",
    "    results[\"Absolute R Square\"] = (Absolute_R_Square)\n",
    "\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "    \n",
    "# 11. Robust Alternatives to Pearson\n",
    "def Robust_Measures_Interval(x,y, confidence_level = 0.95): \n",
    "    \n",
    "    # A. Percentage Bend Correlation\n",
    "    n = len(x)\n",
    "    Omega_Hat_X= sorted((abs(np.array(x) - np.median(x))))[math.floor((1 - 0.2**2) * n) - 1]\n",
    "    psix = (x-np.array(np.median(x)))/Omega_Hat_X\n",
    "    i1 = sum((x_i - np.median(x)) / Omega_Hat_X < -1 for x_i in x)\n",
    "    i2 = sum((x_i - np.median(np.median(x))) / Omega_Hat_X > 1 for x_i in x)\n",
    "    Sx = np.sum(np.where(np.logical_or(psix < -1, psix > 1), 0, x))\n",
    "    Phi_x = (Omega_Hat_X * (i2 - i1) + Sx) / (n - i1 - i2)\n",
    "    Ai = np.clip((x - Phi_x) / Omega_Hat_X, -1, 1)\n",
    "\n",
    "    Omega_Hat_Y = sorted((abs(np.array(y) - np.median(y))))[math.floor((1 - 0.2**2) * n) - 1]\n",
    "    psiy = (y - np.array(np.median(y))) / Omega_Hat_Y\n",
    "    i1_y = sum((y_i - np.median(y)) / Omega_Hat_Y < -1 for y_i in y)\n",
    "    i2_y = sum((y_i - np.median(np.median(y))) / Omega_Hat_Y > 1 for y_i in y)\n",
    "    Sy = np.sum(np.where(np.logical_or(psiy < -1, psiy > 1), 0, y))\n",
    "    Phi_y = (Omega_Hat_Y * (i2_y - i1_y) + Sy) / (n - i1_y - i2_y)\n",
    "    Bi = np.clip((y - Phi_y) / Omega_Hat_Y, -1, 1)\n",
    "\n",
    "    Percentage_Bend_Correlation = np.sum(Ai * Bi)/np.sqrt(np.sum(Ai**2) * np.sum(Bi**2))\n",
    "    T_Statistic_PB = Percentage_Bend_Correlation * np.sqrt((n - 2) / (1 - Percentage_Bend_Correlation**2))\n",
    "    P_value_PB  = 2 * (1 - stats.t.cdf(np.abs(T_Statistic_PB), n - 2))\n",
    "\n",
    "    # B. Winsorized Correlation\n",
    "    lower_items = int(np.floor(0.2 * n)) + 1\n",
    "    upper_items = len(x) - lower_items + 1\n",
    "    sorted_x = np.sort(x)\n",
    "    sorted_y = np.sort(y)\n",
    "\n",
    "    Winzorized_X = np.where((x <= sorted_x[lower_items - 1]) | (x >= sorted_x[upper_items - 1]),np.where(x <= sorted_x[lower_items - 1], sorted_x[lower_items - 1], sorted_x[upper_items - 1]),x)\n",
    "    Winzorized_Y = np.where((y <= sorted_y[lower_items - 1]) | (y >= sorted_y[upper_items - 1]),np.where(y <= sorted_y[lower_items - 1], sorted_y[lower_items - 1], sorted_y[upper_items - 1]),y)\n",
    "    Winzorized_Correlation = np.corrcoef(Winzorized_X, Winzorized_Y)[0, 1]\n",
    "    T_Statistic_WC = Winzorized_Correlation * np.sqrt((n - 2) / (1 - Winzorized_Correlation**2))\n",
    "    degrees_of_freedom_WC = n - 2 * int(np.floor(0.2 * n)) - 2\n",
    "    P_Value_Winzoried = 2 * (1 - t.cdf(np.abs(T_Statistic_WC), df=degrees_of_freedom_WC))\n",
    "\n",
    "    # C. Biweight Midcorrelation (Verified with asbio for r)\n",
    "    Ui = (x - np.median(x)) / (9 * t.ppf(0.75, 100000) * 1.4826*np.median(np.abs(x- np.median(x))))\n",
    "    Vi = (y - np.median(y)) / (9 * t.ppf(0.75, 100000) * 1.4826*np.median(np.abs(y - np.median(y))))\n",
    "    Ai = np.where((Ui <= -1) | (Ui >= 1), 0, 1)\n",
    "    Bi = np.where((Vi <= -1) | (Vi >= 1), 0, 1)\n",
    "\n",
    "    Sxx = (np.sqrt(n) * np.sqrt(sum((Ai * ((x - np.median(x))**2)) * ((1 - Ui**2)**4))) /  abs(sum(Ai * (1 - Ui**2) * (1 - 5 * Ui**2))))**2\n",
    "    Syy = (np.sqrt(n) * np.sqrt(sum((Bi * ((y - np.median(y))**2)) * ((1 - Vi**2)**4))) / (abs(sum(Bi * (1 - Vi**2) * (1 - 5 * Vi**2)))))**2\n",
    "    Sxy = n * sum((Ai *(x - np.median(x))) * ((1-Ui**2)**2) * (Bi* (y-np.median(y))) * ((1-Vi**2)**2)) / (sum((Ai* (1-Ui**2)) * (1-5*Ui**2)) * sum((Bi* (1-Vi**2)) * (1 - 5 * Vi**2)))\n",
    "    Biweight_midcorrelation = Sxy / (np.sqrt(Sxx * Syy))\n",
    "    T_statistic_BM = np.sqrt(n - 2) * np.abs(Biweight_midcorrelation) / np.sqrt(1 - Biweight_midcorrelation**2)\n",
    "    degrees_of_freedom_bm = n-2\n",
    "    P_value_BM = 2 * (1 - t.cdf(T_statistic_BM, degrees_of_freedom_bm))\n",
    "\n",
    "    # D. Gausian Rank Correlation \n",
    "    Normalized_X = norm.ppf((np.argsort(x) + 1) / (len(x) + 1))\n",
    "    Normalized_Y = norm.ppf((np.argsort(y) + 1) / (len(y) + 1))\n",
    "    Gaussian_Rank_Correlation = pearsonr(Normalized_X, Normalized_Y)\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    Normalized_X = norm.ppf((rankdata(x) / (len(x) + 1)))\n",
    "    Normalized_Y = norm.ppf((rankdata(y) / (len(y) + 1)))\n",
    "    Gaussian_Rank_Correlation, GRS_pvalue = pearsonr(Normalized_X, Normalized_Y)\n",
    "    Zr = 0.5 * np.log((1+Gaussian_Rank_Correlation) / (1-Gaussian_Rank_Correlation)) # Fisher's Transformation\n",
    "    Standard_Error_ZR =  1 / (np.sqrt(n-3)) #Fisher (1921) Approximation of the Variance\n",
    "    Z_Lower = Zr - zcrit * Standard_Error_ZR\n",
    "    Z_Upper = Zr + zcrit * Standard_Error_ZR\n",
    "    Gaussian_Rank_Correlation_Fisher_CI_lower = (np.exp(2*Z_Lower) - 1) / (np.exp(2*Z_Lower) + 1)\n",
    "    Gaussian_Rank_Correlation_Fisher_CI_upper = (np.exp(2*Z_Upper) - 1) / (np.exp(2*Z_Upper) + 1)\n",
    "\n",
    "    results = {\n",
    "    \"Percentage Bend Correlation\": Percentage_Bend_Correlation,\n",
    "    \"Percentage Bend Statistic\": T_Statistic_PB,\n",
    "    \"Percentage Bend p-value)\": P_value_PB,\n",
    "    \"Winsorized Correlation\": Winzorized_Correlation}\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "# 12. Point Biserial Correlation\n",
    "# For equal and unequal sample sizes\n",
    "\n",
    "# 13. Skipped Correalation (from Pinguine)\n",
    "def skipped_Correlation(x, y, corr_type=\"spearman\"):\n",
    "    from pingouin.utils import _is_sklearn_installed\n",
    "\n",
    "    _is_sklearn_installed(raise_error=True)\n",
    "    from scipy.stats import chi2\n",
    "    from sklearn.covariance import MinCovDet\n",
    "\n",
    "    X = np.column_stack((x, y))\n",
    "    nrows, ncols = X.shape\n",
    "    gval = np.sqrt(chi2.ppf(0.975, 2))\n",
    "    center = MinCovDet(random_state=42).fit(X).location_\n",
    "    B = X - center\n",
    "    bot = (B**2).sum(axis=1)\n",
    "    dis = np.zeros(shape=(nrows, nrows))\n",
    "    for i in np.arange(nrows):\n",
    "        if bot[i] != 0:  # Avoid division by zero error\n",
    "            dis[i, :] = np.linalg.norm(B.dot(B[i, :, None]) * B[i, :] / bot[i], axis=1)\n",
    "\n",
    "    def idealf(x):\n",
    "        n = len(x)\n",
    "        j = int(np.floor(n / 4 + 5 / 12))\n",
    "        y = np.sort(x)\n",
    "        g = (n / 4) - j + (5 / 12)\n",
    "        low = (1 - g) * y[j - 1] + g * y[j]\n",
    "        k = n - j + 1\n",
    "        up = (1 - g) * y[k - 1] + g * y[k - 2]\n",
    "        return up - low\n",
    "\n",
    "    # One can either use the MAD or the IQR (see Wilcox 2012)\n",
    "    # MAD = mad(dis, axis=1)\n",
    "    iqr = np.apply_along_axis(idealf, 1, dis)\n",
    "    thresh = np.median(dis, axis=1) + gval * iqr\n",
    "    outliers = np.apply_along_axis(np.greater, 0, dis, thresh).any(axis=0)\n",
    "    # Compute correlation on remaining data\n",
    "    if corr_type == \"spearman\":\n",
    "        r, pval = spearmanr(X[~outliers, 0], X[~outliers, 1])\n",
    "    else:\n",
    "        r, pval = pearsonr(X[~outliers, 0], X[~outliers, 1])\n",
    "    return r, pval\n",
    "\n",
    "# 14. Robust Measures Ordinal correlation\n",
    "def Robust_Measures_Ordinal(x,y, confidence_level = 0.95): \n",
    "    # A. Sheperds Pi\n",
    "    def bsmahal(a, b, n_boot=200):\n",
    "        n, m = b.shape\n",
    "        MD = np.zeros((n, n_boot))\n",
    "        nr = np.arange(n)\n",
    "        xB = np.random.choice(nr, size=(n_boot, n), replace=True)\n",
    "        for i in np.arange(n_boot):\n",
    "            s1 = b[xB[i, :], 0]\n",
    "            s2 = b[xB[i, :], 1]\n",
    "            X = np.column_stack((s1, s2))\n",
    "            mu = X.mean(0)\n",
    "            _, R = np.linalg.qr(X - mu)\n",
    "            sol = np.linalg.solve(R.T, (a - mu).T)\n",
    "            MD[:, i] = np.sum(sol**2, 0) * (n - 1)\n",
    "        return MD.mean(1)\n",
    "\n",
    "    def shepherd(x, y, n_boot=200):\n",
    "        X = np.column_stack((x, y))\n",
    "        m = bsmahal(X, X, n_boot)\n",
    "        outliers = m >= 6\n",
    "        r, pval = spearmanr(x[~outliers], y[~outliers])\n",
    "        return r, pval\n",
    "    \n",
    "    #B. Gassuan Rank Correlation\n",
    "    Normalized_X = norm.ppf((np.argsort(x) + 1) / (len(x) + 1))\n",
    "    Normalized_Y = norm.ppf((np.argsort(y) + 1) / (len(y) + 1))\n",
    "    Gaussian_Rank_Correlation = pearsonr(Normalized_X, Normalized_Y)\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    Normalized_X = norm.ppf((rankdata(x) / (len(x) + 1)))\n",
    "    Normalized_Y = norm.ppf((rankdata(y) / (len(y) + 1)))\n",
    "    Gaussian_Rank_Correlation, GRC_pvalue = spearmanr(Normalized_X, Normalized_Y)\n",
    "\n",
    "\n",
    "# 15. Spearman Correlation\n",
    "def Spearman_Correlation(x,y,confidence_level): \n",
    "\n",
    "    spearman_rho_p_value = spearmanr(x,y)[1]\n",
    "\n",
    "    ranked_x = rankdata(x)\n",
    "    ranked_y = rankdata(y)\n",
    "\n",
    "    mean_sample_1 = np.mean(ranked_x)\n",
    "    mean_sample_2 = np.mean(ranked_y)\n",
    "    sample_size = len(x)\n",
    "\n",
    "    Spearman = (np.sum ((ranked_x- mean_sample_1) * (ranked_y - mean_sample_2))) / (np.sqrt(np.sum((ranked_x- mean_sample_1)**2)) * np.sqrt(np.sum((ranked_y- mean_sample_2)**2)))\n",
    "    fisher_Zrho = math.atanh(Spearman)\n",
    "\n",
    "    Bonett_Wright_Standard_Error = np.sqrt((1 + Spearman**2 / 2) / (sample_size - 3)) # 2000\n",
    "    Fieller_Standard_Error = 1.06 / (sample_size-3) # 1957\n",
    "    Caruso_and_Cliff = (1 / (sample_size - 2)) +  (abs(fisher_Zrho)/(6*sample_size + 4 *np.sqrt(sample_size)))# 1997\n",
    "\n",
    "    # Confidence Interval\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "\n",
    "    Lower_ci_BW = math.tanh(fisher_Zrho - zcrit * Bonett_Wright_Standard_Error)\n",
    "    Upper_ci_BW = math.tanh(fisher_Zrho + zcrit * Bonett_Wright_Standard_Error)\n",
    "\n",
    "    Lower_ci_Fieller = math.tanh(fisher_Zrho - zcrit * Fieller_Standard_Error)\n",
    "    Upper_ci_Fieller = math.tanh(fisher_Zrho + zcrit * Fieller_Standard_Error)\n",
    "\n",
    "    Lower_ci_CC = math.tanh(fisher_Zrho - zcrit * Caruso_and_Cliff)\n",
    "    Upper_ci_CC = math.tanh(fisher_Zrho + zcrit * Caruso_and_Cliff)\n",
    "\n",
    "    # Spearman Corrected for ties Oyeka and Nwankwo Chike 2014 \n",
    "    Expected_ranks_x = x.argsort().argsort() + 1\n",
    "    Expected_ranks_y = y.argsort().argsort() + 1\n",
    "    Difference_Rank_x = Expected_ranks_x - ranked_x\n",
    "    Difference_Rank_y = Expected_ranks_y - ranked_y\n",
    "    Product_x = Expected_ranks_x * Difference_Rank_x\n",
    "    Product_y = Expected_ranks_y * Difference_Rank_y\n",
    "    Di_x = Difference_Rank_x**2 /2\n",
    "    Di_y = Difference_Rank_y**2 /2\n",
    "    pi_x = non_zero_count = sum(1 for element in Difference_Rank_x if element != 0) / sample_size\n",
    "    pi_y = non_zero_count = sum(1 for element in Difference_Rank_y if element != 0) / sample_size\n",
    "    Multi_Ranked = ranked_x * ranked_y\n",
    "    term_x = (((sample_size*(sample_size**2-1))/12) - (2*pi_x * (sum(Product_x)-sum(Di_x))))\n",
    "    term_y = (((sample_size*(sample_size**2-1))/12) - (2*pi_y * (sum(Product_y)-sum(Di_y))))\n",
    "    Numerator = np.sum(Multi_Ranked) - ((sample_size*(sample_size+1)**2) / 4)  \n",
    "    Denomirator = np.sqrt(term_x*term_y)\n",
    "    Corrected_Spearman = Numerator / Denomirator\n",
    "\n",
    "    # Spearman Corrected for ties Taylor (1964)\n",
    "    d_square = np.sum((ranked_x - ranked_y)**2)\n",
    "    frequency_vector_x = np.array([count for count in Counter(x).values() if count > 1]) # return only repeating frequencies for ties correction \n",
    "    frequency_vector_y = np.array([count for count in Counter(y).values() if count > 1]) # return only repeating frequencies for ties correction \n",
    "    Tx = sum(frequency_vector_x**3-frequency_vector_x) / 12\n",
    "    Ty = sum(frequency_vector_y**3-frequency_vector_y) / 12\n",
    "    Spearman_Corrected_Taylor = 1 - (6 * (np.sum(d_square)+Tx+Ty)) / (sample_size*(sample_size**2-1))\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    results[\"Spearman\"] = Spearman\n",
    "    results[\"Spearman p-value\"] = spearman_rho_p_value\n",
    "    results[\"Ties Corrected Spearman (Oyeka and Nwankwo Chike, 2014) \"] = Corrected_Spearman\n",
    "    results[\"Ties Corrected Spearman (Taylor, 1964)\"] = Spearman_Corrected_Taylor\n",
    "\n",
    "    results[\"Standard Error (Bonett & Wright)\"] = Bonett_Wright_Standard_Error\n",
    "    results[\"Standard Error (Fieller)\"] = Fieller_Standard_Error\n",
    "    results[\"Standard Error (Caruso and_Cliff)\"] = Caruso_and_Cliff\n",
    "\n",
    "    results[\"Confidence Intervals (Bonett Wright)\"] = f\"({round(Lower_ci_BW, 4)}, {round(Upper_ci_BW, 4)})\"\n",
    "    results[\"Confidence Intervals (Fieller)\"] = f\"({round(Lower_ci_Fieller, 4)}, {round(Upper_ci_Fieller, 4)})\"\n",
    "    results[\"Confidence Intervals (and_Cliff)\"] = f\"({round(Lower_ci_CC, 4)}, {round(Upper_ci_CC, 4)})\"\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "# 16. Blomqvist_beta\n",
    "def Blomqvist_Beta(x,y,confidence_level):\n",
    "    sample_size = len(x)\n",
    "    Sorted_X_Values = rankdata(x) / (len(x) + 1)\n",
    "    Sorted_Y_Values = rankdata(y) / (len(y) + 1)\n",
    "    Sorted_Data = np.column_stack((Sorted_X_Values, Sorted_Y_Values))\n",
    "    Blomqvist_Beta = 2 * np.mean(np.all(Sorted_Data <= 0.5, axis=1) + np.all(Sorted_Data > 0.5, axis=1)) - 1\n",
    "    Statistic = math.atanh(Blomqvist_Beta) * np.sqrt(len(x))\n",
    "    p_value = 2 * norm.cdf(-abs(Statistic))\n",
    "    \n",
    "    results = {\n",
    "    \"Blomqvist_Beta\": Blomqvist_Beta,\n",
    "    \"Statistic\": Statistic,\n",
    "    \"p_value\": p_value}\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "# 17. Gini's Gama\n",
    "def ginis_gamma(x, y, confidence_level=0.95):\n",
    "    ranked_x = rankdata(x)\n",
    "    ranked_y = rankdata(y)\n",
    "    sample_size = len(x)\n",
    "\n",
    "    term1 = np.sum(np.abs((sample_size+1 - ranked_x) - ranked_y) - np.abs(ranked_x - ranked_y))\n",
    "    zcrit = t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "\n",
    "    if sample_size % 2 == 0:\n",
    "        index1 = 1 / (sample_size**2 / 2)\n",
    "        gamma = term1 * index1\n",
    "        ASE = np.sqrt((2 * (sample_size**2 + 2)) / (3 * (sample_size - 1) * (sample_size**2)))\n",
    "        Z = gamma / ASE\n",
    "        p_value = 2 * (1 - t.cdf(np.abs(Z), df=100000))  # Two-tailed test\n",
    "        result_ASE = ASE\n",
    "        lower_ci = gamma - ASE*zcrit\n",
    "        upper_ci = gamma + ASE*zcrit\n",
    "    else:\n",
    "        index2 = 1 / ((sample_size**2 - 1) / 2)\n",
    "        gamma = term1 * index2\n",
    "        ASE = np.sqrt((2 * (sample_size**2 + 3)) / (3 * (sample_size - 1) * (sample_size**2 - 1)))\n",
    "        Z = gamma / ASE\n",
    "        p_value = 2 * (1 - t.cdf(np.abs(Z), df=100000))  \n",
    "        result_ASE = ASE\n",
    "        lower_ci = gamma - ASE*zcrit\n",
    "        upper_ci = gamma + ASE*zcrit\n",
    "\n",
    "    return {\n",
    "        'Ginis Gamma': gamma,\n",
    "        'Standard Error': result_ASE,\n",
    "        'lower_ci': max(lower_ci, -1),  \n",
    "        'upper_ci': min(upper_ci, 1),   \n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "\n",
    "# 18. Polychoric Correlation\n",
    "\n",
    "# 19. Distance Correlation \n",
    "     \n",
    "# 20. Wilson e\n",
    "\n",
    "\n",
    "# Extra Hoffedings D   \n",
    "# Lin's concordance correlation coefficient\n",
    "\n",
    "\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "# Functions to measure the levels of agreement\n",
    "\n",
    "# 1. Cohens Kappa\n",
    "\n",
    "def calculate_p_value_from_z_score(score):\n",
    "    p_value = st.t.sf((abs(score)), 100000) * 2\n",
    "    return min(float(p_value), 0.99999)\n",
    "\n",
    "#Cohens Kappa fpr two raters\n",
    "contingency_table = np.array([[106,10,4],[22,28,10],[2,12,6]])\n",
    "#contingency_table = np.array([[4,4,2],[2,6,0],[0,2,0]])\n",
    "contingency_table = np.array([[10,4,1],[6,16,2],[0,3,8]])\n",
    "contingency_table2 = np.array([[20,5],[10,15]])\n",
    "\n",
    "confidence_level = 0.95\n",
    "\n",
    "def Cohens_Kappa_2_raters(contingency_table, confidence_level):\n",
    "    Sample_Size = np.sum(contingency_table)\n",
    "    Diagonal_of_Agreement = np.diag(contingency_table)\n",
    "    Sum_of_coloumns = np.sum(contingency_table, axis=0) / Sample_Size\n",
    "    Sum_of_rows = np.sum(contingency_table, axis=1) / Sample_Size\n",
    "    Disagreement_Table = np.outer(Sum_of_coloumns, Sum_of_rows)\n",
    "    Percentages_of_DissAgreement = np.trace(Disagreement_Table)\n",
    "    Percentages_of_Agreement = np.sum((Diagonal_of_Agreement)/ Sample_Size)\n",
    "    Cohens_Kappa = (Percentages_of_Agreement - Percentages_of_DissAgreement) / (1 - Percentages_of_DissAgreement)\n",
    "\n",
    "    # Calculate the Variance (Fleiss, Cohen & Everrit, 1969)\n",
    "    number_of_levels = contingency_table.shape[1]\n",
    "    Agreement_Matrix = np.eye(contingency_table.shape[1])\n",
    "    Probabilty_Matrix = (contingency_table/Sample_Size)\n",
    "    Probability_Coloumns = np.repeat(np.sum(Probabilty_Matrix, axis=1) * (1-Cohens_Kappa), repeats=number_of_levels)\n",
    "    Probability_Rows = np.repeat((np.sum(Probabilty_Matrix, axis=0) * (1 - Cohens_Kappa))[np.newaxis, :], repeats=number_of_levels, axis=0)\n",
    "    Final_Probabilities = np.transpose(Probabilty_Matrix).flatten() * (Agreement_Matrix.flatten() -  Probability_Coloumns - Probability_Rows.flatten())**2\n",
    "    Standart_Error_Kappa_Fleisse = np.sqrt((np.sum(Final_Probabilities) - (Cohens_Kappa - Percentages_of_DissAgreement * (1 - Cohens_Kappa))**2) / (1-Percentages_of_DissAgreement)**2 / Sample_Size)\n",
    "\n",
    "    # Calculate the Variance for H0 (Fleiss, Cohen & Everrit, 1969)\n",
    "    outer_mulitiplication_matrix = np.transpose(np.outer(Sum_of_rows, Sum_of_coloumns))\n",
    "    outer_addition_matrix = np.add.outer(Sum_of_rows, Sum_of_coloumns)\n",
    "    Variance_matrix = outer_mulitiplication_matrix * (Agreement_Matrix - outer_addition_matrix)**2\n",
    "    Standard_Error_H0 = np.sqrt((np.sum(Variance_matrix) - Percentages_of_DissAgreement**2) / (Sample_Size * (1-Percentages_of_DissAgreement)**2))\n",
    "\n",
    "    # Signficance\n",
    "    StatisticH0 = Cohens_Kappa / Standard_Error_H0\n",
    "    p_valueH0 = calculate_p_value_from_z_score(StatisticH0)\n",
    "    Statistic_Fleiss = Cohens_Kappa / Standart_Error_Kappa_Fleisse\n",
    "    p_value_Fleiss = calculate_p_value_from_z_score(Statistic_Fleiss)\n",
    "\n",
    "    # Confidence Interval\n",
    "    zcrit = st.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    Lower_Confidence_Interval_Kappa = Cohens_Kappa - Standart_Error_Kappa_Fleisse*zcrit\n",
    "    Upper_Confidence_Interval_Kappa = Cohens_Kappa + Standart_Error_Kappa_Fleisse*zcrit\n",
    "    Lower_Confidence_Interval_Kappa_H0 = Cohens_Kappa - Standard_Error_H0*zcrit\n",
    "    Upper_Confidence_Interval_Kappa_H0 = Cohens_Kappa + Standard_Error_H0*zcrit\n",
    "\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Cohens Kappa\"]= Cohens_Kappa\n",
    "    results[\"p_value\"] = p_valueH0\n",
    "    results[\"Statistic\"]= StatisticH0\n",
    "    results[\"Standard_Error_H0\"]= Standard_Error_H0\n",
    "    results[\"Standart_Error_Fleisse, Cohen & Everett\"] = Standart_Error_Kappa_Fleisse\n",
    "    results[\"p_value (Fleisse, Cohen & Everett)\"] = p_value_Fleiss\n",
    "    results[\"Statistic (Fleisse, Cohen & Everett)\"]= Statistic_Fleiss\n",
    "    results[\"Confidence Intervals Kappa\"] = f\"({round(Lower_Confidence_Interval_Kappa, 4)}, {round(Upper_Confidence_Interval_Kappa, 4)})\"\n",
    "    results[\"Confidence Intervals Kappa_H0\"] = f\"({round(Lower_Confidence_Interval_Kappa_H0, 4)}, {round(Upper_Confidence_Interval_Kappa_H0, 4)})\"\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "# 2. Scott's Agreement Index...# Note that we used the CI and significance with Z.  irrCAC (GPL) used the t distribution while we used the Z distribution as most packages do\n",
    "def scotts_pi(Contingency_Table, confidence_level):\n",
    "    Sample_Size = np.sum(Contingency_Table)\n",
    "    Number_Of_Levels = Contingency_Table.shape[1]\n",
    "\n",
    "    Weights = np.eye(Number_Of_Levels)\n",
    "    Percentages_of_Agreement = np.sum(Weights * Contingency_Table / Sample_Size)\n",
    "    Sum_of_Coloumns = np.sum(Contingency_Table, axis=0) / Sample_Size\n",
    "    Sum_of_Rows = np.transpose(np.sum(Contingency_Table, axis=1) / Sample_Size)\n",
    "    Joint_Distribution = (Sum_of_Coloumns + Sum_of_Rows) / 2\n",
    "    Percentage_Of_Disagreement = np.sum(Weights * np.outer(Joint_Distribution, Joint_Distribution))\n",
    "    Scotts_Pi = (Percentages_of_Agreement - Percentage_Of_Disagreement) / (1 - Percentage_Of_Disagreement)\n",
    "\n",
    "    # Calcaulate the Varaince\n",
    "    Probability_Table = Contingency_Table / Sample_Size\n",
    "    Probability_Rows = np.dot(Weights, Sum_of_Rows)\n",
    "    Probability_Coloumns = np.dot(np.transpose(Weights), Sum_of_Coloumns)\n",
    "    Joint_Probabilities = (Probability_Rows + Probability_Coloumns) / 2\n",
    "    Term1 = sum(Probability_Table[k, l] * (Weights[k, l] - (1 - Scotts_Pi) * (Joint_Probabilities[k] + Joint_Probabilities[l]))**2 for k in range(Number_Of_Levels) for l in range(Number_Of_Levels))\n",
    "    Standart_Error_Scotts_Pi = np.sqrt((1 / (Sample_Size * (1 - Percentage_Of_Disagreement)**2)) * (Term1 - (Percentages_of_Agreement - 2 * (1 - Scotts_Pi) * Percentage_Of_Disagreement)**2))\n",
    "\n",
    "    # Significance and confidence Intervals\n",
    "    Statistic = Scotts_Pi / Standart_Error_Scotts_Pi\n",
    "    p_value = calculate_p_value_from_z_score(Statistic)\n",
    "\n",
    "    # Confidence Interval\n",
    "    zcrit = st.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    Lower_Confidence_Interval_Scotts_Pi = Scotts_Pi - Standart_Error_Scotts_Pi*zcrit\n",
    "    Upper_Confidence_Interval_Kappa_Scotts_Pi = Scotts_Pi + Standart_Error_Scotts_Pi*zcrit\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Scott's Phi\"]= Scotts_Pi\n",
    "    results[\"Z-Statistic\"]= Statistic\n",
    "    results[\"p_value\"] = p_value\n",
    "    results[\"Standart Error Scott's Phi\"] = Standart_Error_Scotts_Pi\n",
    "    results[\"Confidence Intervals Kappa\"] = f\"({round(Lower_Confidence_Interval_Scotts_Pi, 4)}, {round(Upper_Confidence_Interval_Kappa_Scotts_Pi, 4)})\"\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return results\n",
    "\n",
    "\n",
    "# 3. Krippendorff alpha\n",
    "\n",
    "# 4. Spearman Foot rule (Check Berry)\n",
    "\n",
    "\n",
    "# 5. Bhapkar\n",
    "def BhapkarTest(contingency_table):\n",
    "    sample_size = np.sum(contingency_table)\n",
    "    Sum_Of_The_Rows = np.sum(contingency_table, axis=1)[:-1]\n",
    "    Sums_Of_The_Coloumns = np.sum(contingency_table, axis=0)[:-1]\n",
    "    Matrix_of_d_values = np.array([Sum_Of_The_Rows - Sums_Of_The_Coloumns] * len(Sums_Of_The_Coloumns)).T  # Adjusted here\n",
    "    Agreement_Matrix = np.zeros((contingency_table.shape[0] - 1, contingency_table.shape[1] - 1))\n",
    "    np.fill_diagonal(Agreement_Matrix, Sum_Of_The_Rows + Sums_Of_The_Coloumns)\n",
    "    Weights = contingency_table[:-1, :-1]\n",
    "    Variance_Covaraince_Matrix = Agreement_Matrix - Weights.T - Weights - (Matrix_of_d_values * Matrix_of_d_values.T) / sample_size  # Adjusted here\n",
    "    Inverse_Variance_Matrix = np.linalg.inv(Variance_Covaraince_Matrix)\n",
    "    Chisquare_Value = abs((Matrix_of_d_values @ Matrix_of_d_values.T @ Inverse_Variance_Matrix)[0,0])\n",
    "    Degrees_Of_Freedom = contingency_table.shape[0] - 1\n",
    "    p_value = 1 - chi2.cdf(Chisquare_Value, Degrees_Of_Freedom)\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    results[\"Chisquare_Value\"]= Chisquare_Value\n",
    "    results[\"Degrees_Of_Freedom\"]= Degrees_Of_Freedom\n",
    "    results[\"p_value\"] = p_value\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return results\n",
    "\n",
    "# 6. Fleiss Kappa\n",
    "\n",
    "\n",
    "# 7. Weighted Kappa\n",
    "def Cohens_Kappa_2_raters(contingency_table, confidence_level, weights_type='equal-spacing'):\n",
    "    number_of_levels = contingency_table.shape[1]\n",
    "    Sample_Size = np.sum(contingency_table)\n",
    "    Sum_of_coloumns = np.sum(contingency_table, axis=0) / Sample_Size\n",
    "    Sum_of_rows = np.sum(contingency_table, axis=1) / Sample_Size\n",
    "    if weights_type == 'equal-spacing':\n",
    "        Weights_Matrix = 1 - abs(np.subtract.outer(np.arange(1, number_of_levels+1), np.arange(1, number_of_levels+1))) / (number_of_levels-1)\n",
    "    elif weights_type == 'fleiss':\n",
    "        Weights_Matrix = 1 - (abs(np.subtract.outer(np.arange(1, number_of_levels+1), np.arange(1, number_of_levels+1))) / (number_of_levels-1))**2\n",
    "\n",
    "\n",
    "    Percentages_of_Agreement = np.sum(Weights_Matrix * contingency_table)/ Sample_Size\n",
    "    Percentages_Of_Disagreement = np.sum(Weights_Matrix * np.outer(Sum_of_rows, Sum_of_coloumns))\n",
    "    Cohens_Kappa = (Percentages_of_Agreement - Percentages_Of_Disagreement) / (1 - Percentages_Of_Disagreement)\n",
    "\n",
    "    # Calculate the Variance (Fleiss, Cohen & Everrit, 1969)\n",
    "    Agreement_Matrix = np.eye(contingency_table.shape[1])\n",
    "    Probabilty_Matrix = (contingency_table)/Sample_Size\n",
    "    Variance_Matrix = np.subtract((np.subtract(Weights_Matrix.T, (np.dot(Weights_Matrix, np.sum(Probabilty_Matrix, axis=1)) * (1 - Cohens_Kappa))).T), np.dot(Weights_Matrix, np.sum(Probabilty_Matrix, axis=0)) * (1 - Cohens_Kappa)).T\n",
    "    Standart_Error_Kappa_Fleisse = np.sqrt((np.sum(Probabilty_Matrix * Variance_Matrix**2) - (Cohens_Kappa - Percentages_Of_Disagreement * (1 - Cohens_Kappa))**2) / np.inner(1 - Percentages_Of_Disagreement, 1 - Percentages_Of_Disagreement) / Sample_Size)\n",
    "    \n",
    "    # Calculate the Variance of H0\n",
    "    Outer_Probability_Matrix = np.multiply.outer(Sum_of_coloumns, Sum_of_rows)\n",
    "    Sum_of_rows = np.sum(np.sum(Probabilty_Matrix,axis = 0) * Weights_Matrix,axis = 1)\n",
    "    Sum_of_coloumns = np.sum(np.sum(Probabilty_Matrix,axis = 1) * Weights_Matrix,axis = 1)\n",
    "    Weighted_Variance_Matrix = (Weights_Matrix - np.add.outer(Sum_of_rows,Sum_of_coloumns))**2\n",
    "    term = np.sum(Outer_Probability_Matrix * Weighted_Variance_Matrix)\n",
    "    Standard_Error_H0 = np.sqrt((term - Percentages_Of_Disagreement**2) / (Sample_Size* (1-Percentages_Of_Disagreement)**2))\n",
    "\n",
    "    # Signficance\n",
    "    StatisticH0 = Cohens_Kappa / Standart_Error_Kappa_Fleisse\n",
    "    p_valueH0 = calculate_p_value_from_z_score(StatisticH0)\n",
    "    Statistic_Fleiss = Cohens_Kappa / Standard_Error_H0\n",
    "    p_value_Fleiss = calculate_p_value_from_z_score(Statistic_Fleiss)\n",
    "\n",
    "    # Confidence Interval\n",
    "    zcrit = st.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    Lower_Confidence_Interval_Kappa = Cohens_Kappa - Standart_Error_Kappa_Fleisse*zcrit\n",
    "    Upper_Confidence_Interval_Kappa = Cohens_Kappa + Standart_Error_Kappa_Fleisse*zcrit\n",
    "    Lower_Confidence_Interval_Kappa_H0 = Cohens_Kappa - Standard_Error_H0*zcrit\n",
    "    Upper_Confidence_Interval_Kappa_H0 = Cohens_Kappa + Standard_Error_H0*zcrit\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Cohens Kappa\"]= Cohens_Kappa\n",
    "    results[\"p_value\"] = p_valueH0\n",
    "    results[\"Statistic\"]= StatisticH0\n",
    "    results[\"Standard_Error_H0\"]= Standard_Error_H0\n",
    "    results[\"Standart_Error_Fleisse, Cohen & Everett\"] = Standart_Error_Kappa_Fleisse\n",
    "    results[\"p_value (Fleisse, Cohen & Everett)\"] = p_value_Fleiss\n",
    "    results[\"Statistic (Fleisse, Cohen & Everett)\"]= Statistic_Fleiss\n",
    "    results[\"Confidence Intervals Kappa\"] = f\"({round(Lower_Confidence_Interval_Kappa, 4)}, {round(Upper_Confidence_Interval_Kappa, 4)})\"\n",
    "    results[\"Confidence Intervals Kappa_H0\"] = f\"({round(Lower_Confidence_Interval_Kappa_H0, 4)}, {round(Upper_Confidence_Interval_Kappa_H0, 4)})\"\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "# 8. Bennette Alpert and Goldstein S\n",
    "\n",
    "# 9. Brennan-Prediger\n",
    "\n",
    "# 10. More Binary Measures\n",
    "\n",
    "# 11. Aickin's Alpha\n",
    "\n",
    "# 12. Kendall's W\n",
    "\n",
    "# 13. Gwet's Family\n",
    "\n",
    "# 14. ICC\n",
    "\n",
    "# 16. Lin's CCC\n",
    "\n",
    "\n",
    "####################################################\n",
    "##### Relevant functions for proportion effect sizes\n",
    "####################################################\n",
    "\n",
    "def one_sample_proportion_effect_size(sample_size, proportion_sample, population_proportion = 0.5, confidence_level = 0.95):\n",
    "    cohens_g = abs(proportion_sample - population_proportion)\n",
    "    phi_sample = 2 *(np.arcsin(np.sqrt(proportion_sample)))\n",
    "    phi_population = 2 *(np.arcsin(np.sqrt(population_proportion)))\n",
    "    cohens_h = phi_sample - phi_population\n",
    "    Standard_Error_Wald = np.sqrt((proportion_sample * (1 - proportion_sample)) / sample_size)\n",
    "    Standard_Error_score = np.sqrt((population_proportion * (1 - population_proportion)) / sample_size)\n",
    "    z_score_wald = (proportion_sample - population_proportion) / Standard_Error_Wald\n",
    "\n",
    "    Number_of_Succeses_sample = proportion_sample*sample_size\n",
    "    Number_Of_Failures_sample = sample_size - Number_of_Succeses_sample\n",
    "    Number_of_Succeses_Population = population_proportion*sample_size\n",
    "    Number_Of_Failures_Population = sample_size - Number_of_Succeses_Population\n",
    "\n",
    "    z_score = (proportion_sample - population_proportion) / Standard_Error_score\n",
    "    correction = sample_size * population_proportion + 0.5\n",
    "    Z_score_wald_corrected = ((proportion_sample * sample_size) - correction) / np.sqrt((proportion_sample * (1 - proportion_sample) * sample_size))\n",
    "    Z_score_corrected = ((proportion_sample * sample_size) - correction) / np.sqrt((population_proportion * (1 - population_proportion) * sample_size))\n",
    "\n",
    "    p_value_wald = calculate_p_value_from_z_score(z_score_wald)\n",
    "    p_value_wald_corrected = calculate_p_value_from_z_score(Z_score_wald_corrected)\n",
    "    p_value_score = calculate_p_value_from_z_score(z_score)\n",
    "    p_value_score_corrected = calculate_p_value_from_z_score(Z_score_corrected)\n",
    "\n",
    "    # Risk Measures\n",
    "    Relative_Risk = proportion_sample/population_proportion\n",
    "    Odds_Ratio = (proportion_sample/1-proportion_sample) / (population_proportion/1-population_proportion)\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    results[\"Sample's Proportion\"] = proportion_sample\n",
    "    results[\"Population's Proportion\"] = population_proportion\n",
    "    results[\"Sample Size\"] = sample_size\n",
    "    results[\"Confidence Level\"] = confidence_level\n",
    "    results[\"Number of Successes (Sample)\"] = Number_of_Succeses_sample\n",
    "    results[\"Number of Failures (Sample)\"] = Number_Of_Failures_sample\n",
    "    results[\"Expected Number of Successes (Population)\"] = Number_of_Succeses_Population\n",
    "    results[\"Expected Number of Failures (Population)\"] = Number_Of_Failures_Population\n",
    "    results[\"Standard Error (Wald)\"] = round(Standard_Error_Wald, 4)\n",
    "    results[\"Standard Error (Score)\"] = round(Standard_Error_score, 4)\n",
    "    results[\"Z-score (Wald)\"] = round(z_score_wald, 4)\n",
    "    results[\"Z-score (Score)\"] = round(z_score, 4)\n",
    "    results[\"Z-score (Wald Corrected)\"] = round(Z_score_wald_corrected, 4)\n",
    "    results[\"Z-score (Score Corrected)\"] = round(Z_score_corrected, 4)\n",
    "    results[\"P-value (Wald)\"] = round(p_value_wald, 4)\n",
    "    results[\"P-value (Wald) Corrected\"] = round(p_value_wald_corrected, 4)\n",
    "    results[\"P-value (Score)\"] = round(p_value_score, 4)\n",
    "    results[\"P-value (Score) Corrected\"] = round(p_value_score_corrected, 4)\n",
    "    results[\"Cohen's g\"] = round(cohens_g, 4)\n",
    "    results[\"cohens_h\"] = round(cohens_h, 4)\n",
    "    results[\"Relative Risk\"] = round(Relative_Risk, 4)\n",
    "    results[\"Odds_Ratio\"] = round(Odds_Ratio, 4)\n",
    "\n",
    "\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "def proportion_of_hits(number_correct,number_of_trials, Number_of_Choices, confidence_level = 0.95): \n",
    "    Proportion_Correct = number_correct/number_of_trials\n",
    "    pi = (Proportion_Correct*(Number_of_Choices-1)) / (1+Proportion_Correct*(Number_of_Choices-2))\n",
    "    Standard_Error_pi = (1/np.sqrt(number_of_trials)) * (  (pi*(1-pi)) / (np.sqrt(Proportion_Correct*(1-Proportion_Correct))) ) \n",
    "    Z_score = (pi-0.5)/Standard_Error_pi\n",
    "    p_value = calculate_p_value_from_z_score(Z_score)\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    lower_CI = pi - zcrit * Standard_Error_pi\n",
    "    upper_CI = pi + zcrit * Standard_Error_pi\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results[\"Proportion of Hits []\"] = round(pi, 7)\n",
    "    results[\"Standard Error of \"] = round(Standard_Error_pi,7)\n",
    "    results[\"Z-score\"] = round(Z_score, 7)\n",
    "    results[\"p vlaue of \"] = round(p_value,7)\n",
    "    results[\"Confidence Intervals for \"] = f\"({round(lower_CI, 4)}, {round(upper_CI, 4)})\"\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "def one_sample_proportion_conf_int(p1, sample_size, confidence_level):\n",
    "    alpha = 1 - confidence_level\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "\n",
    "    # 1. Agresti-Coull\n",
    "    AC_CI = proportion_confint(p1, sample_size, alpha, method = \"agresti_coull\")\n",
    "    \n",
    "    # 2. Wald CI's\n",
    "    Wald_CI = proportion_confint(p1, sample_size, alpha, method = \"normal\")\n",
    "\n",
    "    # 3. Wald_Corrected \n",
    "    correction = 0.05/sample_size\n",
    "    Wald_Corrected = np.array(Wald_CI) + np.array([-correction, correction])\n",
    "    \n",
    "    # 4. Wilson\n",
    "    wilson_CI = proportion_confint(p1, sample_size, alpha, method = \"wilson\")\n",
    "    \n",
    "    # 5. Wilson Corrected\n",
    "    LowerCi_Wilson_Corrected=(2*p1 + zcrit**2-1-zcrit*np.sqrt(zcrit**2-2-1/sample_size + 4*(p1/sample_size)*(sample_size*(1-p1/sample_size)+1)))/(2*(sample_size +zcrit**2)) \n",
    "    UpperCi_Wilson_Corrected=(2*p1 + zcrit**2+1+zcrit*np.sqrt(zcrit**2+2-1/sample_size + 4*(p1/sample_size)*(sample_size*(1-p1/sample_size)-1)))/(2*(sample_size +zcrit**2))\n",
    "\n",
    "    # 6. Logit\n",
    "    lambdahat = math.log(p1/(sample_size-p1))\n",
    "    term1 = sample_size/(p1*(sample_size-p1))\n",
    "    lambdalow = lambdahat - zcrit*np.sqrt(term1)\n",
    "    lambdaupper = lambdahat + zcrit*np.sqrt(term1)\n",
    "    logitlower = math.exp(lambdalow)/(1 + math.exp(lambdalow))\n",
    "    logitupper = math.exp(lambdaupper)/(1 + math.exp(lambdaupper))\n",
    "\n",
    "    # 7. Jeffereys\n",
    "    lowerjeffreys = beta.ppf(alpha/2, p1+0.5, sample_size-p1+0.5)\n",
    "    upperjeffreys = beta.ppf(1-alpha/2, p1+0.5, sample_size-p1+0.5)\n",
    "\n",
    "    # 8. Clopper-Pearson CI's\n",
    "    lowerCP = beta.ppf(alpha/2, p1, sample_size-p1+1)\n",
    "    upperCP= beta.ppf(1-alpha/2, p1+1, sample_size-p1)\n",
    "\n",
    "    # 9. arcsine CI's 1 (Kulynskaya)\n",
    "    ptilde = (p1 + 0.375)/(sample_size + 0.75)\n",
    "    arcsinelower = math.sin(math.asin(np.sqrt(ptilde)) - 0.5*zcrit/np.sqrt(sample_size))**2\n",
    "    arcsineupper = math.sin(math.asin(np.sqrt(ptilde)) + 0.5*zcrit/np.sqrt(sample_size))**2\n",
    "\n",
    "    # 10. Pratt\n",
    "    A = ((p1 + 1)/(sample_size-p1))**2\n",
    "    B = 81 * (p1+1) * (sample_size-p1) - 9*sample_size - 8\n",
    "    C = -3 * zcrit * np.sqrt(9*(p1+1)*(sample_size - p1) * (9*sample_size + 5 - zcrit**2) + sample_size + 1)\n",
    "    D = 81 * (p1 + 1)**2 - 9 *(p1+1)* (2+zcrit**2) + 1\n",
    "    E = 1 + A * ((B+C)/D)**3\n",
    "    A2 = (p1/ (sample_size-p1-1)) **2\n",
    "    B2 = 81 * (p1) * (sample_size-p1-1) - 9*sample_size - 8\n",
    "    C2 = 3 * zcrit * np.sqrt(9*p1*(sample_size-p1-1) * (9*sample_size + 5 - zcrit**2) + sample_size + 1)\n",
    "    D2 = 81 * p1**2 - (9 *p1) * (2+zcrit**2) + 1\n",
    "    E2 = 1 + A2 * ((B2+C2)/D2)**3\n",
    "\n",
    "    upperPratt = 1/E    \n",
    "    lowerPratt = 1/E2\n",
    "\n",
    "    # 11. Blaker\n",
    "    def blakersCI(x, n, conf_level=0.95, tol=0.00001):\n",
    "    \n",
    "        def acceptance_probability(x, n, p):\n",
    "            probabilty1 = 1 - stats.binom.cdf(x - 1, n, p)\n",
    "            probabilty2 = stats.binom.cdf(x, n, p)\n",
    "            a1 = probabilty1 + stats.binom.cdf(stats.binom.ppf(probabilty1, n, p) - 1, n, p)\n",
    "            a2 = probabilty2 + 1 - stats.binom.cdf(stats.binom.ppf(1 - probabilty2, n, p), n, p)\n",
    "            return min(a1, a2)\n",
    "\n",
    "        CI_lower_blaker = stats.beta.ppf((1 - conf_level) / 2, x, n - x + 1)\n",
    "        CI_upper_blaker = stats.beta.ppf(1 - (1 - conf_level) / 2, x + 1, n - x)\n",
    "\n",
    "        while x != 0 and acceptance_probability(x, n, CI_lower_blaker + tol) < (1 - conf_level):\n",
    "            CI_lower_blaker += tol\n",
    "        while x != n and acceptance_probability(x, n, CI_upper_blaker - tol) < (1 - conf_level):\n",
    "            CI_upper_blaker -= tol\n",
    "\n",
    "        ci = [CI_lower_blaker, CI_upper_blaker]\n",
    "        \n",
    "        return ci\n",
    "    \n",
    "    CI_blakers = blakersCI(p1, sample_size, confidence_level)\n",
    "\n",
    "    \n",
    "    # 12. Mid-p\n",
    "    def calculate_midp(x, n, conf_level):\n",
    "        def f_low(pi):return 0.5 * binom.pmf(x, n, pi) + binom.cdf(x - 1, n, pi) - (1 + conf_level) / 2\n",
    "        def f_up(pi):return 0.5 * binom.pmf(x, n, pi) + binom.cdf(x - 1, n, pi) - (1 - conf_level) / 2\n",
    "        CI_lower_midp = newton(f_low, x/n)\n",
    "        CI_upper_midp = newton(f_up, x/n)\n",
    "        return CI_lower_midp, CI_upper_midp\n",
    "\n",
    "    midp_cis = calculate_midp(p1, sample_size, confidence_level)\n",
    "\n",
    "\n",
    "    results= {}\n",
    "    results[\"Agresti Coull CI's\"] = AC_CI\n",
    "    results[\"Wald CI\"] = Wald_CI\n",
    "    results[\"Wald CI Corrected\"] = Wald_Corrected\n",
    "    results[\"Wilson\"] = wilson_CI\n",
    "    results[\"Wilson Corrected\"] = LowerCi_Wilson_Corrected,UpperCi_Wilson_Corrected\n",
    "    results[\"logit\"] = logitlower,logitupper\n",
    "    results[\"Jeffereys\"] = lowerjeffreys, upperjeffreys\n",
    "    results[\"Clopper-Pearson\"] = lowerCP, upperCP\n",
    "    results[\"Arcsine\"] = arcsinelower, arcsineupper\n",
    "    results[\"Pratt\"] = lowerPratt, upperPratt\n",
    "    results[\"Blaker\"] = CI_blakers\n",
    "    results[\"Mid-p\"] = midp_cis\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def effect_sizes_difference_two_indpendednt_proportions(sample_size_1, sample_size_2, proportion_sample_1, proportion_sample_2,difference_in_population):\n",
    "    #todo - add population difference to the functions and inferntial statistics options\n",
    "    # Effect Sizes to be added\n",
    "    # 1. Point Biserial Correlation\n",
    "    # 2. Chamber's R (see Kirk 2005)\n",
    "    # 3. rBESD (see Rosenthal and Rosnow, 2003) \n",
    "\n",
    "    # Initial Definitions \n",
    "    p1 = proportion_sample_1 * sample_size_1\n",
    "    p2 = proportion_sample_2 * sample_size_2\n",
    "    q1 = sample_size_1 - p1\n",
    "    q2 = sample_size_2 - p2\n",
    "    sample_size = sample_size_1 + sample_size_2 \n",
    "\n",
    "    #Effect_Sizes\n",
    "    \n",
    "    #1. Cohens h\n",
    "    phi1 = 2*np.arcsin(np.sqrt(proportion_sample_1))\n",
    "    phi2 = 2*np.arcsin(np.sqrt(proportion_sample_2))\n",
    "    cohens_h =  abs(phi1 - phi2)\n",
    "    samples_difference = proportion_sample_1-proportion_sample_2\n",
    "     \n",
    "    #2. Calculate Probit_d and its Variance (see Wilson, 2017)\n",
    "    Z_score_Proportion_1 =  scipy.stats.t.ppf(proportion_sample_1, 100000)\n",
    "    Z_score_Proportion_2 =  scipy.stats.t.ppf(proportion_sample_2, 100000)\n",
    "    Probit_d = Z_score_Proportion_1 - Z_score_Proportion_2\n",
    "    #Add variance calculation for CI's\n",
    "\n",
    "    #3. Calculate Logit d\n",
    "    logit_d = np.log((proportion_sample_1/(1-proportion_sample_1))) - np.log((proportion_sample_2/(1-proportion_sample_2)))\n",
    "    # Add the variance for this\n",
    "\n",
    "    # 4. Cohens W\n",
    "    table = np.array([[p1,p2], [q1,q2]])\n",
    "    chi_square = scipy.stats.chi2_contingency(table).statistic\n",
    "    Cohens_w = np.sqrt(chi_square/sample_size)\n",
    "\n",
    "\n",
    "    #Inferntial_Statistics\n",
    "    continuiety_correction = (1/sample_size_1 + 1/sample_size_2)/2\n",
    "\n",
    "    #1. Wald Type Z-Statistic\n",
    "    Standard_Error_Wald_sample_1 = (proportion_sample_1*(1-proportion_sample_1)) / sample_size_1\n",
    "    Standard_Error_Wald_sample_2 = (proportion_sample_2*(1-proportion_sample_2)) / sample_size_2\n",
    "    Standard_Error_Wald = np.sqrt(Standard_Error_Wald_sample_1 + Standard_Error_Wald_sample_2)\n",
    "    Z_wald = (samples_difference - difference_in_population) / Standard_Error_Wald\n",
    "    p_value_wald = calculate_p_value_from_z_score(Z_wald)\n",
    "\n",
    "    Z_Wald_corrected = (samples_difference - difference_in_population - continuiety_correction) / Standard_Error_Wald\n",
    "    p_value_Wald_corrected = calculate_p_value_from_z_score(Z_Wald_corrected)\n",
    "\n",
    "    #2. Wald_H0 Type Z-Statistic\n",
    "    Estimitated_Population_Proportion =  (proportion_sample_1*sample_size_1 + proportion_sample_2*sample_size_2) / (sample_size_1 + sample_size_2)\n",
    "    Standard_error_Wald_H0 = np.sqrt(Estimitated_Population_Proportion * (1-Estimitated_Population_Proportion) * (1/sample_size_1 + 1/sample_size_2))\n",
    "    Z_Wald_H0 = (samples_difference - difference_in_population) / Standard_error_Wald_H0\n",
    "    p_value_Wald_H0 = calculate_p_value_from_z_score(Z_Wald_H0)\n",
    "\n",
    "    Z_Wald_H0_corrected = (samples_difference - difference_in_population - continuiety_correction) / Standard_error_Wald_H0\n",
    "    p_value_Wald_H0_corrected = calculate_p_value_from_z_score(Z_Wald_H0_corrected)\n",
    "\n",
    "    # 3. Hauck and Anderson's Z Statistic\n",
    "    correction_HA = 1/(2 * min(sample_size_1, sample_size_2))# Hauck and Anderson's correction\n",
    "    Standard_Error_sample_1_HA = (proportion_sample_1*(1-proportion_sample_1)) / (sample_size_1 - 1)\n",
    "    Standard_Error_sample_2_HA = (proportion_sample_2*(1-proportion_sample_2)) / (sample_size_2 - 1)\n",
    "    Standard_error_HA = np.sqrt(Standard_Error_sample_1_HA + Standard_Error_sample_2_HA)\n",
    "    z_score_HA = (samples_difference -  difference_in_population - correction_HA) / Standard_error_HA\n",
    "    p_value_HA = calculate_p_value_from_z_score(z_score_HA)\n",
    "\n",
    "    # 4. Conditional Mantel Haenszel Test\n",
    "    Standart_Error_MH = np.sqrt((sample_size_1*sample_size_2*(p1+p2)* (q1+q2)) / (sample_size**2 *(sample_size-1)))\n",
    "    Mean_MH = (sample_size_1*(p1+p2)) / sample_size\n",
    "    Z_MH = (p1 - Mean_MH) / Standart_Error_MH\n",
    "    pval_MH = calculate_p_value_from_z_score(Z_MH)\n",
    "\n",
    "    # 5. Barnard Exact\n",
    "    data_2x2 = [[p1, p2] , [q1,q2]]\n",
    "    Barnard_Exact = barnard_exact(data_2x2)\n",
    "    pvalue_barnard = Barnard_Exact.pvalue\n",
    "    Barnard_D_Statistic = Barnard_Exact.statistic\n",
    "\n",
    "    # 6. Fisher Exact\n",
    "    data_2x2 = [[p1, p2] , [q1,q2]]\n",
    "    Fisher_Exact_p_value = fisher_exact(data_2x2).pvalue\n",
    "\n",
    "   \n",
    "    results = {}\n",
    "\n",
    "    # Descreptive Statistics\n",
    "    results[\"Sample 1's Proportion\"] = round(proportion_sample_1,4)\n",
    "    results[\"Sample 2's Proportion\"] = round(proportion_sample_2,4)\n",
    "    results[\"Difference Between Proportions\"] = round(proportion_sample_1 - proportion_sample_2,4)\n",
    "    results[\"Number Of Sucesses 1\"] = p1\n",
    "    results[\"Number Of Sucesses 2\"] = p2\n",
    "    results[\"Sample 1 Size\"] = sample_size_1\n",
    "    results[\"Sample 2 Size\"] = sample_size_2\n",
    "    results[\"Difference in Population\"] = difference_in_population\n",
    "    \n",
    "    # Inferential Statistics\n",
    "    results[\"Standard Error (Wald/Wald Corrected)\"] = round(Standard_Error_Wald, 4)\n",
    "    results[\"Z-score (Wald)\"] = round(Z_wald, 4)\n",
    "    results[\"P-value (Wald)\"] = round(p_value_wald, 4)\n",
    "    results[\"Z-score (Wald Corrected)\"] = round(Z_Wald_corrected, 4)\n",
    "    results[\"P-value (Wald Corrected)\"] = round(p_value_Wald_corrected, 4)\n",
    "    results[\"Standard Error (Wald_H0/Wald Corrected_H0)\"] = round(Standard_error_Wald_H0, 4)\n",
    "    results[\"Z-score (Wald_H0)\"] = round(Z_Wald_H0, 4)\n",
    "    results[\"P-value (Wald_H0)\"] = round(p_value_Wald_H0, 4)\n",
    "    results[\"Z-score (Wald_H0 Corrected)\"] = round(Z_Wald_H0_corrected, 4)\n",
    "    results[\"P-value (Wald_H0 Corrected)\"] = round(p_value_Wald_H0_corrected, 4)\n",
    "    results[\"Standart Error Hauck & Anderson\"] = round(Standard_error_HA, 4)\n",
    "    results[\"Z-score Hauck & Anderson\"] = round(z_score_HA, 4)\n",
    "    results[\"p-value Hauck & Anderson\"] = round(p_value_HA, 4)\n",
    "    results[\"Standard Error Mantel & Haenszel\"] = round(Standart_Error_MH, 4)\n",
    "    results[\"Z-score Mantel & Haenszel\"] = round(Z_MH, 4)\n",
    "    results[\"p-value Mantel & Haenszel\"] = round(pval_MH, 4)\n",
    "    results[\"D statistic Barnard Exact\"] = round(Barnard_D_Statistic,4)\n",
    "    results[\"p-value Barnard Exact\"] = round(pvalue_barnard,4)\n",
    "    results[\"p-value Fisher Exact\"] = round(Fisher_Exact_p_value,4)\n",
    "\n",
    "    # Effect Sizes and confidence Intervals\n",
    "    results[\"Effect Size - Cohen's h\"] = round(cohens_h, 4)\n",
    "    results[\"Effect Size - Probit d\"] = round(Probit_d, 4)\n",
    "    results[\"Effect Size - Logit d\"] = round(logit_d, 4)\n",
    "    \n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "def confidence_intervals_for_difference_proporiton(sample_size_1, sample_size_2, proportion_sample_1, proportion_sample_2, Population_Difference, confidence_level):\n",
    "    \n",
    "    #Preperations: \n",
    "    p1 = proportion_sample_1 * sample_size_1\n",
    "    p2 = proportion_sample_2 * sample_size_2\n",
    "    q1 = sample_size_1 - p1\n",
    "    q2 = sample_size_2 - p2\n",
    "    sample_size = sample_size_1 + sample_size_2 \n",
    "    \n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    samples_difference = proportion_sample_1 - proportion_sample_2\n",
    "    Standard_Error_Wald_sample_1 = (proportion_sample_1*(1-proportion_sample_1)) / sample_size_1\n",
    "    Standard_Error_Wald_sample_2 = (proportion_sample_2*(1-proportion_sample_2)) / sample_size_2\n",
    "    Standard_Error_Wald = np.sqrt(Standard_Error_Wald_sample_1 + Standard_Error_Wald_sample_2)\n",
    "\n",
    "    # Method 1 - Wald Method\n",
    "    lower_ci_wald = samples_difference - zcrit*(Standard_Error_Wald)\n",
    "    upper_ci_wald = samples_difference + zcrit*(Standard_Error_Wald)\n",
    "\n",
    "    # Method 2 - Wald corrected Method\n",
    "    lower_ci_wald_corrected = samples_difference - (0.5*(1/sample_size_1 + 1/sample_size_2) + zcrit*(Standard_Error_Wald))\n",
    "    upper_ci_wald_corrected = samples_difference + (0.5*(1/sample_size_1 + 1/sample_size_2) + zcrit*(Standard_Error_Wald))\n",
    "\n",
    "    # Method 3 - Haldane\n",
    "    psi_Haldane = (proportion_sample_1 + proportion_sample_2) / 2\n",
    "    v = (1/sample_size_1 - 1/sample_size_2) / 4\n",
    "    mu = (1/sample_size_1 + 1/sample_size_2) / 4\n",
    "    theta_haldane = ((proportion_sample_1 - proportion_sample_2) + zcrit**2*v*(1-2*psi_Haldane)) / (1 + zcrit**2*mu)\n",
    "    w_haldane = (zcrit/ (1+zcrit**2*mu)) * np.sqrt(mu*(4*psi_Haldane*(1-psi_Haldane) - (proportion_sample_1 - proportion_sample_2)**2) + 2*v*(1-2*psi_Haldane) * (proportion_sample_1 - proportion_sample_2) + 4*zcrit**2*mu**2*(1-psi_Haldane)*psi_Haldane  + zcrit**2*v**2*(1-2*psi_Haldane)**2  )\n",
    "    lower_ci_haldane = theta_haldane - w_haldane\n",
    "    upper_ci_haldane = theta_haldane + w_haldane\n",
    "\n",
    "    # Method 4 - Jeffreys-Perks\n",
    "    psi_JP = 0.5*(((p1 + 0.5) / (sample_size_1+1)) + ((p2+ 0.5) / (sample_size_2+1))) \n",
    "    theta_JP = ((proportion_sample_1 - proportion_sample_2) + zcrit**2*v*(1-2*psi_JP)) / (1 + zcrit**2*mu)\n",
    "    w_JP = (zcrit/ (1+zcrit**2*mu)) * np.sqrt(mu*(4*psi_JP*(1-psi_JP) - (proportion_sample_1 - proportion_sample_2)**2) + 2*v*(1-2*psi_JP) * (proportion_sample_1 - proportion_sample_2) + 4*zcrit**2*mu**2*(1-psi_JP)*psi_JP  + zcrit**2*v**2*(1-2*psi_JP)**2  )\n",
    "    lower_ci_JP= theta_JP - w_JP\n",
    "    upper_ci_JP = theta_JP + w_JP\n",
    "\n",
    "    # Method 5+6 - MEE and Miettinen-Nurminen\n",
    "    def Standart_Error_Calcualte(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, difference_in_population):\n",
    "        sample_size_ratio = sample_size_2 / sample_size_1\n",
    "        a = 1 + sample_size_ratio\n",
    "        b = -(1 + sample_size_ratio + proportion_sample_1 + sample_size_ratio * proportion_sample_2 + difference_in_population * (sample_size_ratio + 2))\n",
    "        c = difference_in_population * difference_in_population + difference_in_population * (2 * proportion_sample_1 + sample_size_ratio + 1) + proportion_sample_1 + sample_size_ratio * proportion_sample_2\n",
    "        d = -proportion_sample_1 * difference_in_population * (1 + difference_in_population)\n",
    "        v = (b / a / 3) ** 3 - b * c / (6 * a * a) + d / a / 2\n",
    "        v = np.where(np.abs(v) < np.finfo(float).eps, 0, v)\n",
    "        s = np.sqrt((b / a / 3) ** 2 - c / a / 3)\n",
    "        u = np.where(v > 0, 1, -1) * s\n",
    "        w = (np.pi + np.arccos(v / u ** 3)) / 3\n",
    "        proportion_sample_hat_1 = 2 * u * np.cos(w) - b / a / 3\n",
    "        proportion_sample_hat_2 = proportion_sample_hat_1 - difference_in_population\n",
    "        n = sample_size_1 + sample_size_2\n",
    "        Variance_Miettinen_Nurminen = (proportion_sample_hat_1 * (1 - proportion_sample_hat_1) / sample_size_1 + proportion_sample_hat_2 * (1 - proportion_sample_hat_2) / sample_size_2) * (n / (n - 1)) \n",
    "        Standart_Error_Miettinen_Nurminen = np.sqrt(Variance_Miettinen_Nurminen)\n",
    "        Variance_Miettinen_MEE = (proportion_sample_hat_1 * (1 - proportion_sample_hat_1) / sample_size_1 + proportion_sample_hat_2 * (1 - proportion_sample_hat_2) / sample_size_2)\n",
    "        Standart_Error_MEE = np.sqrt(Variance_Miettinen_MEE)\n",
    "\n",
    "\n",
    "        return Standart_Error_Miettinen_Nurminen, Standart_Error_MEE\n",
    "\n",
    "    def pval(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, difference_in_population):\n",
    "        samples_difference = proportion_sample_1 - proportion_sample_2\n",
    "        se_mn, se_mee = Standart_Error_Calcualte(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, difference_in_population)\n",
    "        z_mn = (samples_difference - difference_in_population) / se_mn\n",
    "        z_mee = (samples_difference - difference_in_population) / se_mee\n",
    "        p_mn = 2 * np.minimum(stats.norm.cdf(z_mn), 1 - stats.norm.cdf(z_mn))\n",
    "        p_mee = 2 * np.minimum(stats.norm.cdf(z_mee), 1 - stats.norm.cdf(z_mee))\n",
    "\n",
    "        return p_mn, p_mee\n",
    "\n",
    "    def confidence_interval(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, confidence_level):\n",
    "        lower_bracket = [-1, proportion_sample_1 - proportion_sample_2]\n",
    "        upper_bracket = [proportion_sample_1 - proportion_sample_2, 0.999999]\n",
    "\n",
    "        def root_func(difference_in_population):\n",
    "            return pval(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, difference_in_population)[0] - confidence_level\n",
    "\n",
    "        CI_mn_lower = optimize.root_scalar(root_func, bracket=lower_bracket).root\n",
    "        CI_mn_upper = optimize.root_scalar(root_func, bracket=upper_bracket).root\n",
    "\n",
    "        def root_func_mee(difference_in_population):\n",
    "            return pval(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, difference_in_population)[1] - confidence_level\n",
    "\n",
    "        CI_mee_lower = optimize.root_scalar(root_func_mee, bracket=lower_bracket).root\n",
    "        CI_mee_upper = optimize.root_scalar(root_func_mee, bracket=upper_bracket).root\n",
    "\n",
    "\n",
    "        return (max(-1, CI_mn_lower), min(1, CI_mn_upper)), (max(-1, CI_mee_lower), min(1, CI_mee_upper))\n",
    "\n",
    "    def z_score(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, Population_Difference):\n",
    "        samples_difference = proportion_sample_1 - proportion_sample_2\n",
    "        se_mn, se_mee = Standart_Error_Calcualte(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, Population_Difference)\n",
    "        z_score_mn = (samples_difference - Population_Difference) / se_mn\n",
    "        z_score_mee = (samples_difference - Population_Difference) / se_mee\n",
    "        return z_score_mn, z_score_mee\n",
    "\n",
    "    CI_mn, CI_mee = confidence_interval(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, 1 - confidence_level)\n",
    "    z_score_mn, z_score_mee = z_score(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, Population_Difference)\n",
    "    se_mn, se_mee = Standart_Error_Calcualte(proportion_sample_1, sample_size_1, proportion_sample_2, sample_size_2, Population_Difference)\n",
    "    \n",
    "    # Method 7 - Agresti_Caffo\n",
    "    p1_agresti_caffo = (p1 +1) / (sample_size_1 +2)\n",
    "    p2_agresti_caffo = (p2 +1) / (sample_size_2 +2)\n",
    "    Standard_Error_sample_1_AC = (p1_agresti_caffo*(1-p1_agresti_caffo)) / (sample_size_1 + 2)\n",
    "    Standard_Error_sample_2_AC = (p2_agresti_caffo*(1-p2_agresti_caffo)) / (sample_size_2 + 2)\n",
    "    Standard_error_AC = np.sqrt(Standard_Error_sample_1_AC + Standard_Error_sample_2_AC)\n",
    "    lower_ci_AC= (p1_agresti_caffo-p2_agresti_caffo) - zcrit* Standard_error_AC\n",
    "    upper_ci_AC = (p1_agresti_caffo-p2_agresti_caffo) + zcrit* Standard_error_AC\n",
    "\n",
    "    #Method 8 - Wilson\n",
    "    epsilon_1, constant1 = (p1 + zcrit ** 2 / 2) / ( sample_size_1 + zcrit ** 2), zcrit * math.sqrt(sample_size_1) / (sample_size_1 + zcrit ** 2) * math.sqrt(proportion_sample_1 * (1 - proportion_sample_1) + zcrit ** 2 / (4 * sample_size_1))\n",
    "    CI_wilson1_lower, CI_wilson1_upper = max(0, epsilon_1 - constant1), min(1, epsilon_1 + constant1)\n",
    "    epsilon_2, constant2 = (p2 + zcrit ** 2 / 2) / ( sample_size_2 + zcrit ** 2), zcrit * math.sqrt(sample_size_2) / (sample_size_2 + zcrit ** 2) * math.sqrt(proportion_sample_2 * (1 - proportion_sample_2) + zcrit ** 2 / (4 * sample_size_2))\n",
    "    CI_wilson2_lower, CI_wilson2_upper = max(0, epsilon_2 - constant2), min(1, epsilon_2 + constant2)\n",
    "\n",
    "    CI_wilson_lower = samples_difference - zcrit * np.sqrt(CI_wilson1_lower*(1-CI_wilson1_lower)/sample_size_1 + CI_wilson2_upper*(1-CI_wilson2_upper)/sample_size_2)\n",
    "    CI_wilson_upper = samples_difference + zcrit * np.sqrt(CI_wilson1_upper*(1-CI_wilson1_upper)/sample_size_1 + CI_wilson2_lower*(1-CI_wilson2_lower)/sample_size_2)\n",
    "\n",
    "    #Method 9 - Wilson Corrected\n",
    "    CI_wilsonc1_lower = (2 * p1 + zcrit**2 - 1 - zcrit * math.sqrt(zcrit**2 - 2 - 1/sample_size_1 + 4 * proportion_sample_1 * (sample_size_1 * (1 - proportion_sample_1) + 1))) / (2 * (sample_size_1 + zcrit**2))\n",
    "    CI_wilsonc1_upper = (2 * p1 + zcrit**2 + 1 + zcrit * math.sqrt(zcrit**2 + 2 - 1/sample_size_1 + 4 * proportion_sample_1 * (sample_size_1 * (1 - proportion_sample_1) - 1))) / (2 * (sample_size_1 + zcrit**2))\n",
    "    CI_wilsonc2_lower = (2 * p2 + zcrit**2 - 1 - zcrit * math.sqrt(zcrit**2 - 2 - 1/sample_size_2 + 4 * proportion_sample_2 * (sample_size_2 * (1 - proportion_sample_2) + 1))) / (2 * (sample_size_2 + zcrit**2))\n",
    "    CI_wilsonc2_upper = (2 * p2 + zcrit**2 + 1 + zcrit * math.sqrt(zcrit**2 + 2 - 1/sample_size_2 + 4 * proportion_sample_2 * (sample_size_2 * (1 - proportion_sample_2) - 1))) / (2 * (sample_size_2 + zcrit**2))\n",
    "\n",
    "    CI_wilsonc_lower = max(-1, samples_difference - np.sqrt((proportion_sample_1 - CI_wilsonc1_lower)**2 + (CI_wilsonc2_upper-proportion_sample_2)**2) )\n",
    "    CI_wilsonc_upper = min( 1, samples_difference + np.sqrt((CI_wilsonc1_upper-proportion_sample_1)**2 + (proportion_sample_2-CI_wilsonc2_lower)**2) )\n",
    "\n",
    "    #Method 10 - Hauck-Anderson\n",
    "    correction_HA = 1/(2 * min(sample_size_1, sample_size_2))# Hauck and Anderson's correction\n",
    "    Standard_Error_sample_1_HA = (proportion_sample_1*(1-proportion_sample_1)) / (sample_size_1 - 1)\n",
    "    Standard_Error_sample_2_HA = (proportion_sample_2*(1-proportion_sample_2)) / (sample_size_2 - 1)\n",
    "    Standard_error_HA = np.sqrt(Standard_Error_sample_1_HA + Standard_Error_sample_2_HA)\n",
    "   \n",
    "    CI_lower_HA = max(samples_difference - 1/(2 * min(sample_size_1,sample_size_2)) - zcrit * Standard_error_HA, -1)\n",
    "    CI_upper_HA = min(samples_difference + 1/(2 * min(sample_size_1,sample_size_2)) + zcrit * Standard_error_HA, 1)\n",
    "\n",
    "    #Method 11 - Brown, Lis Jeffreys\n",
    "    p1_BLJ = (p1 + 0.5) / (sample_size_1+1)\n",
    "    p2_BLJ = (p2 + 0.5) / (sample_size_2+1)\n",
    "    Standard_Error_BLJ = np.sqrt(p1_BLJ*(1-p1_BLJ)/sample_size_1 + p2_BLJ*(1-p2_BLJ)/sample_size_2)\n",
    "    CI_BLJ_lower = (p1_BLJ - p2_BLJ) - zcrit * Standard_Error_BLJ\n",
    "    CI_BLJ_upper = (p1_BLJ - p2_BLJ) + zcrit * Standard_Error_BLJ\n",
    "\n",
    "    #method 12 - Gart Nam\n",
    "    robjects.r('''scoretheta <- function(x1, n1, x2, n2, theta, level = 0.95) {Prop_Diff <- ((x1 / n1) - (x2 / n2)) - theta\n",
    "            N <- n1 + n2\n",
    "            a <- (n1 + 2 * n2) * theta - N - (x1 + x2)\n",
    "            b <- (a / N / 3)^3 - a * ((n2 * theta - N - 2 * x2) * theta + (x1 + x2)) / (6 * N * N) + (x2 * theta * (1 - theta)) / N / 2\n",
    "            c <- ifelse(b > 0, 1, -1) * sqrt(pmax(0, (a / N / 3)^2 - ((n2 * theta - N - 2 * x2) * theta + (x1 + x2)) / N / 3))\n",
    "            p2d <- pmin(1, pmax(0, round(2 * c * cos(((pi + acos(pmax(-1, pmin(1, ifelse(c == 0 & b == 0, 0, b / c^3))))) / 3)) - a / N / 3, 10)))\n",
    "            p1d <- pmin(1, pmax(0, p2d + theta))\n",
    "            Variance <- pmax(0, (p1d * (1 - p1d) / n1 + p2d * (1 - p2d) / n2))\n",
    "            scterm <- (p1d * (1 - p1d) * (1 - 2 * p1d) / (n1^2) - p2d * (1 - p2d) * (1 - 2 * p2d) / (n2^2)) / (6 * Variance^(3 / 2))\n",
    "            score <- ifelse(scterm == 0, (Prop_Diff / sqrt(Variance)), (-1 + sqrt(pmax(0, 1^2 - 4 * scterm * -(Prop_Diff / sqrt(Variance) + scterm))) ) / (2 * scterm))\n",
    "            return(score)}\n",
    "\n",
    "            Binary_Search <- function(score_function, max.iter = 100, tail = \"lower\") {\n",
    "            nstrat <- length(eval(score_function(1)))\n",
    "            hi <- rep(1, nstrat)\n",
    "            lo <- rep(-1, nstrat)\n",
    "            niter <- 1\n",
    "            while (niter <= max.iter && any(2 > 0.0000005 | is.na(hi))) {\n",
    "            mid <- pmax(-1, pmin(1, round((hi + lo) / 2, 10)))\n",
    "            scor <- score_function(mid)\n",
    "            check <- (scor <= 0) | is.na(scor)\n",
    "            hi[check] <- mid[check]\n",
    "            lo[!check] <- mid[!check]\n",
    "            niter <- niter + 1}\n",
    "            ci <- if (tail == \"lower\") lo else hi\n",
    "            return(ci)}\n",
    "\n",
    "            gart_nam = function(x1, n1, x2, n2, level = 0.95) {\n",
    "            zcrit <- qnorm(1 - (1 - level)/2)\n",
    "            lower <- Binary_Search(score_function = function(theta) scoretheta(x1, n1, x2, n2,theta) - zcrit, tail = \"lower\")\n",
    "            upper <- Binary_Search(score_function = function(theta) scoretheta(x1, n1, x2, n2,theta) + zcrit, tail = \"upper\")\n",
    "            return(c(lower, upper))\n",
    "            }''')\n",
    "\n",
    "    Gart_Nam_CI = robjects.r['gart_nam'](p1, sample_size_1, p2, sample_size_2, confidence_level)  # type: ignore\n",
    "\n",
    "\n",
    "    results = {}    \n",
    "    \n",
    "    results[\"Confidence Intervals Wald\"] = f\"({round(lower_ci_wald, 4)}, {round(upper_ci_wald, 4)})\"\n",
    "    results[\"Confidence Intervals Wald Corrected\"] = f\"({round(lower_ci_wald_corrected, 4)}, {round(upper_ci_wald_corrected, 4)})\"\n",
    "    results[\"Confidence Intervals Haldane\"] = f\"({round(lower_ci_haldane, 4)}, {round(upper_ci_haldane, 4)})\"\n",
    "    results[\"Confidence Intervals Jeffreys-Perks\"] = f\"({round(lower_ci_JP, 4)}, {round(upper_ci_JP, 4)})\"\n",
    "    results[\"Confidence Intervals Mee\"] = around(CI_mee,4)\n",
    "    results[\"Confidence Intervals Miettinen-Nurminen\"] = around(CI_mn,4)\n",
    "    results[\"Confidence Intervals Gart-Nam\"] = around(Gart_Nam_CI,4)\n",
    "    results[\"Confidence Intervals Agrest-Caffo\"] = f\"({round(lower_ci_AC, 4)}, {round(upper_ci_AC, 4)})\"\n",
    "    results[\"Confidence Intervals Wilson\"] = f\"({round(CI_wilson_lower, 4)}, {round(CI_wilson_upper, 4)})\"\n",
    "    results[\"Confidence Intervals Wilson Corrected\"] = f\"({round(CI_wilsonc_lower, 4)}, {round(CI_wilsonc_upper, 4)})\"\n",
    "    results[\"Confidence Intervals Hauck-Anderson\"] = f\"({round(CI_lower_HA, 4)}, {round(CI_upper_HA, 4)})\"\n",
    "    results[\"Confidence Intervals Brown-Lee-Jeffereys\"] = f\"({round(CI_BLJ_lower, 4)}, {round(CI_BLJ_upper, 4)})\"\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def odds_Ratio_two_independent_samples(sample_size_1, sample_size_2, proportion_sample_1, proportion_sample_2, confidence_level):\n",
    "    \n",
    "    #Initial Definitions\n",
    "    p1 = proportion_sample_1 * sample_size_1\n",
    "    p2 = proportion_sample_2 * sample_size_2\n",
    "    q1 = sample_size_1 - p1\n",
    "    q2 = sample_size_2 - p2\n",
    "    sample_size = sample_size_1 + sample_size_2\n",
    "\n",
    "    \n",
    "    # Odds Ratio effect sizes Calculation\n",
    "    \n",
    "    # 1. Conditional MLE (Corenfield)\n",
    "    odds_ratio_Conditional_test = odds_ratio([[int(p1), int(q1)], [int(p2), int(q2)]])\n",
    "    odds_ratio_Conditional = odds_ratio_Conditional_test.statistic\n",
    "    \n",
    "    # 2. Unconditional MLE (Wald)\n",
    "    odds_ratio_wald = (proportion_sample_1*(1-proportion_sample_2)) / (proportion_sample_2* (1-proportion_sample_1))\n",
    "\n",
    "    # 3. Wald_Adjusted\n",
    "    odds_ratio_wald_adjusted = ((p1 + 0.5) * (q2 + 0.5)) / ((p2 + 0.5) * (q1 + 0.5))\n",
    "\n",
    "    # 4. Wald Small Samples Correction\n",
    "    odds_ratio_small_samples_adjusted = (p1 * q2) / ((p2 + 1) * (q1 + 1))\n",
    "\n",
    "    # 5. Median unbaised odds ratio\n",
    "    robjects.r('''odds_ratio_median_unbaised = function(p1, q1, p2, q2, conf.level = 0.95, interval = c(0, 1000)) {\n",
    "        alpha = 1 - conf.level\n",
    "        x = matrix(c(p1, q1, p2, q2), 2, 2)\n",
    "        oddsratio = uniroot(function(or_val) {fisher.test(x, or = or_val, alternative = \"less\")$p.value - fisher.test(x, or = or_val, alternative = \"greater\")$p.value},interval = interval)$root\n",
    "        return(oddsratio)}''')\n",
    "    \n",
    "    odds_ratio_median_unbaised = robjects.r['odds_ratio_median_unbaised'](p1, p2, q1, q2)  # type: ignore\n",
    "\n",
    "\n",
    "    # Standard Error of the Odds Ratio\n",
    "    standard_error_odds_ratio =np.sqrt((1 / (p1)) + (1 / (p2)) + (1 / (q1)) + (1 / (q2)))\n",
    "    standard_error_odds_ratio_adjusted =np.sqrt((1 / (p1+0.5)) + (1 / (p2+0.5)) + (1 / (q1 + 0.5)) + (1 / (q2 + 0.5)))\n",
    "\n",
    "\n",
    "    # Inferential Statistics\n",
    "    \n",
    "    # 1. chi_square_p values\n",
    "    chi2_odds_ratio = (abs(p1 - ((p1+q1)*(p1+p2)/sample_size)))**2 / ((p1+q1)*(p1+p2)/sample_size) + \\\n",
    "    (abs(q1 - ((p1+q1)*(q1+q2)/sample_size)))**2 / ((p1+q1)*(q1+q2)/sample_size) + \\\n",
    "    (abs(p2 - ((p2+q2)*(p1+p2)/sample_size)))**2 / ((p2+q2)*(p1+p2)/sample_size) + \\\n",
    "    (abs(q2 - ((p2+q2)*(q1+q2)/sample_size)))**2 / ((p2+q2)*(q1+q2)/sample_size) \n",
    "\n",
    "    p_value_odds_ratio  = calculate_p_value_from_chi_score(chi2_odds_ratio, 1)\n",
    "\n",
    "    # 2. chi_square_adjusted_p values\n",
    "    chi2_odds_ratio_adjusted = (abs(p1 - ((p1+q1)*(p1+p2)/sample_size)) - 0.5)**2 / ((p1+q1)*(p1+p2)/sample_size) + \\\n",
    "    (abs(q1 - ((p1+q1)*(q1+q2)/sample_size)) - 0.5)**2 / ((p1+q1)*(q1+q2)/sample_size) + \\\n",
    "    (abs(p2 - ((p2+q2)*(p1+p2)/sample_size)) - 0.5)**2 / ((p2+q2)*(p1+p2)/sample_size) + \\\n",
    "    (abs(q2 - ((p2+q2)*(q1+q2)/sample_size)) - 0.5)**2 / ((p2+q2)*(q1+q2)/sample_size)\n",
    "\n",
    "    p_value_odds_ratio_adjusted = calculate_p_value_from_chi_score(chi2_odds_ratio_adjusted, 1)\n",
    "\n",
    "    # 3. Fisher Exact p.value\n",
    "    contingency_table = [[p1, p2], [q1, q2]]\n",
    "    p_value_fisher = fisher_exact(contingency_table)[1]\n",
    "\n",
    "    # 4. Mid_p_value\n",
    "    def odds_ratio_mid_p_value_function(p1, q1, p2, q2):\n",
    "        x = [[p1, q1], [p2, q2]]\n",
    "        p_value_less = fisher_exact(x, alternative='less')[1]\n",
    "        p_value_greater = fisher_exact(x, alternative='greater')[1]\n",
    "        mid_p = 0.5 * (p_value_less - p_value_greater + 1)\n",
    "        one_sided_mid_p = min(mid_p, 1 - mid_p)\n",
    "        two_sided_mid_p = 2 * one_sided_mid_p\n",
    "        return two_sided_mid_p\n",
    "\n",
    "    mid_p_value = odds_ratio_mid_p_value_function(p1, p2, q1, q2)\n",
    "\n",
    "    # Yules Q\n",
    "    Yules_Q = (p1*q2 - q1*p2) / (p1*q2 + q1*p2)\n",
    "    Yules_Y = ((p1*q2)**0.5 - (q1*p2)**0.5) / ((p1*q2)**0.5 + (q1*p2)**0.5)\n",
    "    Digbys_H = ((p1*q2)**0.75 - (q1*p2)**0.75) / ((p1*q2)**0.75 + (q1*p2)**0.75)\n",
    "\n",
    "    #Yules Y* (Bonett and Price, 2007) \n",
    "    Marginal_Proportion_1 = (p1+p2)/sample_size\n",
    "    Marginal_Proportion_2 = (p1+q1)/sample_size\n",
    "    Marginal_Proportion_3 = (p2+q2)/sample_size\n",
    "    Marginal_Proportion_4 = (q1+q2)/sample_size\n",
    "\n",
    "    marginal_proportions = np.array([Marginal_Proportion_1, Marginal_Proportion_2, Marginal_Proportion_3, Marginal_Proportion_4])\n",
    "    min_marginal_prooportion = min(marginal_proportions)\n",
    "    c = 0.5 - (0.5 - min_marginal_prooportion) **2\n",
    "    corrected_odds_ratio = ((p1+0.1) * (q2+0.1)) / ((p2+0.1) * (q1+0.1))\n",
    "    Yules_Y_Star = (corrected_odds_ratio**c - 1) / (corrected_odds_ratio**c + 1)\n",
    "\n",
    "    # Standard Errors by Bishop, Fienberg, Holland - 2007 (Discrete Mulyivariete analysis). For the Y star its from Bonett and Star\n",
    "    Standard_Error_Yules_Q = 0.5*(1-Yules_Q**2) * standard_error_odds_ratio\n",
    "    Standard_Error_Yules_Y = 0.25*(1-Yules_Y**2) * standard_error_odds_ratio\n",
    "    Standard_Error_Digbys_H = 0.5*0.75*(1-Digbys_H**2) * standard_error_odds_ratio\n",
    "    Standard_Error_Yules_Y_Star = np.sqrt( (c**2/4) * ((1 /(p1+0.1)) + (1/(q2+0.1)) + (1/(p2+0.1)) +( 1/(q1+0.1))))\n",
    "    \n",
    "    #Confidence Intervals for the Yules Family\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "\n",
    "    Yules_Q_CI_lower = (Yules_Q - zcrit*Standard_Error_Yules_Q)\n",
    "    Yules_Q_CI_upper = (Yules_Q + zcrit*Standard_Error_Yules_Q)\n",
    "\n",
    "    Digbys_H_Lower = (Yules_Q - zcrit*Standard_Error_Digbys_H)\n",
    "    Digbys_H_Upper = (Digbys_H + zcrit*Standard_Error_Digbys_H)\n",
    "\n",
    "    Yules_Y_CI_lower  = (Yules_Y - zcrit*Standard_Error_Yules_Y)\n",
    "    Yules_Y_CI_upper  = (Yules_Y + zcrit*Standard_Error_Yules_Y)\n",
    "\n",
    "    Yules_Y_Star_CI_lower = np.tanh(np.arctanh(Yules_Y_Star) - zcrit * Standard_Error_Yules_Y_Star)\n",
    "    Yules_Y_Star_CI_upper = np.tanh(np.arctanh(Yules_Y_Star) + zcrit * Standard_Error_Yules_Y_Star)\n",
    "\n",
    "\n",
    "    # add the log odds ratio next to the odds ratio\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Odds Ratio (Wald Unconditional MLE)\"] = round(odds_ratio_wald, 4)\n",
    "    results[\"Odds Ratio (Wald Adjusted)\"] = round(odds_ratio_wald_adjusted, 4)\n",
    "    results[\"Odds Ratio (Fisher Conditional MLE)\"] = round(odds_ratio_Conditional, 4)\n",
    "    results[\"Odds Ratio Small Samples Adaptation\"] = round(odds_ratio_small_samples_adjusted, 4)\n",
    "    results[\"Odds Ratio Median Unbaised (Mid-P)\"] = around(odds_ratio_median_unbaised, 4)\n",
    "    results[\"Yule's Q\"] = around(Yules_Q, 4)\n",
    "    results[\"Yule's Y\"] = around(Yules_Y, 4)\n",
    "    results[\"Digby's H\"] = around(Digbys_H, 4)\n",
    "    results[\"Yule's Y* (Bonett & Price)\"] = around(Yules_Y_Star, 4)\n",
    "\n",
    "    results[\"Standard Error of the Odds Ratio\"] = round(standard_error_odds_ratio, 4)\n",
    "    results[\"Standard Error of the Odds Ratio (adjusted)\"] = round(standard_error_odds_ratio_adjusted, 4)\n",
    "    results[\"Standard Error of Yules Q\"] = around(Standard_Error_Yules_Q, 4)\n",
    "    results[\"Standard Error of Yules Y\"] = around(Standard_Error_Yules_Y, 4)\n",
    "    results[\"Standard Error of Digby's H\"] = around(Standard_Error_Digbys_H, 4)\n",
    "    results[\"Standard Error of Yule's Y*\"] = around(Standard_Error_Yules_Y_Star, 4)\n",
    "\n",
    "    results[\"Odds Ratio p-value (Chi square)\"] = round(p_value_odds_ratio, 7)\n",
    "    results[\"Odds Ratio p-value (Adjusted Chi square)\"] = round(p_value_odds_ratio_adjusted, 7)\n",
    "    results[\"Odds Ratio p-value (Fisher Exact)\"] = round(p_value_fisher, 7)\n",
    "    results[\"Odds Ratio p-value (Mid-p value)\"] = round(mid_p_value, 7)\n",
    "    \n",
    "    results[\"Confidence Intervals Yules Q\"] = f\"({round(Yules_Q_CI_lower, 4)}, {round(Yules_Q_CI_upper, 4)})\"\n",
    "    results[\"Confidence Intervals Yules Y\"] = f\"({round(Yules_Y_CI_lower, 4)}, {round(Yules_Y_CI_upper, 4)})\"\n",
    "    results[\"Confidence Intervals Digby's H \"] = f\"({round(Digbys_H_Lower, 4)}, {round(Digbys_H_Upper, 4)})\"\n",
    "    results[\"Confidence Intervals Yules Y*\"] = f\"({round(Yules_Y_Star_CI_lower, 4)}, {round(Yules_Y_Star_CI_upper, 4)})\"\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "         \n",
    "\n",
    "def Indpendent_Odds_Ratio_CI(sample_size_1, sample_size_2, proportion_sample_1, proportion_sample_2, confidence_level):\n",
    "    \n",
    "    p1 = proportion_sample_1 * sample_size_1\n",
    "    p2 = proportion_sample_2 * sample_size_2\n",
    "    q1 = sample_size_1 - p1\n",
    "    q2 = sample_size_2 - p2\n",
    "\n",
    "    odds_ratio_wald = (proportion_sample_1*(1-proportion_sample_2)) / (proportion_sample_2* (1-proportion_sample_1))\n",
    "    standard_error_odds_ratio =np.sqrt((1 / (p1)) + (1 / (p2)) + (1 / (q1)) + (1 / (q2)))\n",
    "    standard_error_odds_ratio_adjusted =np.sqrt((1 / (p1+0.5)) + (1 / (p2+0.5)) + (1 / (q1 + 0.5)) + (1 / (q2 + 0.5)))\n",
    "\n",
    "    #Confidence Intervals:\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "\n",
    "    # Method 1 - Woolf (Aka Wald) - Logarithmic asymptotyc CI's\n",
    "    upper_ci_woolf = np.exp(np.log(odds_ratio_wald) + zcrit* standard_error_odds_ratio)\n",
    "    lower_ci_woolf = np.exp(np.log(odds_ratio_wald) - zcrit* standard_error_odds_ratio)\n",
    "    \n",
    "    # Method 2 - Woolf Corrected  (Wald adjusted) - Logarithmic asymptotyc CI's with + 0.5 correction\n",
    "    upper_or_woolf_adjusted = np.exp(np.log(odds_ratio_wald) + zcrit* standard_error_odds_ratio_adjusted)\n",
    "    lower_or_woolf_adjusted = np.exp(np.log(odds_ratio_wald) - zcrit* standard_error_odds_ratio_adjusted)\n",
    "\n",
    "    # Method 3 - Cornfield (Fisher exact CI's)\n",
    "    odds_ratio_Conditional_test = odds_ratio([[int(p1), int(q1)], [int(p2), int(q2)]])\n",
    "    odds_ratio_fisher_ci = odds_ratio_Conditional_test.confidence_interval(confidence_level)\n",
    "    formatted_ci_lower = format(odds_ratio_fisher_ci[0], \".6f\")\n",
    "    formatted_ci_upper = format(odds_ratio_fisher_ci[1], \".6f\")\n",
    "\n",
    "    # Method 4 - Mid-p Confidence Intervals\n",
    "    robjects.r('''odds_ratio_mid_p_value = function(p1, q1, p2, q2, conf.level = 0.95, interval = c(0, 1000)) {\n",
    "        mid_p_function = function(or_val = 1) {\n",
    "            less_p_value = fisher.test(matrix(c(p1, q1, p2, q2), 2, 2), or = or_val, alternative = \"less\")$p.value\n",
    "            greater_p_value = fisher.test(matrix(c(p1, q1, p2, q2), 2, 2), or = or_val, alternative = \"greater\")$p.value\n",
    "            0.5 * (less_p_value - greater_p_value + 1)\n",
    "            }\n",
    "\n",
    "        alpha = 1 - conf.level\n",
    "        x = matrix(c(p1, q1, p2, q2), 2, 2)\n",
    "            \n",
    "        oddsratio = uniroot(function(or_val) {fisher.test(x, or = or_val, alternative = \"less\")$p.value - fisher.test(x, or = or_val, alternative = \"greater\")$p.value},interval = interval)$root\n",
    "        lower_ci_small = uniroot(function(or_val) {1 - mid_p_function(or_val) - alpha/2}, interval = interval)$root\n",
    "        upper_ci_small = 1/uniroot(function(or_val) {mid_p_function(1/or_val) - alpha/2}, interval = interval)$root\n",
    "\n",
    "        return(c(oddsratio, lower_ci_small, upper_ci_small))}''')\n",
    "\n",
    "    lower_ci_mid_p  = robjects.r['odds_ratio_mid_p_value'](p1, p2, q1, q2)[1] # type: ignore\n",
    "    upper_ci_mid_p  = robjects.r['odds_ratio_mid_p_value'](p1, p2, q1, q2)[2] # type: ignore\n",
    "   \n",
    "    # Method 5 - the score method of Miettinen and Nurminen (1985)\n",
    "    CI_MN = confint_proportions_2indep(nobs1=sample_size_1,count1 = p1,nobs2=sample_size_2,count2 = p2,method = \"score\", compare = \"odds-ratio\", alpha = 1 - confidence_level)\n",
    "\n",
    "\n",
    "    # Method 6 - Baptista and Pike 1977\n",
    "    robjects.r('''Baptista_Pike = function(p1,q1,p2,q2, conf.level = 0.95, orRange = c(10^-10, 10^10)) {\n",
    "        x = matrix(c(p1,q1,p2,q2),2,2)\n",
    "        alpha <- 1 - conf.level; \n",
    "        n1 <- sum(x[1, ]); n2 <- sum(x[2, ])\n",
    "        sum_of_ps <- sum(x[, 1]); x <- x[1, 1]\n",
    "        support <- max(0, sum_of_ps - n2):min(n1, sum_of_ps)\n",
    "        dnhyper <- function(OR) {\n",
    "            d <- dhyper(support, n1, n2, sum_of_ps, log = TRUE) + log(OR) * support\n",
    "            exp(d - max(d)) / sum(exp(d - max(d)))}\n",
    "        pnhyper <- function(x, OR, lower.tail = TRUE) {\n",
    "            f <- dnhyper(OR)\n",
    "            X <- if (lower.tail) support <= x else support >= x\n",
    "            sum(f[X])}\n",
    "        intercept <- function(xlo, xhi, ORRange = orRange) {\n",
    "            X <- support <= xlo; Y <- support >= xhi\n",
    "            uniroot(function(beta) sum(dnhyper(beta)[X]) - sum(dnhyper(beta)[Y]), ORRange)$root}\n",
    "        ints_greater <- intercept(x, x + 1)\n",
    "        ints_less <- intercept(x - 1, x)\n",
    "        CINT_upper <-uniroot(function(or) alpha - pnhyper(x, or, lower.tail = TRUE), c(ints_greater, orRange[2]))$root\n",
    "        CINT_lower <-uniroot(function(or) alpha - pnhyper(x, or, lower.tail = FALSE), c(orRange[1], ints_less))$root\n",
    "        c(CINT_lower, CINT_upper)}''')\n",
    "\n",
    "    lower_bp  = robjects.r['Baptista_Pike'](p1,q1,p2,q2)[0] # type: ignore\n",
    "    upper_ci_bp  = robjects.r['Baptista_Pike'](p1,q1,p2,q2)[1] # type: ignore\n",
    "\n",
    "    # Method 7 - inverse hyperbolic sine method (Newcomb, 2001)\n",
    "    standart_error_sinh = 2 * math.asinh(zcrit / 2 * math.sqrt(1/p1 + 1/(q1) + 1/p2 + 1/(q2)))\n",
    "    lower_limit_sinh = math.exp(math.log(odds_ratio_wald) - standart_error_sinh)\n",
    "    upper_limit_sinh = math.exp(math.log(odds_ratio_wald) + standart_error_sinh) \n",
    "\n",
    "    # Method 8 - Independent Smooth logit (Agresti, 1999)\n",
    "    p1new = p1 + 2 * sample_size_1 * (p1 + p2) / (sample_size_1 + sample_size_2)**2\n",
    "    q1new = sample_size_1 - p1 + 2 * sample_size_1 * (sample_size_1 - p1 + sample_size_2 - p2) / (sample_size_1 + sample_size_2)**2\n",
    "    p2new = p2 + 2 * sample_size_2 * (p1 + p2) / (sample_size_1 + sample_size_2)**2\n",
    "    q2new = sample_size_2 - p2 + 2 * sample_size_2 * (sample_size_1 - p1 + sample_size_2 - p2) / (sample_size_1 + sample_size_2)**2\n",
    "    log_theta = np.log(p1new * q2new / (p2new * q1new))\n",
    "    ci_half_len = norm.ppf(1 - (1 - confidence_level) / 2) * np.sqrt(1 / p1new + 1 / q1new + 1 / p2new + 1 / q2new)\n",
    "    ci_lower_agresti_ind = np.exp(log_theta - ci_half_len)\n",
    "    ci_upper_agresti_ind = np.exp(log_theta + ci_half_len)\n",
    "\n",
    "    # Method 9 - Farrington-Manning\n",
    "    robjects.r('''score_test_statistic.Uncorrected <- function(theta0, n11, n21, n1p, n2p) {\n",
    "        p2hat <- (-(n1p * theta0 + n2p - (n11 + n21) * (theta0 - 1)) + sqrt((n1p * theta0 +\n",
    "        n2p - (n11 + n21) * (theta0 - 1))^2 - 4 * n2p * (theta0 - 1) * -(n11 + n21))) / (2 * n2p * (theta0 - 1))\n",
    "        p1hat <- p2hat * theta0 / (1 + p2hat * (theta0 - 1))\n",
    "        T0 <- ((n1p * (n11 / n1p - p1hat)) * sqrt(1 / (n1p * p1hat * (1 - p1hat)) + 1 / (n2p * p2hat * (1 - p2hat))))}\n",
    "\n",
    "        lower_limit <- function(theta0, n11, n21, n1p, n2p, alpha) {\n",
    "        T0 <- score_test_statistic.Uncorrected(theta0, n11, n21, n1p, n2p)\n",
    "        f <- T0 - qnorm(1 - alpha / 2, 0, 1)}\n",
    "\n",
    "        upper_limit <- function(theta0, n11, n21, n1p, n2p, alpha) {\n",
    "        T0 <- score_test_statistic.Uncorrected(theta0, n11, n21, n1p, n2p)\n",
    "        f <- T0 + qnorm(1 - alpha / 2, 0, 1)}\n",
    "\n",
    "        FM_CI <- function(p1,p2,q1,q2, alpha = 0.05) {\n",
    "        L <- uniroot(lower_limit, c(0.000001, 100000),p1, q1, (p1+p2), (q1+q2), alpha)$root\n",
    "        U <- uniroot(upper_limit, c(0.000001, 100000),p1, q1, (p1+p2), (q1+q2), alpha )$root\n",
    "        c(L, U)}''')\n",
    "    \n",
    "    lower_FM  = robjects.r['FM_CI'](p1,q1,p2,q2)[0]  #type: ignore\n",
    "    upper_FM  = robjects.r['FM_CI'](p1,q1,p2,q2)[1]  #type: ignore\n",
    "\n",
    "    # Method 10 - Unconditional Score Method (Agresti & Min, 2002) #see exact method (parmtype = oddsration and method = scores)\n",
    "\n",
    "\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Confidence Intervals (Woolf)\"] = f\"({round(lower_ci_woolf, 4)}, {round(upper_ci_woolf, 4)})\"\n",
    "    results[\"Confidence Intervals (Woolf Adjusted)\"] = f\"({round(lower_or_woolf_adjusted, 4)}, {round(upper_or_woolf_adjusted, 4)})\"\n",
    "    results[\"Confidence Intervals Fisher Exact (Cornfield)\"] = formatted_ci_lower, formatted_ci_upper\n",
    "    results[\"Confidence Intervals Mid-p\"] = f\"({round(lower_ci_mid_p, 4)}, {round(upper_ci_mid_p, 4)})\"\n",
    "    results[\"Confidence Intervals Miettinen and Nurminen\"] = around(CI_MN,4)\n",
    "    results[\"Confidence Intervals Baptista-Pike\"] = f\"({round(lower_bp, 4)}, {round(upper_ci_bp, 4)})\"\n",
    "    results[\"Confidence Intervals Inverse Sine (Newcomb)\"] = f\"({round(lower_limit_sinh, 4)}, {round(upper_limit_sinh, 4)})\"\n",
    "    results[\"Confidence Intervals Independent Smooth (Agresti)\"] = f\"({round(ci_lower_agresti_ind, 4)}, {round(ci_upper_agresti_ind, 4)})\"\n",
    "    results[\"Confidence Intervals Uncorrected Score (Farringot and Manning)\"] = f\"({round(lower_FM, 4)}, {round(upper_FM, 4)})\"\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "def risk_Ratio_two_independent_samples(sample_size_1, sample_size_2, proportion_sample_1, proportion_sample_2,confidence_level):\n",
    "    p1 = proportion_sample_1 * sample_size_1\n",
    "    p2 = proportion_sample_2 * sample_size_2\n",
    "    q1 = sample_size_1 - p1\n",
    "    q2 = sample_size_2 - p2\n",
    "    samples_difference = (proportion_sample_1 - proportion_sample_2)\n",
    "    \n",
    "    sample_size = sample_size_1 + sample_size_2 \n",
    "\n",
    "    Realtive_risk = proportion_sample_1/proportion_sample_2\n",
    "    Realtive_risk_unbiased = (p1*(q1+q2+1)) / (((p1+p2) * (q1 + 1)))\n",
    "    Realtive_risk_adjusted_Walters = np.exp(np.log((p1+0.5) / (sample_size_1+0.5)) - np.log((p2+0.5) / (sample_size_2+0.5)) )\n",
    "\n",
    "    \n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "\n",
    "\n",
    "    #Standard Error of the relative risk\n",
    "    standart_error_Relative_Risk_katz = np.sqrt((((1 - proportion_sample_1)) / p1) + ((1-proportion_sample_2)/p2))\n",
    "    standart_error_Relative_Risk_Walters = np.sqrt((1/(p1+0.5)) - (1/(sample_size_1+0.5)) + (1/(p2+0.5)) - (1/(sample_size_2+0.5)))\n",
    "\n",
    "    # 1. Walters\n",
    "    upper_rr_walters = np.exp(np.log(Realtive_risk_adjusted_Walters) + zcrit* standart_error_Relative_Risk_Walters)\n",
    "    lower_rr_walters = np.exp(np.log(Realtive_risk_adjusted_Walters) - zcrit* standart_error_Relative_Risk_Walters)\n",
    "\n",
    "    # 2. Katz\n",
    "    upper_rr_kats = np.exp(np.log(Realtive_risk) + zcrit* standart_error_Relative_Risk_katz)\n",
    "    lower_rr_kats = np.exp(np.log(Realtive_risk) - zcrit* standart_error_Relative_Risk_katz)\n",
    "\n",
    "    # 3. Miettinen and Nurminen\n",
    "    def RRci(x1, n1, x2, n2, conf_level):\n",
    "        z = abs(stats.norm.ppf((1 - conf_level) / 2))\n",
    "        a1 = n2 * (n2 * (n2 + n1) * x1 + n1 * (n2 + x1) * (z ** 2))\n",
    "        a2 = -n2 * (n2 * n1 * (x2 + x1) + 2 * (n2 + n1) * x2 * x1 + n1 * (n2 + x2 + 2 * x1) * (z ** 2))\n",
    "        a3 = 2 * n2 * n1 * x2 * (x2 + x1) + (n2 + n1) * (x2 ** 2) * x1 + n2 * n1 * (x2 + x1) * (z ** 2)\n",
    "        a4 = -n1 * (x2 ** 2) * (x2 + x1)\n",
    "        b1 = a2 / a1\n",
    "        b2 = a3 / a1\n",
    "        b3 = a4 / a1\n",
    "        c1 = b2 - (b1 ** 2) / 3\n",
    "        c2 = b3 - b1 * b2 / 3 + 2 * (b1 ** 3) / 27\n",
    "        ceta = math.acos(math.sqrt(27) * c2 / (2 * c1 * math.sqrt(-c1)))\n",
    "        t1 = -2 * math.sqrt(-c1 / 3) * math.cos(math.pi / 3 - ceta / 3)\n",
    "        t2 = -2 * math.sqrt(-c1 / 3) * math.cos(math.pi / 3 + ceta / 3)\n",
    "        t3 = 2 * math.sqrt(-c1 / 3) * math.cos(ceta / 3)\n",
    "        p01 = t1 - b1 / 3\n",
    "        p02 = t2 - b1 / 3\n",
    "        p03 = t3 - b1 / 3\n",
    "        p0sum = p01 + p02 + p03\n",
    "        p0up = min(p01, p02, p03)\n",
    "        p0low = p0sum - p0up - max(p01, p02, p03)\n",
    "        ul = (1 - (n1 - x1) * (1 - p0up) / (x2 + n1 - (n2 + n1) * p0up)) / p0up\n",
    "        ll = (1 - (n1 - x1) * (1 - p0low) / (x2 + n1 - (n2 + n1) * p0low)) / p0low\n",
    "        cint = [ll, ul]\n",
    "        return cint\n",
    "\n",
    "    CI_Koopman = RRci(p1,sample_size_1, p2, sample_size_2, confidence_level)\n",
    "\n",
    "    # 4+5 Inverse Sine and adjusted Inverse Sine\n",
    "    Adjusted_Epsilon = math.asinh(0.5 * zcrit * np.sqrt(1 / p1 + 1 / p2 - 1 / (sample_size_1+ 1) - 1 / (sample_size_2 + 1)))\n",
    "    Epsilon = math.asinh(0.5 * zcrit * np.sqrt(1/p1 + 1/p2 - 1/sample_size_1 - 1/sample_size_2))\n",
    "    Lower_Sinh_adjusted = np.exp(np.log(Realtive_risk) - 2 * Adjusted_Epsilon)\n",
    "    Upper_Sinh_adjusted = np.exp(np.log(Realtive_risk) + 2 * Adjusted_Epsilon)\n",
    "    Lower_Sinh_ = np.exp(np.log(Realtive_risk) - 2 * Epsilon)\n",
    "    Upper_Sinh_ = np.exp(np.log(Realtive_risk) + 2 * Epsilon)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Realtive risk\"] = round(Realtive_risk, 7)\n",
    "    results[\"Realtive risk unbiased (Jewell)\"] = round(Realtive_risk_unbiased, 7)\n",
    "    results[\"Realtive risk adjusted (Walters)\"] = round(Realtive_risk_adjusted_Walters, 7)\n",
    "\n",
    "    results[\"standart_error_Relative_Risk_Katz\"] = round(standart_error_Relative_Risk_katz, 7)\n",
    "    results[\"standart_error_Relative_Risk_WalterS\"] = round(standart_error_Relative_Risk_Walters, 7)\n",
    "\n",
    "    results[\"Confidence Intervals (Katz)\"] = f\"({round(lower_rr_kats, 4)}, {round(upper_rr_kats, 4)})\"\n",
    "    results[\"Confidence (Walters)\"] = f\"({round(lower_rr_walters, 4)}, {round(upper_rr_walters, 4)})\"\n",
    "    results[\"Confidence Intervals (Koopman)\"] = around(CI_Koopman, 4)\n",
    "    results[\"Confidence Intervals Inverse Sine\"] = f\"({round(Lower_Sinh_, 4)}, {round(Upper_Sinh_, 4)})\"\n",
    "    results[\"Confidence Intervals Inverse Sine (Adjusted)\"] = f\"({round(Lower_Sinh_adjusted, 4)}, {round(Upper_Sinh_adjusted, 4)})\"\n",
    "    \n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def risk_measures_of_effect_size_two_indpendent_samples (sample_size_1, sample_size_2, proportion_sample_1, proportion_sample_2,difference_in_population, confidence_level): \n",
    "    p1 = proportion_sample_1 * sample_size_1\n",
    "    p2 = proportion_sample_2 * sample_size_2\n",
    " \n",
    "    sample_size = sample_size_1 + sample_size_2 \n",
    "\n",
    "    # Risk Measures and Ratios\n",
    "    Risk_Difference = proportion_sample_1-proportion_sample_2 #Samples Difference / Atributional Risk / Excess Risk / Risk Reduction / Absoloute Risk Reduction\n",
    "    Relative_Risk = proportion_sample_1 / proportion_sample_2\n",
    "    Population_Attributional_Risk = ((p1/(p1+p2)) * ((Relative_Risk-1) /Relative_Risk)) \n",
    "    NNT = 1 / (proportion_sample_1-proportion_sample_2)\n",
    "    NNH = 1 / (proportion_sample_2-proportion_sample_1)\n",
    "    Incidental_Rate_Exposed = proportion_sample_1\n",
    "    Incidental_Rate_UnExposed = proportion_sample_2\n",
    "    Incidental_Rate_Population = (p1+p2)/sample_size\n",
    "\n",
    "    Risk_Differnce = proportion_sample_1 - proportion_sample_2\n",
    "    Risk_Differnce_percentages = Risk_Differnce * 100\n",
    "    Exposed_Attributable_Fraction =  Risk_Differnce / Incidental_Rate_Exposed\n",
    "    Exposed_Attributable_Fraction_percentages = Exposed_Attributable_Fraction * 100\n",
    "    Population_attributable_risk_percentages = Population_Attributional_Risk \n",
    "    Population_Attributable_Fraction = (Population_Attributional_Risk / Incidental_Rate_Population) / 100\n",
    "    Population_Attributable_Fraction_percentages = Population_Attributable_Fraction \n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results[\"Incidental Rate Exposed\"] = round((Incidental_Rate_Exposed), 4)\n",
    "    results[\"Incidental Rate UnExposed\"] = round((Incidental_Rate_UnExposed), 4)\n",
    "    results[\"Incidental Rate Population\"] = round((Incidental_Rate_Population), 4)\n",
    "\n",
    "    results[\"Risk Difference (Absolute Risk Reduction)\"] = round((proportion_sample_1 - proportion_sample_2), 4)\n",
    "    results[\"Risk Difference (%)\"] = round(abs(Risk_Differnce_percentages), 4)\n",
    "    results[\"Exposed Attributable Fraction\"] = round((Exposed_Attributable_Fraction), 4)\n",
    "    results[\"Exposed_Attributable_Fraction (%)\"] = round((abs(Exposed_Attributable_Fraction_percentages)), 4)\n",
    "\n",
    "    results[\"Population Atributable Risk\"] = round((Population_Attributional_Risk), 4)\n",
    "    results[\"Population Atributable Risk (%) \"] = round((abs(Population_attributable_risk_percentages*100)), 4)\n",
    "    results[\"Population Atributable Fraction (Relative Risk Reduction) \"] = round((Population_Attributable_Fraction), 4)\n",
    "    results[\"Population Atributable Fraction (%) \"] = round((abs(Population_Attributable_Fraction_percentages*100)), 4)\n",
    "\n",
    "    results[\"Number Needed to Treat (NNT)\"] = round(NNT, 7)\n",
    "    results[\"Number Needed to Harm (NNH)\"] = round(NNH, 7)\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "# Effect Sizes Categorical_Correlations\n",
    "def Nominal_Variables_Correlation(contingency_table, confidence_level = 0.95):\n",
    "\n",
    "    # To do \n",
    "    # add the standrd error of cramers  and for phi\n",
    "\n",
    "    #1. Phi/Cramer's v abased on chi square\n",
    "    #coloumn_1, coloumn_2 = Wide_to_Long_2_coloumns(coloumn_1,coloumn_2)\n",
    "    sample_size_1 = len(x)\n",
    "    sample_size_2 = len(y)\n",
    "    sample_size = sample_size_1+sample_size_2\n",
    "    number_of_rows= np.array(contingency_table).shape[0]\n",
    "    number_of_coloumns= np.array(contingency_table).shape[1]\n",
    "\n",
    "    Chisquare_test = scipy.stats.chi2_contingency(contingency_table, correction=False)\n",
    "    Chisquare_test_corrected = scipy.stats.chi2_contingency(contingency_table) #This one is with Yates correction\n",
    "\n",
    "    chi_square_statistic = Chisquare_test[0]\n",
    "    chi_square_p_value = Chisquare_test[1]\n",
    "    chi_square_statistic_corrected = Chisquare_test_corrected[0]\n",
    "    chi_square_p_value_corrected = Chisquare_test_corrected[1]\n",
    "\n",
    "    Phi = np.sqrt(chi_square_statistic/sample_size)\n",
    "    Phi_corrected = np.sqrt(chi_square_statistic_corrected/sample_size)\n",
    "\n",
    "    #2. Contingency  Coefficient and adjusgted contingency coefficient\n",
    "    # adjusted is for situations in which CC can't reach 1\n",
    "\n",
    "    # in a 2x2 max CC and CC are equal  \n",
    "    Contingency_Coefficient = np.sqrt((chi_square_statistic /  (chi_square_statistic+sample_size)))\n",
    "    k = min(number_of_rows,number_of_coloumns)\n",
    "    max_contingency_coefficaient = np.sqrt((k-1)/k)\n",
    "    adjusted_Contingency_Coefficient = Contingency_Coefficient/max_contingency_coefficaient\n",
    "\n",
    "    #3. Tetrechoric correlation\n",
    "    p1 = int(contingency_table[0][0])\n",
    "    p2 = int(contingency_table[0][1])\n",
    "    q1 = int(contingency_table[1][0])\n",
    "    q2 = int(contingency_table[1][1])\n",
    "    Tetrechoric_correlation = np.cos(3.14 / (np.sqrt( (q1*p2) / (p1*q2) ) + 1))\n",
    "\n",
    "    # 4. loglikelihood\n",
    "    LR, Pval_LR = scipy.stats.chi2_contingency(contingency_table, correction=False, lambda_ = 0)[0:2]\n",
    "    MOD_LR, Pval_Mod_LR = scipy.stats.chi2_contingency(contingency_table, correction=False, lambda_ = -1)[0:2]\n",
    "\n",
    "    # 5. Tschuprov's T\n",
    "    Tschuprovs_T = chi_square_statistic/ (sample_size*(number_of_rows-1)*(number_of_coloumns-1))\n",
    "    Tschuprovs_T_corrected = chi_square_statistic_corrected / (sample_size*(number_of_rows-1)*(number_of_coloumns-1))\n",
    "\n",
    "\n",
    "    # 1. uncertinty coefficient\n",
    "    Uncertainty_Coefficient_Output = uncertainty_coefficient(contingency_table, confidence_level)\n",
    "        \n",
    "    # 2.lambda\n",
    "    Goodman_Krushkal_Lambda_Output = goodman_kruskal_lamda_correlation(contingency_table, confidence_level)\n",
    "    \n",
    "    # 3. tau         \n",
    "    Goodman_Krushkal_Tau_Output = Goodman_Kruskal_Tau(contingency_table, confidence_level)\n",
    "\n",
    "\n",
    "    #more options:\n",
    "\n",
    "    #4.1 modified log likelihood (G-test)\n",
    "    #mod_log_likelihood = scipy.stats.chi2_contingency(contingency_table, correction=False, lambda_ = -1)[0]\n",
    "\n",
    "    #4.2 freeman tukey \n",
    "    #tukey_freeman = scipy.stats.chi2_contingency(contingency_table, correction=False, lambda_ = -0.5)[0]\n",
    "\n",
    "    #4.3 Cressie-Read\n",
    "    #Cressie_Read = scipy.stats.chi2_contingency(contingency_table, correction=False, lambda_ = \"cressie-read\")[0]\n",
    "    \n",
    "    #4.4 Neyman \n",
    "    #Neyman = scipy.stats.chi2_contingency(contingency_table, correction=False, lambda_ = -2)[0]\n",
    "    \n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Chi Square\"] = round(chi_square_statistic, 4)\n",
    "    results[\"p.value\"] = round(chi_square_p_value, 4)\n",
    "    results[\"Chi Square (With Yates Correcteion)\"] = round(chi_square_statistic_corrected, 4)\n",
    "    results[\"p.value_(With Yates Correction)\"] = round(chi_square_p_value_corrected, 4)\n",
    "    results[\"Likelihood Ratio\"] = round(LR, 4)\n",
    "    results[\"p.value Likelihood Ratio\"] = round(Pval_LR, 4)\n",
    "    results[\"Phi/Cramer's V\"] = round(Phi, 4)\n",
    "    results[\"Phi/Cramer's V Corrected\"] = round(Phi_corrected, 4)\n",
    "    results[\"Maximum Correcte Cramer's V\"] = round(Phi_corrected, 4)\n",
    "    results[\"Tschuprovs T\"] = round(Tschuprovs_T, 4)\n",
    "    results[\"Tschuprovs T Corrected\"] = round(Tschuprovs_T_corrected, 4)\n",
    "    results[\"Contingency Coefficient\"] = round(Contingency_Coefficient, 4)\n",
    "    results[\"Contingency Coefficient\"] = round(adjusted_Contingency_Coefficient, 4)\n",
    "    results[\"Tetrechoric_correlation's v corrected\"] = round(Tetrechoric_correlation, 4)\n",
    "    results[\"Goodman Kruskal Lambda Table\"] = Goodman_Krushkal_Lambda_Output\n",
    "    results[\"Goodman Kruskal Tau Table\"] = Goodman_Krushkal_Tau_Output\n",
    "    results[\"Uncrtinty Coefficeint Table\"] = Uncertainty_Coefficient_Output\n",
    "\n",
    "\n",
    "    #results[\"tukey_freeman\"] = round(tukey_freeman, 4)\n",
    "    #results[\"mod_log_likelihood\"] = round(mod_log_likelihood, 4)\n",
    "    #results[\"Cressie_Read\"] = round(Cressie_Read, 4)\n",
    "    #results[\"Neyman\"] = round(Neyman, 4)\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def effect_sizes_difference_two_dependent_proportions_2x2(yes_yes,yes_no,no_yes,no_no, confidence_level):\n",
    "\n",
    "    # TO DO \n",
    "    # Consider adding the log odd ratios\n",
    "    #adding +0.5 to deal with zeros\n",
    "    #Check Panello 2010 for multiple Proportions\n",
    "\n",
    "    sample_size = yes_yes + yes_no + no_yes + no_no\n",
    "    Delta = 0.95     #This is just for Debugging\n",
    "\n",
    "    # p1total = The number of peopole that said yes in Variable 1 (including yes1 and no1 and yes1 and no2)\n",
    "    # p2total = The number of peopole that said yes in Variable 2 (including yes2 and no1 and yes2 and no2)\n",
    "    p1total = yes_yes + yes_no\n",
    "    p2total = yes_yes + no_yes\n",
    "    Proportion_Sample_1 = p1total / sample_size\n",
    "    Proportion_Sample_2 = p2total / sample_size\n",
    "    Proportion_Matching = (yes_yes + no_no) / sample_size\n",
    "    Proportion_Not_Matching = 1 - Proportion_Matching\n",
    "    contingency_table = [[yes_yes, yes_no], [no_yes, no_no]]\n",
    "    x = np.array(contingency_table).flatten()\n",
    "\n",
    "    #These are the counts of the sum rows and coloumns of the 2X2 matrix\n",
    "    p1 = yes_yes+yes_no; p2=yes_yes+no_yes; q1 = no_yes+no_no; q2=yes_no+no_no\n",
    "\n",
    "    #Difference Between Proportions and confidence intervals\n",
    "    difference_between_proportions = Proportion_Sample_1 - Proportion_Sample_2\n",
    "\n",
    "    # Confidence Intervals\n",
    "    z_critical_value = st.norm.ppf(confidence_level + ((1 - confidence_level) / 2))\n",
    "\n",
    "    # 1. Wald CI's\n",
    "    Standard_Error_Wald = np.sqrt((yes_no + no_yes) - ((yes_no - no_yes)**2) / sample_size) / sample_size\n",
    "    LowerCi_WALD = max(difference_between_proportions - z_critical_value *Standard_Error_Wald,-1 )\n",
    "    UpperCi_WALD = min(difference_between_proportions + z_critical_value *Standard_Error_Wald,1 )\n",
    "\n",
    "    # 2. Wald with CC correction (Fleiss et al., 2003)\n",
    "    Standard_Error_Wald_Corrected = np.sqrt((yes_no + no_yes) - ((yes_no - no_yes)**2) / sample_size) / sample_size\n",
    "    LowerCi_WALD_Corrected = max(difference_between_proportions - z_critical_value *Standard_Error_Wald_Corrected - (1/sample_size),-1 )\n",
    "    UpperCi_WALD_Corrected = min(difference_between_proportions + z_critical_value *Standard_Error_Wald_Corrected + (1/sample_size),1 )\n",
    "    \n",
    "    # 3. Wald with Yates correction\n",
    "    Standard_Error_Wald_Corrected_yates = np.sqrt((yes_no + no_yes) - ((yes_no - no_yes - 1)**2) / sample_size) / sample_size\n",
    "    LowerCi_WALD_Corrected_Yates = max(difference_between_proportions - z_critical_value *Standard_Error_Wald_Corrected_yates,-1 )\n",
    "    UpperCi_WALD_Corrected_Yates = min(difference_between_proportions + z_critical_value *Standard_Error_Wald_Corrected_yates,1 )\n",
    "    \n",
    "    # 4. Agresti & Min (2005)\n",
    "    Standard_Error_AM =np.sqrt(((yes_no+0.5) + (no_yes+0.5)) - (((yes_no+0.5) - (no_yes+0.5))**2) / (sample_size+2)) / (sample_size+2)\n",
    "    LowerCi_AM = max(((yes_no+0.5) - (no_yes+0.5)) /  (sample_size + 2) - z_critical_value * Standard_Error_AM,-1 )\n",
    "    UpperCi_AM = min(((yes_no+0.5) - (no_yes+0.5)) /  (sample_size + 2) + z_critical_value * Standard_Error_AM, 1 )\n",
    "\n",
    "    # 5. Bonett & Price (2005)\n",
    "    p1_adjusted = (yes_no + 1) / (sample_size + 2)\n",
    "    p2_adjusted = (no_yes + 1) / (sample_size + 2)\n",
    "    Standard_Error_BP = np.sqrt((p1_adjusted+p2_adjusted-(p2_adjusted-p1_adjusted)**2)/(sample_size+2))\n",
    "    LowerCi_BP = max(p1_adjusted - p2_adjusted - z_critical_value * Standard_Error_BP,-1 )\n",
    "    UpperCi_BP = min(p1_adjusted - p2_adjusted + z_critical_value * Standard_Error_BP,1 )\n",
    "\n",
    "    # 6. Newcomb, Square and Add\n",
    "    \n",
    "    A1 = (2 * sample_size * ((p1total)/sample_size) + z_critical_value**2) / (2 * sample_size + 2 * z_critical_value**2)\n",
    "    B1 = (z_critical_value * np.sqrt(z_critical_value**2 + 4 * sample_size * (p1total/sample_size) * (1 - (p1total/sample_size)))) / (2 * sample_size + 2 * z_critical_value**2)  \n",
    "    A2 = (2 * sample_size * ((p2total)/sample_size) + z_critical_value**2) / (2 * sample_size + 2 * z_critical_value**2)\n",
    "    B2 = (z_critical_value * np.sqrt(z_critical_value**2 + 4 * sample_size * (p2total/sample_size) * (1 - (p2total/sample_size)))) / (2 * sample_size + 2 * z_critical_value**2)\n",
    "    lower_p1 = A1 - B1\n",
    "    upper_p1 = A1 + B1\n",
    "    lower_p2 = A2 - B2\n",
    "    upper_p2 = A2 + B2\n",
    "\n",
    "    if p1total == 0 or p2total == 0 or (sample_size-p1total) == 0 or (sample_size - p2total) == 0:\n",
    "             products_correction= 0\n",
    "    else:\n",
    "        marginals_product = p1total*p2total*(sample_size-p1total)*(sample_size-p2total)\n",
    "        cells_product = yes_yes*no_no - no_yes*no_yes\n",
    "        if cells_product > sample_size / 2:\n",
    "            products_correction = (cells_product - sample_size / 2) / np.sqrt(marginals_product)\n",
    "        elif cells_product >= 0 and cells_product <= sample_size / 2:\n",
    "            products_correction = 0\n",
    "        else:\n",
    "            products_correction = cells_product / np.sqrt(marginals_product)\n",
    "\n",
    "    LowerCi_newcomb = difference_between_proportions - np.sqrt((Proportion_Sample_1 - lower_p1)**2 + (upper_p2 - Proportion_Sample_2)**2 - 2 * products_correction * (Proportion_Sample_1 - lower_p1) * (upper_p2 - Proportion_Sample_2))\n",
    "    UpperCi_newcomb = difference_between_proportions + np.sqrt((Proportion_Sample_2 - lower_p2)**2 + (upper_p1 - Proportion_Sample_1)**2 - 2 * products_correction * (Proportion_Sample_2 - lower_p2) * (upper_p1 - Proportion_Sample_1))\n",
    "    \n",
    "    # Significance\n",
    "\n",
    "    # Mcnemar Test (2x2)\n",
    "    exact_mcnemar = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "    mcnemar_chi_square = ((abs(yes_no - no_yes))**2) / (yes_no+no_yes)\n",
    "    mcnemar_chi_square_corrected = ((abs(yes_no - no_yes) - 1)**2) / (yes_no+no_yes)\n",
    "\n",
    "    mcnemar_pvalue = calculate_p_value_from_chi_score(mcnemar_chi_square,1)\n",
    "    mcnemar_pvalue_corrected = calculate_p_value_from_chi_score(mcnemar_chi_square_corrected,1)\n",
    "\n",
    "    # Optional - Can check this built in Functions for more version of the mcnemar test\n",
    "    #data = [[p1, q1], [p2, q2]]\n",
    "    #a = (mcnemar(data, exact=True))\n",
    "    #b = (mcnemar(data, exact=False, correction=False))\n",
    "    #c = (mcnemar(data, exact=False, correction=True))\n",
    "\n",
    "\n",
    "    # Matched Pairs Odds Ratio\n",
    "    ##########################\n",
    "    # 1+ 2 Matched Pairs Odds Ratio and adjusted matched pairs odds ratio\n",
    "    Match_Pairs_Odds_Ratio = yes_no / no_yes\n",
    "    Match_Pairs_Odds_Ratio_Adjusted = (yes_no / (no_yes+1))\n",
    "\n",
    "    #More rare adjusments for the matched pairs odds ratio (Jeweel, 1984) - fits small samples\n",
    "    #3. Jacknifhe odds ratio\n",
    "    Odds_Ratio_Matched_Pairs_Jackknife = Match_Pairs_Odds_Ratio - ((sample_size - 1) / (no_yes-1)) * (Match_Pairs_Odds_Ratio/sample_size)\n",
    "    Odds_Ratio_Bias_Corrected = Match_Pairs_Odds_Ratio * (1-no_yes**-1)\n",
    "\n",
    "    # Confidence Intervals for the matched pairs Odds Ratio\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "\n",
    "    # 1. Rigby & Robinsone (2000)\n",
    "    CI_Mcnemar_odds_ratio_lower = Match_Pairs_Odds_Ratio**(1-zcrit/np.sqrt(mcnemar_chi_square))\n",
    "    CI_Mcnemar_odds_ratio_upper = Match_Pairs_Odds_Ratio**(1+zcrit/np.sqrt(mcnemar_chi_square))\n",
    "    \n",
    "    CI_Mcnemar_corrected_odds_ratio_lower = Match_Pairs_Odds_Ratio**(1-zcrit/np.sqrt(mcnemar_chi_square_corrected))\n",
    "    CI_Mcnemar_corrected_odds_ratio_upper = Match_Pairs_Odds_Ratio**(1+zcrit/np.sqrt(mcnemar_chi_square_corrected))\n",
    "\n",
    "    # 2. Logarithmic (find the source)\n",
    "    Standrd_Error_Odds_ratio = np.sqrt(1/yes_no + 1/no_yes) \n",
    "    CI_odds_ratio_log_lower = Match_Pairs_Odds_Ratio * math.exp(-zcrit*Standrd_Error_Odds_ratio)\n",
    "    CI_odds_ratio_log_upper = Match_Pairs_Odds_Ratio * math.exp(zcrit*Standrd_Error_Odds_ratio)\n",
    "\n",
    "    # 3. Binomial Methods - basicly transfer the proportions to odds ratio by (p1/(1-p1)) \n",
    "    # Note: Delta=Logit, Score = Wilson, IM= Clopper-Pearson, Fiducial = Jeffereys - These are the names in Chen et al., 2020\n",
    "\n",
    "    # 4. The Score Method also known as the Wilson score Method\n",
    "    A = (2*yes_no*no_yes) + zcrit**2*(yes_no+no_yes)\n",
    "    lower_ci_score_method = (A - np.sqrt(A**2 - ((2*yes_no*no_yes)**2))) / (2*no_yes**2)\n",
    "    upper_ci_score_method = (A + np.sqrt(A**2 - ((2*yes_no*no_yes)**2))) / (2*no_yes**2)\n",
    "\n",
    "\n",
    "    #Matched Pairs Relative Risk:\n",
    "    Matched_Pairs_Relative_Risk = (yes_yes+yes_no)/ (yes_yes+no_yes)\n",
    "    sample_size = yes_yes+yes_no+no_no+no_yes\n",
    "    Realtive_risk_unbiased = (p1*(q1+q2+1)) / (((p1+p2) * (q1 + 1)))\n",
    "    Realtive_risk_adjusted_Walters = np.exp(np.log((p1+0.5) / (sample_size+0.5)) - np.log((p2+0.5) / (sample_size+0.5)))\n",
    "\n",
    "    # Statistics, Standard Errors and Confidence Intervals for the rate ratio(Relative Risk)\n",
    "    #################################################################################\n",
    "\n",
    "    #Function based on ratesci\n",
    "    def Tang_T1_function(x, Delta): \n",
    "        N = np.sum(x)\n",
    "        Stheta = ((x[1] + x[0]) - (x[2] + x[0]) * Delta)\n",
    "        A = N * (1 + Delta)\n",
    "        B = (x[0] + x[2]) * Delta**2 - (x[0] + x[1] + 2 * x[2])\n",
    "        C_ = x[2] * (1 - Delta) * (x[0] + x[1] + x[2]) / N\n",
    "        num = -B + np.sqrt(B**2 - 4 * A * C_)\n",
    "        q21 = num / (2 * A)\n",
    "        Variance = np.maximum(0, N * (1 + Delta) * q21 + (x[0] + x[1] + x[2]) * (Delta - 1))\n",
    "        Z_Score_Tang = Stheta / np.sqrt(Variance)\n",
    "        return Z_Score_Tang, np.sqrt(Variance)\n",
    "    \n",
    "    def Tang_CI_Function(Function, Maximum_Iterations=100, CI = \"lower_CI\"):\n",
    "        hi = 1; lo = -1; niter = 1\n",
    "        while niter <= Maximum_Iterations:\n",
    "            mid = max(-1, min(1, (hi + lo) / 2))\n",
    "            scor = Function(np.tan(np.pi * (mid + 1) / 4))\n",
    "            check = (scor <= 0) or np.isnan(scor)\n",
    "            hi = mid if check else hi\n",
    "            lo = mid if not check else lo\n",
    "            niter += 1\n",
    "        Optimize = lo if CI == \"lower_CI\" else hi\n",
    "        return np.tan((Optimize + 1) * np.pi / 4)\n",
    "\n",
    "    def myfun(Delta):\n",
    "        return Tang_T1_function(Delta=Delta, x=x)[0]\n",
    "    Statistic_Tang, Standard_Deviation_Tang = Tang_T1_function(Delta=Delta, x=x)\n",
    "    Standard_Error_Tang = Standard_Deviation_Tang / np.sqrt((sample_size-no_no)**2)\n",
    "    P_Val_Tang = calculate_p_value_from_z_score(Statistic_Tang)\n",
    "    CI_Tang_lower = Tang_CI_Function(lambda Delta: myfun(Delta) - zcrit, CI=\"lower_CI\")\n",
    "    CI_Tang_Upper = Tang_CI_Function(lambda Delta: myfun(Delta) + zcrit, CI=\"upper_CI\")\n",
    "\n",
    "    # 2. Delta Method\n",
    "    Standard_Error_Delta = np.sqrt((yes_yes+yes_no)*(yes_no+no_yes)/(yes_yes+no_yes)**3)\n",
    "    Statistic_Delta = (Matched_Pairs_Relative_Risk - Delta) / Standard_Error_Delta\n",
    "    P_Val_Delta = calculate_p_value_from_z_score(Statistic_Delta)\n",
    "    CI_delta_lower = Matched_Pairs_Relative_Risk - Standard_Error_Delta * zcrit\n",
    "    CI_delta_Upper = Matched_Pairs_Relative_Risk + Standard_Error_Delta * zcrit\n",
    "\n",
    "    # 3. Delta Method Modified\n",
    "    Standard_Error_Delta_modified = np.sqrt(Delta * (yes_no+no_yes)/(yes_yes+no_yes)**2)\n",
    "    Statistic_Delta_modified = (Matched_Pairs_Relative_Risk - Delta) / Standard_Error_Delta_modified\n",
    "    P_Val_Delta_modified = calculate_p_value_from_z_score(Statistic_Delta_modified)\n",
    "    CI_delta_lower_modified = Matched_Pairs_Relative_Risk - Standard_Error_Delta_modified * zcrit\n",
    "    CI_delta_Upper_modified = Matched_Pairs_Relative_Risk + Standard_Error_Delta_modified * zcrit\n",
    "\n",
    "    # 4. Katz Logarithmic\n",
    "    Standard_Error_Katz = np.sqrt((no_yes+yes_no)/((yes_yes+yes_no)*(yes_yes+no_yes)))\n",
    "    Statistic_Katz = (math.log(yes_no+yes_yes) - math.log(yes_yes+no_yes) - math.log(Delta)) / Standard_Error_Katz\n",
    "    P_Val_Katz = calculate_p_value_from_z_score(Statistic_Katz)\n",
    "    CI_katz_lower = Matched_Pairs_Relative_Risk - Standard_Error_Katz * zcrit\n",
    "    CI_katz_Upper = Matched_Pairs_Relative_Risk + Standard_Error_Katz * zcrit\n",
    "\n",
    "    # 5. Katz logarithmic Modified\n",
    "    Standard_Error_Katz_Modified = np.sqrt((yes_no + no_yes) / (Delta*(yes_yes+no_yes)**2))\n",
    "    Statistic_Katz_Modified = (math.log(yes_no+yes_yes) - math.log(yes_yes+no_yes) - math.log(Delta))  / Standard_Error_Katz_Modified\n",
    "    P_Val_Katz_Modified = calculate_p_value_from_z_score(Statistic_Katz_Modified)\n",
    "    CI_katz_modified_lower = Matched_Pairs_Relative_Risk - Standard_Error_Katz_Modified * zcrit\n",
    "    CI_katz_modified_Upper = Matched_Pairs_Relative_Risk + Standard_Error_Katz_Modified * zcrit\n",
    "\n",
    "    # 6. Lachenbruch and Lynch\n",
    "    Standard_Deviation_Lachenbruch_Lynch = np.sqrt((yes_no*(sample_size-yes_no) + yes_yes*(sample_size-yes_yes)*(1-Delta)**2 + no_yes*(sample_size-no_yes)*Delta**2 + 2*yes_no*no_yes*Delta-2*yes_yes*yes_no*(1-Delta) + 2*yes_yes*no_yes*Delta*(1-Delta))/ sample_size)\n",
    "    Standard_Error_Lachenbruch_Lynch = Standard_Deviation_Lachenbruch_Lynch / np.sqrt((sample_size-no_no)**2)\n",
    "    Statistic_Lachenbruch_Lynch = ((yes_yes+yes_no) - Delta*(yes_yes+no_yes)) / Standard_Deviation_Lachenbruch_Lynch\n",
    "    P_Val_Lachenbruch_Lynce = calculate_p_value_from_z_score(Statistic_Lachenbruch_Lynch)\n",
    "    CI_Lachenbruch_Lynce_lower = Matched_Pairs_Relative_Risk - Standard_Error_Lachenbruch_Lynch * zcrit\n",
    "    CI_Lachenbruch_Lynce_Upper = Matched_Pairs_Relative_Risk + Standard_Error_Lachenbruch_Lynch * zcrit\n",
    "\n",
    "    # Other Relative Measures\n",
    "    Population_Attributional_Risk = ((p1/(p1+p2)) * ((Matched_Pairs_Relative_Risk-1) /Matched_Pairs_Relative_Risk)) \n",
    "    NNT = 1 / (Proportion_Sample_1-Proportion_Sample_2)\n",
    "    NNH = 1 / (Proportion_Sample_2-Proportion_Sample_1)\n",
    "    Incidental_Rate_Exposed = Proportion_Sample_1\n",
    "    Incidental_Rate_UnExposed = Proportion_Sample_2\n",
    "    Incidental_Rate_Population = (p1+p2)/sample_size\n",
    "\n",
    "    Risk_Differnce = Proportion_Sample_1 - Proportion_Sample_2\n",
    "    Risk_Differnce_percentages = Risk_Differnce * 100\n",
    "    Exposed_Attributable_Fraction =  Risk_Differnce / Incidental_Rate_Exposed\n",
    "    Exposed_Attributable_Fraction_percentages = Exposed_Attributable_Fraction * 100\n",
    "    Population_attributable_risk_percentages = Population_Attributional_Risk \n",
    "    Population_Attributable_Fraction = (Population_Attributional_Risk / Incidental_Rate_Population) / 100\n",
    "    Population_Attributable_Fraction_percentages = Population_Attributable_Fraction \n",
    "\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    results[\"Proportion of Sucess of Variable 1\"] = round(Proportion_Sample_1, 7)\n",
    "    results[\"Proportion of Sucess of Variable 2\"] = round(Proportion_Sample_2, 7)\n",
    "    results[\"Proportion Matching\"] = round(Proportion_Matching, 7)\n",
    "    results[\"Proportion not Matching\"] = round(Proportion_Not_Matching, 7)\n",
    "    \n",
    "    results[\"Difference Between Proportions\"] = round(difference_between_proportions, 7)\n",
    "    results[\"Confidence Intervals Wald\"] = f\"({round(LowerCi_WALD, 4)}, {round(UpperCi_WALD, 4)})\"\n",
    "    results[\"Confidence Intervals Wald Corrected (Edwards)\"] = f\"({round(LowerCi_WALD_Corrected, 4)}, {round(UpperCi_WALD_Corrected, 4)})\"\n",
    "    results[\"Confidence Intervals Wald Corrected (Yates)\"] = f\"({round(LowerCi_WALD_Corrected_Yates, 4)}, {round(UpperCi_WALD_Corrected_Yates, 4)})\"\n",
    "    results[\"Confidence Intervals adjusted (Agresti & Min, 2005)\"] = f\"({round(LowerCi_AM, 4)}, {round(UpperCi_AM, 4)})\"\n",
    "    results[\"Confidence Intervals adjusted (Bonett & Price, 2012)\"] = f\"({round(LowerCi_BP, 4)}, {round(UpperCi_BP, 4)})\"\n",
    "    results[\"Confidence Intervals (NewComb)\"] = f\"({round(LowerCi_newcomb, 4)}, {round(UpperCi_newcomb, 4)})\"\n",
    "\n",
    "    results[\"McNemar Chi Square Statistic\"] = f\"({round(mcnemar_chi_square, 4)})\"\n",
    "    results[\"McNemar Chi Square Statistic Corrected)\"] = f\"({round(mcnemar_chi_square_corrected, 4)})\"\n",
    "    results[\"McNemar pvalue\"] = f\"({round(mcnemar_pvalue, 4)})\"\n",
    "    results[\"McNemar pvalue corrected)\"] = f\"({round(mcnemar_pvalue_corrected, 4)})\"\n",
    "    results[\"McNemar Exact)\"] = f\"({(exact_mcnemar)})\"\n",
    "\n",
    "    results[\"Matched Pairs Odds Ratio)\"] = f\"({round(Match_Pairs_Odds_Ratio, 4)})\"\n",
    "    results[\"Matched Pairs Odds Ratio (Adjusted)\"] = f\"({round(Match_Pairs_Odds_Ratio_Adjusted, 4)})\"\n",
    "    results[\"Matched Pairs Odds Ratio (Jacknife Correction)\"] = f\"({round(Odds_Ratio_Matched_Pairs_Jackknife, 4)})\"\n",
    "    results[\"Matched Pairs Odds Ratio)\"] = f\"({round(Odds_Ratio_Bias_Corrected, 4)})\"\n",
    "\n",
    "    results[\"Matched Pairs Odds Ratio CI's (Rigby & Robinsone)\"] = f\"({round(CI_Mcnemar_odds_ratio_lower, 4)}, {round(CI_Mcnemar_odds_ratio_upper, 4)})\"\n",
    "    results[\"Matched Pairs Odds Ratio Corrected CI's (Rigby & Robinsone)\"] = f\"({round(CI_Mcnemar_corrected_odds_ratio_lower, 4)}, {round(CI_Mcnemar_corrected_odds_ratio_upper, 4)})\"\n",
    "    \n",
    "    # All these confidence intervals are also computed for one sample proportion\n",
    "    results[\"Matched Pairs Odds Ratio CI's (Logarithmic / Delta Method)\"] = f\"({round(CI_odds_ratio_log_lower, 4)}, {round(CI_odds_ratio_log_upper, 4)})\"\n",
    "    results[\"Matched Pairs Odds Ratio CI's Score Method\"] = f\"({round(lower_ci_score_method, 4)}, {round(upper_ci_score_method, 4)})\"\n",
    "\n",
    "    results[\"Matched Pairs Relative Risk\"] = f\"({round(Matched_Pairs_Relative_Risk, 4)})\"\n",
    "    results[\"Matched Pairs Relative Risk (Unbiased)\"] = f\"({round(Realtive_risk_unbiased, 4)})\"\n",
    "    results[\"Matched Pairs Relative Risk (Adjusted Walters)\"] = f\"({round(Realtive_risk_adjusted_Walters, 4)})\"\n",
    "    \n",
    "    results[\"Rate Ratio Z-score (Tang T1 Method)\"] = round(Statistic_Tang, 4)\n",
    "    results[\"Rate Ratio Z-score (Delta Method)\"] = round(Statistic_Delta, 4)\n",
    "    results[\"Rate Ratio Z-score (Delta Method Modified)\"] = round(Statistic_Delta_modified, 4)\n",
    "    results[\"Rate Ratio Z-score (Katz Logarithmic)\"] = round(Statistic_Katz, 4)\n",
    "    results[\"Rate Ratio Z-score (Katz Logarithmic Modified)\"] = round(Statistic_Katz_Modified, 4)\n",
    "    results[\"Rate Ratio Z-score (Lachenbruch and Lynch)\"] = round(Statistic_Lachenbruch_Lynch, 4)\n",
    "\n",
    "    results[\"Rate Ratio P-value (Tang T1 Method)\"] = round(P_Val_Tang, 4)\n",
    "    results[\"Rate Ratio P-value (Delta Method)\"] = round(P_Val_Delta, 4)\n",
    "    results[\"Rate Ratio P-value (Delta Method Modified)\"] = round(P_Val_Delta_modified, 4)\n",
    "    results[\"Rate Ratio P-value (Katz Logarithmic)\"] = round(P_Val_Katz, 4)\n",
    "    results[\"Rate Ratio P-value (Katz Logarithmic Modified)\"] = round(P_Val_Katz_Modified, 4)\n",
    "    results[\"Rate Ratio P-value (Lachenbruch and Lynch)\"] = round(P_Val_Lachenbruch_Lynce, 4)\n",
    "\n",
    "    results[\"Rate Ratio CI's (Tang T1 Method)\"] = f\"({round(CI_Tang_lower, 4)}, {round(CI_Tang_Upper, 4)})\"\n",
    "    results[\"Rate Ratio CI's (Delta Method)\"] = f\"({round(CI_delta_lower, 4)}, {round(CI_delta_Upper, 4)})\"\n",
    "    results[\"Rate Ratio CI's (Delta Method Modified)\"] = f\"({round(CI_delta_lower_modified, 4)}, {round(CI_delta_Upper_modified, 4)})\"\n",
    "    results[\"Rate Ratio CI's (Katz Logarithmic)\"] = f\"({round(CI_katz_lower, 4)}, {round(CI_katz_Upper, 4)})\"\n",
    "    results[\"Rate Ratio CI's (Katz Logarithmic Modified)\"] = f\"({round(CI_katz_modified_lower, 4)}, {round(CI_katz_modified_Upper, 4)})\"\n",
    "    results[\"Rate Ratio CI's (Lachenbruch and Lynch)\"] = f\"({round(CI_Lachenbruch_Lynce_lower, 4)}, {round(CI_Lachenbruch_Lynce_Upper, 4)})\"\n",
    "\n",
    "    results[\"Rate Ratio Standard Error (Tang T1 Method)\"] = round(Standard_Error_Tang, 4)\n",
    "    results[\"Rate Ratio Standard Error (Delta Method)\"] = round(Standard_Error_Delta, 4)\n",
    "    results[\"Rate Ratio Standard Error (Delta Method Modified)\"] = round(Standard_Error_Delta_modified, 4)\n",
    "    results[\"Rate Ratio Standard Error (Katz Logarithmic)\"] = round(Standard_Error_Katz, 4)\n",
    "    results[\"Rate Ratio Standard Error (Katz Logarithmic Modified)\"] = round(Standard_Error_Katz_Modified, 4)\n",
    "    results[\"Rate Ratio Standard Error (Lachenbruch and Lynch)\"] = round(Standard_Error_Lachenbruch_Lynch, 4)\n",
    "\n",
    "    results[\"Population Attributional Risk\"] = round(Population_Attributional_Risk, 4)\n",
    "    results[\"Number Needed to Treat (NNT)\"] = round(NNT, 4)\n",
    "    results[\"Number Needed to Harm (NNH)\"] = round(NNH, 4)\n",
    "    results[\"Incidental Rate Exposed\"] = round(Incidental_Rate_Exposed, 4)\n",
    "    results[\"Incidental Rate Unexposed\"] = round(Incidental_Rate_UnExposed, 4)\n",
    "    results[\"Incidental Rate Population\"] = round(Incidental_Rate_Population, 4)\n",
    "\n",
    "\n",
    "    results[\"Risk Difference (Absolute Risk Reduction)\"] = round((Proportion_Sample_1 - Proportion_Sample_2), 4)\n",
    "    results[\"Risk Difference (%)\"] = round(abs(Risk_Differnce_percentages), 4)\n",
    "    results[\"Exposed Attributable Fraction\"] = round((Exposed_Attributable_Fraction), 4)\n",
    "    results[\"Exposed_Attributable_Fraction (%)\"] = round((abs(Exposed_Attributable_Fraction_percentages)), 4)\n",
    "\n",
    "    results[\"Population Atributable Risk\"] = round((Population_Attributional_Risk), 4)\n",
    "    results[\"Population Atributable Risk (%) \"] = round((abs(Population_attributable_risk_percentages*100)), 4)\n",
    "    results[\"Population Atributable Fraction (Relative Risk Reduction) \"] = round((Population_Attributable_Fraction), 4)\n",
    "    results[\"Population Atributable Fraction (%) \"] = round((abs(Population_Attributable_Fraction_percentages*100)), 4)\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "#Cochran todo = it only works with 1 and 0 so change the first value in data to sucess (1) and the second failure (0)\n",
    "def Cochran_Q_based_Effect_Size(Final_Data):\n",
    "    sample_size = len(Final_Data)\n",
    "\n",
    "    Variables_Number = Final_Data.shape[1]\n",
    "    Degrees_Of_Freedom = Variables_Number - 1\n",
    "    row_sums = np.sum(Final_Data, axis=1)\n",
    "    Pis = ((1/Variables_Number)*row_sums)\n",
    "    A = sum(Pis)\n",
    "    B = sample_size - A\n",
    "    C = Degrees_Of_Freedom / (2*sum(Pis * (1-Pis)))\n",
    "\n",
    "    # Display the transposed DataFrame\n",
    "    Q = cochrans_q(Final_Data).statistic\n",
    "    pval = cochrans_q(Final_Data).pvalue\n",
    "    VarianceQ = (Q/C - 2*A*B) / (-sample_size *(sample_size-1))\n",
    "    MeanQ = (2 / (sample_size*(sample_size-1))) * (A * B - (sum(Pis * (1-Pis))))\n",
    "    Effect_Size_R = 1 - VarianceQ/MeanQ\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    results[\"Cochrans Q\"] = round(Q, 7)\n",
    "    results[\"Degrees of Freedom\"] = round(Degrees_Of_Freedom)\n",
    "    results[\"Chocran's Q p-value\"] = round(pval, 7)\n",
    "    results[\"Variance of Q\"] = round(VarianceQ)\n",
    "    results[\"Mean Q\"] = round(MeanQ, 7)\n",
    "    results[\"Chance Corrected Q-based Effect Size (Berry et al., 2010)\"] = round(Effect_Size_R, 7)\n",
    "\n",
    "    return results\n",
    "\n",
    "def goodness_of_fit_from_frequency(column_1, expected_proportions=None, expected_frequencies=None, expected_ratios=None, confidence_level=None):\n",
    "    data_series = pd.Series(column_1)\n",
    "    Observed = data_series.value_counts()\n",
    "    sample_size = sum(Observed)\n",
    "    Final_Data = pd.DataFrame({'level name': Observed.index, 'frequency': Observed.values})\n",
    "    levels_number = Final_Data.shape[0]\n",
    "\n",
    "\n",
    "    if expected_proportions is not None:\n",
    "        Expected = np.array(expected_proportions) * sample_size\n",
    "    elif expected_frequencies is not None:\n",
    "        Expected = np.array(expected_frequencies)\n",
    "    elif expected_ratios is not None:\n",
    "        ratio_sum = sum(expected_ratios)\n",
    "        Expected = (np.array(expected_ratios) / ratio_sum) * sample_size\n",
    "    else:\n",
    "        Expected = np.array([(1 / levels_number)] * levels_number) * sample_size\n",
    "\n",
    "    degrees_of_freedom = levels_number - 1\n",
    "    \n",
    "    # Pearson Chi Square Test\n",
    "    Chi_square = sum((Observed - Expected)**2 / Expected)\n",
    "    p_value_chi_square = calculate_p_value_from_chi_score( Chi_square,degrees_of_freedom)\n",
    "    \n",
    "    # Wilks_G_Square Test\n",
    "    Observed_Proportions = Observed / sample_size\n",
    "    Expected_Proportions = Expected / sample_size\n",
    "\n",
    "    Wilks_G_Square = 2 *sample_size * (sum(Observed_Proportions * np.log(Observed_Proportions/Expected_Proportions)))\n",
    "\n",
    "\n",
    "    # Effect Sizes\n",
    "\n",
    "    # Cohens W\n",
    "    Cohens_w = np.sqrt(Chi_square / sample_size)\n",
    "\n",
    "    # maximum-corrected pearson chi_square\n",
    "    q_chi = min(Expected)\n",
    "    max_chi_Square = (sample_size*(sample_size-q_chi))/q_chi\n",
    "    max_corrected_lambda = Chi_square/max_chi_Square\n",
    "    \n",
    "    # maximum-corrected wilks g square\n",
    "    q_wilks = min(Expected_Proportions)\n",
    "    max_wilks_G = -2*sample_size*np.log(q_wilks)\n",
    "    max_corrected_gamma = Wilks_G_Square / max_wilks_G\n",
    "\n",
    "    # Chance_Corrected measure of effect size\n",
    "    Variance = (1/ levels_number) * (np.sum((np.array(Observed_Proportions) - (Expected_Proportions))**2))\n",
    "    Mean = (1/levels_number**2) * (np.sum(([(elem1 - elem2)**2 for elem1 in np.array(Observed_Proportions) for elem2 in Expected_Proportions])))\n",
    "    Chance_Corrected_R = Variance / Mean\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Chi Square\"] = round(Chi_square, 7)\n",
    "    results[\"Degrees of Freedom\"] = round(degrees_of_freedom, 7)\n",
    "    results[\"p value Chi Square\"] = round(p_value_chi_square)\n",
    "    results[\"Wilks' G Square\"] = round(Wilks_G_Square, 7)\n",
    "    results[\"max_corrected_lambda\"] = round(max_corrected_lambda,4)\n",
    "    results[\"max_corrected_gamma\"] = around(max_corrected_gamma,4)\n",
    "    results[\"Chance_Corrected_R\"] = Chance_Corrected_R\n",
    "    results[\"Variance of R\"] = around(Variance,4)\n",
    "    results[\"Mean of R\"] = Mean\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "################################################################################\n",
    "####################       Medians    ##########################################\n",
    "################################################################################\n",
    "\n",
    "#### General Functions for Medians\n",
    "\n",
    "# 1.  Function for confidence intervals for Hodges-Lehmann based on Wilcox test (both within and between subject)\n",
    "def Hodges_Lehmann_based_on_wilcox_test(x, y, confidence_level=confidence_level):\n",
    "    alpha = 1-confidence_level\n",
    "    sample_size_1 = len(x)\n",
    "    sample_size_2 = len(y) if y is not None else 0\n",
    "    if y is None:\n",
    "        def Hodges_Lhemann_Estimation_differences_sample(Value, Z_score):\n",
    "            Differnce_Vector = x - Value\n",
    "            sample_size_1 = len(Differnce_Vector)\n",
    "            Ranked_Data = rankdata(np.abs(Differnce_Vector))\n",
    "            Difference = np.sum(Ranked_Data[Differnce_Vector > 0]) - sample_size_1 * (sample_size_1 + 1) / 4\n",
    "            Number_of_Ties = np.histogram(Ranked_Data, bins=np.arange(1, len(Ranked_Data) + 2))[0]\n",
    "            Variance = np.sqrt(sample_size_1 * (sample_size_1 + 1) * (2 * sample_size_1 + 1) / 24 - np.sum((Number_of_Ties ** 3 - Number_of_Ties) / 48))\n",
    "            return Difference / Variance - Z_score\n",
    "        UpperCi = root_scalar(Hodges_Lhemann_Estimation_differences_sample, bracket=(min(x), max(x)), xtol=1e-4, args=(norm.ppf((1-confidence_level) / 2),)).root\n",
    "        LowerCi = root_scalar(Hodges_Lhemann_Estimation_differences_sample, bracket=(min(x), max(x)), xtol=1e-4, args=(norm.ppf(1 - (1-confidence_level) / 2),)).root\n",
    "        Hodges_Lehmann_Estimator = root_scalar(Hodges_Lhemann_Estimation_differences_sample, bracket=(min(x), max(x)), xtol=1e-4, args=(0,)).root\n",
    "    \n",
    "    else:\n",
    "        alpha = 1 - confidence_level\n",
    "        Minimum_Value = min(x) - max(y)\n",
    "        Maximum_Value = max(x) - min(y)\n",
    "        def Hodges_Lhemann_Estimation_two_samples(Value, Zscore):\n",
    "            Ranked_Data = rankdata(np.concatenate([x - Value, y]))\n",
    "            Number_of_Ties = np.histogram(Ranked_Data, bins=np.arange(1, len(Ranked_Data) + 2))[0]\n",
    "            Difference = (np.sum(Ranked_Data[:sample_size_1]) - sample_size_1 * (sample_size_1 + 1) / 2 - sample_size_1 * sample_size_2 / 2)\n",
    "            Varaince = np.sqrt((sample_size_1 * sample_size_2 / 12) * ((sample_size_1 + sample_size_2 + 1) - np.sum((Number_of_Ties ** 3 - Number_of_Ties) / ((sample_size_1 + sample_size_2) * (sample_size_1 + sample_size_2 - 1)))))\n",
    "            return Difference / Varaince - Zscore\n",
    "        LowerCi = root_scalar(Hodges_Lhemann_Estimation_two_samples, bracket=(Minimum_Value, Maximum_Value), xtol=1e-4, args=(norm.ppf(1 - alpha / 2),)).root\n",
    "        UpperCi = root_scalar(Hodges_Lhemann_Estimation_two_samples, bracket=(Minimum_Value, Maximum_Value), xtol=1e-4, args=(norm.ppf(alpha / 2),)).root\n",
    "        Hodges_Lehmann_Estimator = root_scalar(Hodges_Lhemann_Estimation_two_samples, bracket=(Minimum_Value, Maximum_Value), xtol=1e-4, args=(0,)).root\n",
    "    return Hodges_Lehmann_Estimator, LowerCi, UpperCi\n",
    "\n",
    "def hltest(x, y, confidence_level = confidence_level):\n",
    "    res = Hodges_Lehmann_based_on_wilcox_test(x, y, confidence_level=confidence_level)\n",
    "    return res\n",
    "\n",
    "# 2. Function for the confidence intervels for median difference and ratios using Price and Bonett method\n",
    "def CI_price_bonett(x,y,confidence_interval):\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    Median_sample_1 = np.median(x)\n",
    "    sample_size_1 = len(x)\n",
    "    sorted_x = np.sort(x)\n",
    "    c1 = np.round((sample_size_1 + 1) / 2 - sample_size_1**0.5)\n",
    "    X1 = sorted_x[int(sample_size_1 - c1)]\n",
    "    X2 = sorted_x[int(c1 - 1)]\n",
    "    Z1 = norm.ppf(1 - binom.cdf(c1 - 1, sample_size_1, 0.5))\n",
    "    Variance_Sample_1 = (((X1 - X2) / (2 * Z1))**2)\n",
    "\n",
    "    Median_Sample_2 = np.median(y)\n",
    "    sample_size_2 = len(y)\n",
    "    sorted_y = np.sort(y)\n",
    "    c2 = np.round((sample_size_2 + 1) / 2 - sample_size_2**0.5)\n",
    "    Y1 = (sorted_y[int(sample_size_2 - c2)])\n",
    "    Y2 = (sorted_y[int(c2 - 1)])\n",
    "    Z2 = norm.ppf(1 - binom.cdf(c2 - 1, sample_size_2, 0.5))\n",
    "    Variance_Sample_2 = ((Y1 - Y2) / (2 * Z2))**2\n",
    "\n",
    "    Standard_Error_Indpednent_Differnece_Price_Bonett = np.sqrt(Variance_Sample_1 + Variance_Sample_2)\n",
    "\n",
    "    #### Confidence Intervals ratio of means\n",
    "    log_X1 = np.log(sorted_x[int(sample_size_1 - c1)])\n",
    "    log_X2 = np.log(sorted_x[int(c1 - 1)])\n",
    "    log_Y1 = np.log(sorted_y[int(sample_size_2 - c2)])\n",
    "    log_Y2 = np.log(sorted_y[int(c2 - 1)])\n",
    "    log_Z1 = norm.ppf(1 - binom.cdf(c1 - 1, sample_size_1, 0.5))\n",
    "    log_Variance_Sample_1 = (((log_X1 - log_X2) / (2 * log_Z1))**2)\n",
    "    log_Z2 = norm.ppf(1 - binom.cdf(c2 - 1, sample_size_2, 0.5))\n",
    "    log_Variance_Sample_2 = ((log_Y1 - log_Y2) / (2 * log_Z2))**2\n",
    "    \n",
    "    log_Standard_Error_Indpednent_Differnece_Price_Bonett = np.sqrt(log_Variance_Sample_1+log_Variance_Sample_2)\n",
    "\n",
    "    values_lower_than_medians = np.sum((x < Median_sample_1) & (y<Median_Sample_2))\n",
    "    Median_Probability = (values_lower_than_medians + 0.25) / (sample_size_1 + 1) if sample_size_1 % 2 == 0 else (values_lower_than_medians + 0.25) / sample_size_1\n",
    "    Covariance_log = (4*Median_Probability - 1)*np.sqrt(log_Variance_Sample_1)*np.sqrt(log_Variance_Sample_2)\n",
    "    Log_Standard_Error_Within_Ratio = np.sqrt(log_Variance_Sample_1 + log_Variance_Sample_2 - 2*Covariance_log)\n",
    "    Covariance = (4*Median_Probability - 1)*np.sqrt(Variance_Sample_1)*np.sqrt(Variance_Sample_2)\n",
    "    Standrd_Error_Within_Difference = np.sqrt(Variance_Sample_1 + Variance_Sample_2 - 2*Covariance)\n",
    "    ratio_of_medians = Median_sample_1 / Median_Sample_2\n",
    "\n",
    "    # Confidence Intervals independent difference\n",
    "    LowerCi_Price_Bonett_between_difference = (Median_sample_1 - Median_Sample_2) - zcrit * Standard_Error_Indpednent_Differnece_Price_Bonett\n",
    "    UpperCi_Price_Bonett_between_difference = (Median_sample_1 - Median_Sample_2) + zcrit * Standard_Error_Indpednent_Differnece_Price_Bonett\n",
    "\n",
    "    # Confidence Intervals indpendent ratio_of medians\n",
    "    LowerCi_Price_Bonett_between_ratio = ratio_of_medians * math.exp(-zcrit * log_Standard_Error_Indpednent_Differnece_Price_Bonett)\n",
    "    UpperCi_Price_Bonett_between_ratio = ratio_of_medians * math.exp(zcrit * log_Standard_Error_Indpednent_Differnece_Price_Bonett)\n",
    "\n",
    "    # Confidence Intervals depednent difference\n",
    "    LowerCi_Price_Bonett_within_difference = (Median_sample_1 - Median_Sample_2) - zcrit * Standrd_Error_Within_Difference\n",
    "    UpperCi_Price_Bonett_within_difference = (Median_sample_1 - Median_Sample_2) + zcrit * Standrd_Error_Within_Difference\n",
    "\n",
    "    # Confidence Intervals dependent ratio_of medians\n",
    "    LowerCi_Price_Bonett_within_ratio = ratio_of_medians * math.exp(-zcrit * Log_Standard_Error_Within_Ratio)\n",
    "    UpperCi_Price_Bonett_within_ratio = ratio_of_medians * math.exp(zcrit * Log_Standard_Error_Within_Ratio)\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results[\"Median Sample 1\"] = Median_sample_1\n",
    "    results[\"Sample Size 1\"] = sample_size_1\n",
    "    results[\"Variance Sample 1\"] = Variance_Sample_1\n",
    "    results[\"Median Sample 2\"] = Median_Sample_2\n",
    "    results[\"Sample Size 2\"] = sample_size_2\n",
    "    results[\"Variance Sample 2\"] = Variance_Sample_2\n",
    "    results[\"Standard Error Independent Difference\"] = Standard_Error_Indpednent_Differnece_Price_Bonett\n",
    "    results[\"Log Standard Error\"] = log_Standard_Error_Indpednent_Differnece_Price_Bonett\n",
    "    results[\"Values Lower Than Medians\"] = values_lower_than_medians\n",
    "    results[\"Median Probability\"] = Median_Probability\n",
    "    results[\"Covariance (log)\"] = Covariance_log\n",
    "    results[\"Log Standard Error Within Ratio\"] = Log_Standard_Error_Within_Ratio\n",
    "    results[\"Covariance\"] = Covariance\n",
    "    results[\"Standard Error Within Difference\"] = Standrd_Error_Within_Difference\n",
    "    results[\"Ratio of Medians\"] = ratio_of_medians\n",
    "\n",
    "    results[\"Lower CI Independent Difference\"] = LowerCi_Price_Bonett_between_difference\n",
    "    results[\"Upper CI Independent Difference\"] = UpperCi_Price_Bonett_between_difference\n",
    "\n",
    "    results[\"Lower CI Independent Ratio of Medians\"] = LowerCi_Price_Bonett_between_ratio\n",
    "    results[\"Upper CI Independent Ratio of Medians\"] = UpperCi_Price_Bonett_between_ratio\n",
    "\n",
    "    results[\"Lower CI Dependent Difference\"] = LowerCi_Price_Bonett_within_difference\n",
    "    results[\"Upper CI Dependent Difference\"] = UpperCi_Price_Bonett_within_difference\n",
    "\n",
    "    results[\"Lower CI Dependent Ratio of Medians\"] = LowerCi_Price_Bonett_within_ratio\n",
    "    results[\"Upper CI Dependent Ratio of Medians\"] = UpperCi_Price_Bonett_within_ratio\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_sample_median(x, population_median = 0, confidence_level = 0.95):\n",
    "    Median = np.median(x)\n",
    "    Mean = np.mean(x)\n",
    "    absolute_deviations_from_median = abs(x-Median)\n",
    "    mean_absolute_deviation = np.mean(absolute_deviations_from_median)\n",
    "    normal_corrected_mean_absolout_deviation = mean_absolute_deviation * 1.2533\n",
    "    median_absolute_deviation = np.median(absolute_deviations_from_median)\n",
    "    normal_corrected_median_absoloute_deviation = 1.4826*median_absolute_deviation\n",
    "    Inter_Quartile_Range = iqr(x)\n",
    "    Standard_Deviation = np.std(x, ddof = 1)\n",
    "    sample_size = len(x)\n",
    "    \n",
    "    Standrd_Error_Median = np.sqrt(math.pi/2) * (Standard_Deviation/np.sqrt(sample_size)) # Note - there are many other ways to estimate the median - for simplicity, we chose this one\n",
    "    Standrd_Error_Median_AD = np.sqrt(2/math.pi) * (Standard_Deviation/np.sqrt(sample_size)) # Gia & Hung, 2001\n",
    "    Standrd_Error_IQR = Standard_Deviation / (2*np.sqrt(sample_size)*0.31777)\n",
    "\n",
    "    # Another Estimator for the dispertion around median Qn (Rousseeuw and Croux, 1993)\n",
    "    def pairwise_differences(x):      # a function to calcualte all pairwise comparison\n",
    "        return [b - a for a, b in itertools.combinations(x, 2)]\n",
    "    Pairwise_Vector = pairwise_differences(x)\n",
    "    Qn = 2.2219 * (np.quantile(abs(np.array(Pairwise_Vector)),0.25))\n",
    "\n",
    "    # biweight standard Error Goldberg & Iglewicz, 1992; Lax, 1985)\n",
    "\n",
    "    # Effect Sizes \n",
    "    # 1. Thompson 2007\n",
    "    d_Mdn = abs(Median -  population_median) / Standard_Deviation\n",
    "\n",
    "    # 2. Median Dispersion based measures\n",
    "    d_Mdn_AD = abs(Median -  population_median) / median_absolute_deviation\n",
    "    d_Mdn_AD_corrected = abs(Median -  population_median)  / normal_corrected_median_absoloute_deviation\n",
    "    d_IQR = abs(Median -  population_median)  / Inter_Quartile_Range\n",
    "    d_Qn = abs(Median -  population_median)  / Qn\n",
    "\n",
    "    # Signifcance of the median\n",
    "\n",
    "    # One Sample Median Significant Test\n",
    "    t_median = (Median -  population_median) / Standrd_Error_Median\n",
    "    P_value_median = calculate_p_value_from_t_score(t_median, sample_size-1)\n",
    "    \n",
    "\n",
    "    # One sample Sign Test\n",
    "    Sign_Test_vector = np.where(x > population_median, 1, np.where(x < population_median, 0, np.nan))\n",
    "    K = sum(Sign_Test_vector)\n",
    "    p_value_sign = binom.cdf(k=K, n=sample_size, p=0.5)\n",
    "    p_val_wilcoxon = sign_test_wilcoxon_method(x, y=None)\n",
    "\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Median\"] = round((Median), 4)\n",
    "    results[\"Median_Absolute_Deviation (MAD)\"] = around((median_absolute_deviation), 4)\n",
    "    results[\"Median Absolute Deviation (normal corrected)\"] = around((normal_corrected_median_absoloute_deviation), 4)\n",
    "    results[\"Mean Absolute_Deviation (MAD)\"] = around((mean_absolute_deviation), 4)\n",
    "    results[\"Mean Absolute Deviation (normal corrected)\"] = around((normal_corrected_mean_absolout_deviation), 4)\n",
    "    results[\"Qn Median Estimator\"] = around((Qn), 4)\n",
    "    results[\"Inter_Quartile_Range\"] = around((Inter_Quartile_Range), 4)\n",
    "    results[\"Standard_Deviation\"] = round((Standard_Deviation), 4)\n",
    "    results[\"Standrd_Error_Median\"] = round((Standrd_Error_Median), 4)\n",
    "    \n",
    "    # Significant Tests\n",
    "    results[\"p_value_sign\"] = around((p_value_sign), 4)\n",
    "    results[\"p_value_median\"] = around((P_value_median), 4)\n",
    "\n",
    "\n",
    "    # Effect Sizes\n",
    "    results[\"Delta Median (Thompson, 2007)\"] = round((d_Mdn), 4)\n",
    "    results[\"Delta Median Absouloute Deviation\"] = round((d_Mdn_AD), 4)\n",
    "    results[\"Delta Median Absouloute Deviation Corrected\"] = round((d_Mdn_AD_corrected), 4)\n",
    "    results[\"Delta IQR\"] = round((d_IQR), 4)\n",
    "    results[\"Delta Qn\"] = round((d_Qn), 4)\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "def two_independnet_medians(x,y, confidence_level = 0.95):\n",
    "    Median_sample_1 = np.median(x)\n",
    "    Mean_sample_1 = np.mean(x)\n",
    "    absolute_deviations_from_median_sample_1 = abs(x-Median_sample_1)\n",
    "    mean_absolute_deviation_sample_1 = np.mean(absolute_deviations_from_median_sample_1)\n",
    "    normal_corrected_mean_absolout_deviation_sample_1 = mean_absolute_deviation_sample_1 * 1.2533\n",
    "    median_absolute_deviation_sample_1 = np.median(absolute_deviations_from_median_sample_1)\n",
    "    normal_corrected_median_absoloute_deviation_sample_1 = 1.4826*median_absolute_deviation_sample_1\n",
    "    Inter_Quartile_Range_sample_1 = iqr(x)\n",
    "    Standard_Deviation_sample_1 = np.std(x, ddof = 1)\n",
    "    sample_size_sample_1 = len(x)\n",
    "    Standrd_Error_Median_sample_1 = np.sqrt(math.pi/2) * (Standard_Deviation_sample_1/np.sqrt(sample_size_sample_1)) # Note - there are many other ways to estimate the median - for simplicity, we chose this one\n",
    "    Standrd_Error_Median_AD_sample_1 = np.sqrt(2/math.pi) * (Standard_Deviation_sample_1/np.sqrt(sample_size_sample_1)) # Gia & Hung, 2001\n",
    "    Standrd_Error_IQR_sample_1 = Standard_Deviation_sample_1 / (2*np.sqrt(sample_size_sample_1)*0.31777)\n",
    "    \n",
    "    Median_sample_2 = np.median(y)\n",
    "    Mean_sample_2 = np.mean(y)\n",
    "    absolute_deviations_from_median_sample_2 = abs(y-Median_sample_2)\n",
    "    mean_absolute_deviation_sample_2 = np.mean(absolute_deviations_from_median_sample_2)\n",
    "    normal_corrected_mean_absolout_deviation_sample_2 = mean_absolute_deviation_sample_2 * 1.2533\n",
    "    median_absolute_deviation_sample_2 = np.median(absolute_deviations_from_median_sample_2)\n",
    "    normal_corrected_median_absoloute_deviation_sample_2 = 1.4826*median_absolute_deviation_sample_2\n",
    "    Inter_Quartile_Range_sample_2 = iqr(y)\n",
    "    Standard_Deviation_sample_2 = np.std(y, ddof = 1)\n",
    "    sample_size_sample_2 = len(y)\n",
    "    Standrd_Error_Median_sample_2 = np.sqrt(math.pi/2) * (Standard_Deviation_sample_2/np.sqrt(sample_size_sample_2)) # Note - there are many other ways to estimate the median - for simplicity, we chose this one\n",
    "    Standrd_Error_Median_AD_sample_2 = np.sqrt(2/math.pi) * (Standard_Deviation_sample_2/np.sqrt(sample_size_sample_2)) # Gia & Hung, 2001\n",
    "    Standrd_Error_IQR_sample_2 = Standard_Deviation_sample_2 / (2*np.sqrt(sample_size_sample_2)*0.31777)\n",
    "\n",
    "    # Effect Sizes for the difference betweem Medians\n",
    "    Median_Difference = Median_sample_1 - Median_sample_2\n",
    "\n",
    "    # 1. Based on SD's (Thompson, 2007)\n",
    "    d_mdns = Median_Difference / np.sqrt((Standard_Deviation_sample_1**2 + Standard_Deviation_sample_2**2) / 2)\n",
    "\n",
    "    # 2. Based on Pooled MAD Ricca & Blaine\n",
    "    Median_Absolute_Deviations_Pooled = ((sample_size_sample_1-1)*median_absolute_deviation_sample_1 + (sample_size_sample_2-1)*median_absolute_deviation_sample_2) / (sample_size_sample_1 + sample_size_sample_2 - 2)\n",
    "    Median_Absolute_Deviations_Pooled_Corrected = ((sample_size_sample_1-1)*normal_corrected_median_absoloute_deviation_sample_1 + (sample_size_sample_2-1)*normal_corrected_median_absoloute_deviation_sample_2) / (sample_size_sample_1 + sample_size_sample_2 - 2)\n",
    "    d_mad_pooled = Median_Difference / Median_Absolute_Deviations_Pooled\n",
    "    d_mad_pooled_corrected = Median_Difference / Median_Absolute_Deviations_Pooled\n",
    "\n",
    "    \n",
    "    # Ratio of the medians\n",
    "    Ratio_of_the_Medians = Median_sample_1 / Median_sample_2\n",
    "\n",
    "    # Quantile Shift effect size group2 is the reference group\n",
    "    Quantile_1_Group1 = np.mean(Median_sample_1<=x)\n",
    "    Quantile_2_Group1 = np.mean(Median_sample_2<=x)\n",
    "    Quantile_Measure_Effect_Size_Median_Group1 = Quantile_1_Group1 - Quantile_2_Group1\n",
    "    Quantile_1_Group2 = np.mean(Median_sample_1<=y)\n",
    "    Quantile_2_Group2 = np.mean(Median_sample_2<=y)\n",
    "    Quantile_Measure_Effect_Size_Median_Group2 = Quantile_2_Group2 - Quantile_1_Group2\n",
    "\n",
    "    trimmed_mean_1 = stats.trim_mean(x, 0.2)\n",
    "    trimmed_mean_2 = stats.trim_mean(y, 0.2)\n",
    "    Quantile_1_trimmed_mean = np.mean(trimmed_mean_1<=x)\n",
    "    Quantile_2_trimmed_mean = np.mean(trimmed_mean_2<=x)\n",
    "    Quantile_Measure_Effect_Size_Trimmed_Mean = Quantile_1_trimmed_mean - Quantile_2_trimmed_mean\n",
    "\n",
    "    # Quantile Shift of typical differences **Check why did Rand limits the function to n>10\n",
    "    Pairwise_Comparisons = np.subtract.outer(x, y)\n",
    "    Median_of_Comparisons = np.median(Pairwise_Comparisons)\n",
    "    Trimmed_Mean_of_Comparisons = np.median(Pairwise_Comparisons)\n",
    "    Quantile_Symmetric_Measure_Effect_Size = np.mean(Pairwise_Comparisons - Median_of_Comparisons <= Median_of_Comparisons)\n",
    "    Quantile_Symmetric_Measure_Effect_Size_trimmed_mean = np.mean(Pairwise_Comparisons - Trimmed_Mean_of_Comparisons <= Trimmed_Mean_of_Comparisons)\n",
    "\n",
    "    \n",
    "\n",
    "    # Significant Tests for 2 medians\n",
    "    Chi_Square_Statistic,p_value = median_test(x, y)[0:2]\n",
    "\n",
    "    # Hodges Lehmann Estimator - I need to take the relvant parameters\n",
    "    averages_vector_indepednent = [np.abs(xi - yi) for xi in x for yi in y]\n",
    "    Hodges_Lehmann_estimator_independent_samples = np.median(averages_vector_indepednent)    \n",
    "    CI_HL_between = Hodges_Lehmann_based_on_wilcox_test(x,y,confidence_level) \n",
    "\n",
    "    # Price and Bonett CI'sfor ratio of median and difference between medians \n",
    "    ci_between_price_bonett = CI_price_bonett(x,y,confidence_level)\n",
    " \n",
    "\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Median_Sample_1\"] = Median_sample_1\n",
    "    results[\"Mean_Sample_1\"] = Mean_sample_1\n",
    "    results[\"Mean_Absolute_Deviation_Sample_1\"] = mean_absolute_deviation_sample_1\n",
    "    results[\"Normal_Corrected_Mean_Absolute_Deviation_Sample_1\"] = normal_corrected_mean_absolout_deviation_sample_1\n",
    "    results[\"Median_Absolute_Deviation_Sample_1\"] = median_absolute_deviation_sample_1\n",
    "    results[\"Normal_Corrected_Median_Absolute_Deviation_Sample_1\"] = normal_corrected_median_absoloute_deviation_sample_1\n",
    "    results[\"Inter_Quartile_Range_Sample_1\"] = Inter_Quartile_Range_sample_1\n",
    "    results[\"Standard_Deviation_Sample_1\"] = Standard_Deviation_sample_1\n",
    "    results[\"Sample_Size_Sample_1\"] = sample_size_sample_1\n",
    "    results[\"Standrd_Error_Median_Sample_1\"] = Standrd_Error_Median_sample_1\n",
    "    results[\"Standrd_Error_Median_AD_Sample_1\"] = Standrd_Error_Median_AD_sample_1\n",
    "    results[\"Standrd_Error_IQR_Sample_1\"] = Standrd_Error_IQR_sample_1\n",
    "\n",
    "    results[\"Median_Sample_2\"] = Median_sample_2\n",
    "    results[\"Mean_Sample_2\"] = Mean_sample_2\n",
    "    results[\"Mean_Absolute_Deviation_Sample_2\"] = mean_absolute_deviation_sample_2\n",
    "    results[\"Normal_Corrected_Mean_Absolute_Deviation_Sample_2\"] = normal_corrected_mean_absolout_deviation_sample_2\n",
    "    results[\"Median_Absolute_Deviation_Sample_2\"] = median_absolute_deviation_sample_2\n",
    "    results[\"Normal_Corrected_Median_Absolute_Deviation_Sample_2\"] = normal_corrected_median_absoloute_deviation_sample_2\n",
    "    results[\"Inter_Quartile_Range_Sample_2\"] = Inter_Quartile_Range_sample_2\n",
    "    results[\"Standard_Deviation_Sample_2\"] = Standard_Deviation_sample_2\n",
    "    results[\"Sample_Size_Sample_2\"] = sample_size_sample_2\n",
    "    results[\"Standrd_Error_Median_Sample_2\"] = Standrd_Error_Median_sample_2\n",
    "    results[\"Standrd_Error_Median_AD_Sample_2\"] = Standrd_Error_Median_AD_sample_2\n",
    "    results[\"Standrd_Error_IQR_Sample_2\"] = Standrd_Error_IQR_sample_2\n",
    "\n",
    "    results[\"Median_Difference\"] = Median_Difference\n",
    "    results[\"Mean_Absolute_Deviations_Pooled\"] = Mean_Absolute_Deviations_Pooled\n",
    "    results[\"Effect_Size_Median\"] = d_mdn\n",
    "    results[\"Effect_Size_Median_AD_Pooled\"] = d_mad_pooled\n",
    "    results[\"Effect_Size_Median_AD_Pooled_Corrected\"] = d_mad_pooled_corrected\n",
    "\n",
    "    results[\"Quantile Measure (Group 1 as rference group) \"] = Quantile_Measure_Effect_Size_Median_Group1\n",
    "    results[\"Quantile Measure (Group 2 as reference group) \"] = Quantile_Measure_Effect_Size_Median_Group2\n",
    "    results[\"Quantile Measure Typical Difference\"] = Quantile_Symmetric_Measure_Effect_Size\n",
    "\n",
    "    results[\"Ratio_of_the_Medians\"] = Ratio_of_the_Medians # Check out Bonnete and Price 2020 for confidence intervals  \n",
    "    results[\"Moods Median Test Chi Square Statitic\"] = Ratio_of_the_Medians   \n",
    "    results[\"Moods Median Test p-value\"] = Ratio_of_the_Medians \n",
    "\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "def two_dependent_medians(x,y, confidence_level = 0.95):\n",
    "    Median_sample_1 = np.median(x)\n",
    "    Mean_sample_1 = np.mean(x)\n",
    "    absolute_deviations_from_median_sample_1 = abs(x-Median_sample_1)\n",
    "    mean_absolute_deviation_sample_1 = np.mean(absolute_deviations_from_median_sample_1)\n",
    "    normal_corrected_mean_absolout_deviation_sample_1 = mean_absolute_deviation_sample_1 * 1.2533\n",
    "    median_absolute_deviation_sample_1 = np.median(absolute_deviations_from_median_sample_1)\n",
    "    normal_corrected_median_absoloute_deviation_sample_1 = 1.4826*median_absolute_deviation_sample_1\n",
    "    Inter_Quartile_Range_sample_1 = iqr(x)\n",
    "    Standard_Deviation_sample_1 = np.std(x, ddof = 1)\n",
    "\n",
    "    Median_sample_2 = np.median(y)\n",
    "    Mean_sample_2 = np.mean(y)\n",
    "    absolute_deviations_from_median_sample_2 = abs(y-Median_sample_2)\n",
    "    mean_absolute_deviation_sample_2 = np.mean(absolute_deviations_from_median_sample_2)\n",
    "    normal_corrected_mean_absolout_deviation_sample_2 = mean_absolute_deviation_sample_2 * 1.2533\n",
    "    median_absolute_deviation_sample_2 = np.median(absolute_deviations_from_median_sample_2)\n",
    "    normal_corrected_median_absoloute_deviation_sample_2 = 1.4826*median_absolute_deviation_sample_2\n",
    "    Inter_Quartile_Range_sample_2 = iqr(y)\n",
    "    Standard_Deviation_sample_2 = np.std(y, ddof = 1)\n",
    "\n",
    "    # Median of the Difference and Related Measures\n",
    "    Difference_Between_Samples = x-y\n",
    "    Median_of_the_difference = np.median(Difference_Between_Samples)\n",
    "    absolute_deviations_from_median_of_difference = abs(Difference_Between_Samples-Median_of_the_difference)\n",
    "    mean_absolute_deviation_of_difference = np.mean(absolute_deviations_from_median_of_difference)\n",
    "    normal_corrected_mean_absolout_deviation = mean_absolute_deviation_of_difference * 1.2533\n",
    "    median_absolute_deviation_of_difference = np.median(absolute_deviations_from_median_of_difference)\n",
    "    normal_corrected_median_absoloute_deviation = 1.4826*median_absolute_deviation_of_difference\n",
    "    Inter_Quartile_Range_of_difference = iqr(Difference_Between_Samples)\n",
    "    Standard_Deviation_of_difference = np.std(Difference_Between_Samples, ddof = 1)\n",
    "    sample_size = len(Difference_Between_Samples)\n",
    "\n",
    "    # Difference Between Medians and Confidence Interval (Price Bonett, 2021)\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    sorted_x = np.sort(Difference_Between_Samples)\n",
    "    c1 = np.floor((sample_size + 1) / 2 - sample_size**0.5)\n",
    "    X1 = sorted_x[int(sample_size - c1)]\n",
    "    X2 = sorted_x[int(c1 - 1)]\n",
    "    Z1 = norm.ppf(1 - binom.cdf(c1 - 1, sample_size, 0.5))\n",
    "    Standrad_Error_Difference_price_Bonett = np.sqrt(((X1 - X2) / (2 * Z1))**2) \n",
    "    values_lower_medians = np.sum (((x < Median_sample_1) + (y < Median_sample_2)) == 2)  \n",
    "\n",
    "\n",
    "    Standrd_Error_Median_of_difference = np.sqrt(math.pi/2) * (Standard_Deviation_of_difference/np.sqrt(sample_size)) # Note - there are many other ways to estimate the median - for simplicity, we chose this one\n",
    "    Standrd_Error_Median_AD = np.sqrt(2/math.pi) * (Standard_Deviation_of_difference/np.sqrt(sample_size)) # Gia & Hung, 2001\n",
    "    Standrd_Error_IQR = Standard_Deviation_of_difference / (2*np.sqrt(sample_size)*0.31777)\n",
    "\n",
    "    # Another Estimator for the dispertion around median Qn (Rousseeuw and Croux, 1993)\n",
    "    def pairwise_differences(x):      # a function to calcualte all pairwise comparison\n",
    "        return [b - a for a, b in itertools.combinations(x, 2)]\n",
    "    Pairwise_Vector = pairwise_differences(x)\n",
    "    Qn = 2.2219 * (np.quantile(abs(np.array(Pairwise_Vector)),0.25))\n",
    "\n",
    "    # Effect Sizes \n",
    "    # 1. Thompson 2007\n",
    "    d_Mdn = abs(Median_of_the_difference) / Standard_Deviation_of_difference\n",
    "\n",
    "    # 2. Median Dispersion based measures\n",
    "    d_Mdn_AD = abs(Median_of_the_difference) / median_absolute_deviation_of_difference\n",
    "    d_Mdn_AD_corrected = (Median_of_the_difference)  / normal_corrected_median_absoloute_deviation\n",
    "    d_IQR = (Median_of_the_difference)  / Inter_Quartile_Range_of_difference\n",
    "    d_Qn = (Median_of_the_difference)  / Qn\n",
    "\n",
    "    #3. Ratio of the means and confidence intervals\n",
    "    Ratio_of_the_Medians = Median_sample_1 / Median_sample_2\n",
    "    \n",
    "    #4. Wilcoxon Quantile Shift measure of effect size\n",
    "    Quantile_Shift_Effect_Size = np.mean(Difference_Between_Samples - Median_of_the_difference <= Median_of_the_difference)\n",
    "\n",
    "    # Two depdendent Samples Median Significant Test\n",
    "    t_median = (Median_of_the_difference) / Standrd_Error_Median_of_difference\n",
    "    P_value_median = calculate_p_value_from_t_score(t_median, sample_size-1)\n",
    "    \n",
    "    # One sample Sign Test\n",
    "    Sign_Test_vector = np.where(Difference_Between_Samples > 0, 1, np.where(Difference_Between_Samples < 0, 0, np.nan))\n",
    "    K = sum(Sign_Test_vector)\n",
    "    p_value_sign = binom.cdf(k=K, n=sample_size, p=0.5)\n",
    "\n",
    "    # Wilcox Qs for dependent Samples \n",
    "    Wilcox_Qs = np.mean(Difference_Between_Samples - Median_of_the_difference <= Median_of_the_difference)\n",
    "\n",
    "    # Hodges-Lehemann Estimator Dependent Samples\n",
    "    averages_vector_paired = [(Difference_Between_Samples[i] + Difference_Between_Samples[j]) / 2\n",
    "    for i in range(len(Difference_Between_Samples))\n",
    "    for j in range(i, len(Difference_Between_Samples))]\n",
    "    Hodges_Lehmann_estimator_Paired_samples = np.median(averages_vector_paired)\n",
    "    HL_CI = Hodges_Lehmann_based_on_wilcox_test(Difference_Between_Samples, y = None, confidence_level=confidence_level)\n",
    "\n",
    "    # ****Here I need to choose the relevant ouput parameters from the function \n",
    "    CIs = CI_price_bonett(x,y,confidence_level)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
