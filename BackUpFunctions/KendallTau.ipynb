{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendalls Tau-a:: -0.024630541871921183\n",
      "Kendalls Tau-a Standard Error: 0.07096158749883964\n",
      "Kendalls Tau-a Z-score: -0.34709682717179824\n",
      "Kendalls Tau-a p-value: 0.7285193074757812\n",
      "Kendalls Tau-a Confidence Intervals: (-0.16371438107678848, 0.11445329733294612)\n",
      "Kendalls Tau-b:: -0.043420842143242176\n",
      "Kendalls Tau-b Standard Error: 0.17622674446405345\n",
      "Kendalls Tau-b Z-score: -0.24639189854692636\n",
      "Kendalls Tau-b p-value: 0.8053793827230222\n",
      "Kendalls Tau-b. Confidence Intervals: (-0.38882309503162965, 0.30198141074514534)\n",
      "Kendalls Tau-c: -0.04756242568370987\n",
      "Kendalls Tau-c Standard Error: 0.19309448012256167\n",
      "Kendalls Tau-c Z-score: -0.24631685822153418\n",
      "Kendalls Tau-c p-value: 0.8054374664915745\n",
      "Kendalls Tau-c Confidence Intervals: (-0.4260252331167167, 0.3309003817492969)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "def Kendall_Tau_Family(contingency_table, confidence_level = 0.95): #inspired by Desctools function\n",
    "    number_of_rows, number_of_columns = contingency_table.shape\n",
    "    concordant_table = np.zeros((number_of_rows, number_of_columns))\n",
    "    disconcordant_table = np.zeros((number_of_rows, number_of_columns))\n",
    "    for i in range(number_of_rows):\n",
    "        for j in range(number_of_columns):\n",
    "            concordant_table[i, j] = np.sum(contingency_table[:i, :j]) + np.sum(contingency_table[i+1:, j+1:])\n",
    "            disconcordant_table[i, j] = np.sum(contingency_table[:i, j+1:]) + np.sum(contingency_table[i+1:, :j])\n",
    "    Concordant_pairs, Disconcordant_pairs = np.sum(concordant_table * contingency_table) / 2, np.sum(disconcordant_table * contingency_table) / 2\n",
    "    sample_size = np.sum(contingency_table)\n",
    "    number_of_total_pairs = sample_size * (sample_size-1)/2  \n",
    "    sum_of_the_rows = contingency_table.sum(axis=1)\n",
    "    sum_of_the_cols = contingency_table.sum(axis=0)\n",
    "    Number_of_ties_rows = np.sum(sum_of_the_rows * (sum_of_the_rows-1)/2)\n",
    "    Number_of_ties_columns = np.sum(sum_of_the_cols * (sum_of_the_cols-1)/2)\n",
    "    low_rc = (min(number_of_rows, number_of_columns)) # choose whethehr there are less row or coloumns\n",
    "\n",
    "    Kendall_tau_a = (Concordant_pairs - Disconcordant_pairs) / number_of_total_pairs\n",
    "    Kendall_tau_b = (Concordant_pairs - Disconcordant_pairs) / np.sqrt((number_of_total_pairs-Number_of_ties_rows) * (number_of_total_pairs-Number_of_ties_columns))\n",
    "    Kendall_tau_c = 2 * (Concordant_pairs - Disconcordant_pairs) / (sample_size**2 *  ((low_rc - 1) / low_rc)) \n",
    "\n",
    "    # Standard Errors of tau\n",
    "    Term1 = (concordant_table - disconcordant_table) * (contingency_table !=0)\n",
    "    Term2 = np.sum(Term1[Term1 != 0]) / sample_size\n",
    "    Standard_Error_Tau_a = np.sqrt(2 / (sample_size * (sample_size - 1)) * ((2 * (sample_size - 2)) / (sample_size * (sample_size - 1)**2) * np.sum((Term1 - Term2)**2) + 1 - Kendall_tau_a**2))\n",
    "    Proportions_Tables = contingency_table / sample_size\n",
    "    Proportions_Difference = (concordant_table - disconcordant_table) / sample_size\n",
    "    sum_of_proportions_rows = np.sum(Proportions_Tables, axis=1)\n",
    "    sum_of_proportions_columns = np.sum(Proportions_Tables, axis=0)\n",
    "    proportions_matrix_row = (np.tile(sum_of_proportions_rows, (contingency_table.shape[1], 1)).T) * (2 * (Concordant_pairs - Disconcordant_pairs) / sample_size**2)\n",
    "    proportions_matrix_columns = np.tile(sum_of_proportions_columns, (contingency_table.shape[0], 1)) * (2 * (Concordant_pairs - Disconcordant_pairs) / sample_size**2)\n",
    "    delta1 = np.sqrt(1 - np.sum(sum_of_proportions_rows**2))\n",
    "    delta2 = np.sqrt(1 - np.sum(sum_of_proportions_columns**2))\n",
    "    Term3 = (2 * Proportions_Difference + proportions_matrix_columns) * delta2 * delta1 + (proportions_matrix_row * delta2) / delta1\n",
    "    Standard_Error_Tau_b = np.sqrt(((np.sum(Proportions_Tables * Term3**2) - np.sum(Proportions_Tables * Term3)**2) / (delta1 * delta2)**4) / sample_size)\n",
    "    Standard_Error_Tau_c = np.sqrt(4 * low_rc**2/((low_rc - 1)**2 * sample_size**4) * (np.sum(contingency_table * (concordant_table - disconcordant_table)**2) - 4 * (Concordant_pairs - Disconcordant_pairs)**2/sample_size))\n",
    "\n",
    "    # Approximate Confidence Intervals Kendall's Tau\n",
    "    zcrit = scipy.stats.t.ppf(1 - (1 - confidence_level) / 2, 100000)\n",
    "    Confidence_Interval_Tau_a = (Kendall_tau_a - zcrit * Standard_Error_Tau_a,Kendall_tau_a + zcrit * Standard_Error_Tau_a)\n",
    "    Confidence_Interval_Tau_b = (Kendall_tau_b - zcrit * Standard_Error_Tau_b,Kendall_tau_b + zcrit * Standard_Error_Tau_b)\n",
    "    Confidence_Interval_Tau_c = (Kendall_tau_c - zcrit * Standard_Error_Tau_c,Kendall_tau_c + zcrit * Standard_Error_Tau_c)\n",
    "\n",
    "    Z_Score_Tau_a = Kendall_tau_a / Standard_Error_Tau_a\n",
    "    Z_Score_Tau_b = Kendall_tau_b / Standard_Error_Tau_b\n",
    "    Z_Score_Tau_c = Kendall_tau_c / Standard_Error_Tau_c\n",
    "\n",
    "    p_value_Tau_a = scipy.stats.t.sf((abs(Z_Score_Tau_a)), 100000) * 2\n",
    "    p_value_Tau_b = scipy.stats.t.sf((abs(Z_Score_Tau_b)), 100000) * 2\n",
    "    p_value_Tau_c = scipy.stats.t.sf((abs(Z_Score_Tau_c)), 100000) * 2\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"Kendalls Tau-a:\"]= Kendall_tau_a\n",
    "    results[\"Kendalls Tau-a Standard Error\"] = Standard_Error_Tau_a\n",
    "    results[\"Kendalls Tau-a Z-score\"] = Z_Score_Tau_a\n",
    "    results[\"Kendalls Tau-a p-value\"]= p_value_Tau_a\n",
    "    results[\"Kendalls Tau-a Confidence Intervals\"]= Confidence_Interval_Tau_a\n",
    "    results[\"Kendalls Tau-b:\"]= Kendall_tau_b\n",
    "    results[\"Kendalls Tau-b Standard Error\"] = Standard_Error_Tau_b\n",
    "    results[\"Kendalls Tau-b Z-score\"] = Z_Score_Tau_b\n",
    "    results[\"Kendalls Tau-b p-value\"]= p_value_Tau_b\n",
    "    results[\"Kendalls Tau-b. Confidence Intervals\"]= Confidence_Interval_Tau_b\n",
    "    results[\"Kendalls Tau-c\"]= Kendall_tau_c\n",
    "    results[\"Kendalls Tau-c Standard Error\"] = Standard_Error_Tau_c\n",
    "    results[\"Kendalls Tau-c Z-score\"] = Z_Score_Tau_c\n",
    "    results[\"Kendalls Tau-c p-value\"]= p_value_Tau_c\n",
    "    results[\"Kendalls Tau-c Confidence Intervals\"]= Confidence_Interval_Tau_c\n",
    "\n",
    "\n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str\n",
    "\n",
    "# Your data\n",
    "contingency_table = np.array([[2, 3], [4, 5], [7, 8]])\n",
    "result = Kendall_Tau_Family(contingency_table)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
