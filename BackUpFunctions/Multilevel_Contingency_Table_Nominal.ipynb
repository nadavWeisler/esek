{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 500\n",
      "Number of Rows: 2\n",
      "Number of Columns: 3\n",
      "____________________________________________: \n",
      "Chi Square: 6.6678\n",
      "Degrees of Freedom Chi Square: 2\n",
      "p_value chi square: 0.0356532724\n",
      "___________________________________________: \n",
      "Phi Square: 0.0133356574\n",
      "Phi / Cohen's w: 0.1155\n",
      "Cramer V: 0.1155\n",
      "Tschuprow's T: 0.0971068\n",
      "Pearson's Contingency Coefficient: 0.1147\n",
      "Likelihood Ratio: 3.715\n",
      "Likelihood Ratio p_value: 0.0539\n",
      "__________________________________________________: \n",
      "Adjusted Phi: 0.0093276\n",
      "Adjusted Contingency Coefficient: 0.0961\n",
      "Adjusted Cramer's V: 0.0966766\n",
      "Adjusted Tschuprow's T: 0.0813359\n",
      "__________________________________________: \n",
      "Maximum Corrected Contingency Coefficient (Sakoda, 1977): 0.1623\n",
      "Maximum Corrected Tschuprow's T: 0.1014188\n",
      "Maximum Corrected Cramers V (Berry, Mielke, Jhonston): 0.0971567\n",
      "_________________________________________: \n",
      "Standard Deviation of Phi Square: 0.0095\n",
      "Standard Deviation of Contingency Coefficient: 0.0404\n",
      "Standard Deviation of Maximum Corrected Contingency Coefficient: 0.0572\n",
      "Standard Deviation of Cramer V: 0.0413\n",
      "Standard Deviation of Tschuprows T: 0.0245\n",
      "______________________________________________: \n",
      "CI Contingency Coefficient: (0.0355, 0.194)\n",
      "CI maximum corrected Contingency Coefficient: (0.0502, 0.2744)\n",
      "CI Cramer V: (0.0346, 0.1963)\n",
      "CI bias corrected Cramer V: (0.0, 0.1934)\n",
      "CI Tschuprow's T: (0.049, 0.1452)\n",
      "CI bias corrected Tschuprows T: (0.0236, 0.1391)\n",
      "______________________________________________________: \n",
      "NCP CI Contingency Coefficient: (0.0, 0.1928)\n",
      "NCP CI Cramer V: (0.0, 0.1389)\n",
      "NCP CI Tschuprow's T: (0.0, 0.1652)\n",
      "NCP CI Contingency Coefficient Bias corrected: (0.0, 0.1732)\n",
      "NCP CI Cramer's V Bias corrected: (0.0, 0.1248)\n",
      "NCP CI Tschuprow's T Bias corrected: (0.0, 0.1484)\n",
      "NCP CI Contingency Coefficient Max corrected: (0.0, 0.2458)\n",
      "NCP CI Cramer's V Max corrected: (0.0, 0.1859)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.stats import chi2_contingency, ncx2\n",
    "\n",
    "\n",
    "contingency_table = np.array([[18201, 14293],[2696, 2488]], dtype=np.int64) # This definition is very important for large values \n",
    "\n",
    "\n",
    "contingency_table = np.array([\n",
    "    [225, 53, 206],\n",
    "    [3, 1, 12]\n",
    "])\n",
    "\n",
    "confidence_level = 0.95\n",
    "\n",
    "\n",
    "# Function from Utils\n",
    "def calculate_p_value_from_chi_score(chi_score, df):\n",
    "    p_value = scipy.stats.chi2.sf((abs(chi_score)), df)\n",
    "    return min(float(p_value), 0.99999)\n",
    "\n",
    "confidence_level = 0.95\n",
    "\n",
    "def ncp_ci(chival, df, conf):\n",
    "    def low_ci(chival, df, conf):\n",
    "            bounds = [0.001, chival / 2, chival]\n",
    "            ulim = 1 - (1 - conf) / 2\n",
    "            while ncx2.cdf(chival, df, bounds[0]) < ulim:\n",
    "                return [0, ncx2.cdf(chival, df, bounds[0])]\n",
    "            while (diff := abs(ncx2.cdf(chival, df, bounds[1]) - ulim)) > 0.00001:\n",
    "                bounds = [bounds[0], (bounds[0] + bounds[1]) / 2, bounds[1]] if ncx2.cdf(chival, df, bounds[1]) < ulim else [bounds[1], (bounds[1] + bounds[2]) / 2, bounds[2]]\n",
    "            return [bounds[1]]\n",
    "    def high_ci(chival, df, conf):\n",
    "        # This first part finds upper and lower starting values.\n",
    "        uc = [chival, 2 * chival, 3 * chival]\n",
    "        llim = (1 - conf) / 2\n",
    "        while ncx2.cdf(chival, df, uc[0]) < llim: uc = [uc[0] / 4, uc[0], uc[2]]\n",
    "        while ncx2.cdf(chival, df, uc[2]) > llim: uc = [uc[0], uc[2], uc[2] + chival]\n",
    "\n",
    "        diff = 1\n",
    "        while diff > 0.00001:\n",
    "            uc = [uc[0], (uc[0] + uc[1]) / 2, uc[1]] if ncx2.cdf(chival, df, uc[1]) < llim else [uc[1], (uc[1] + uc[2]) / 2, uc[2]]\n",
    "            diff = abs(ncx2.cdf(chival, df, uc[1]) - llim)\n",
    "            lcdf = ncx2.cdf(chival, df, uc[1])   \n",
    "        return uc[1]\n",
    "    low_ncp = low_ci(chival, df, conf)\n",
    "    high_ncp = high_ci(chival, df, conf)\n",
    "\n",
    "    return low_ncp[0], high_ncp\n",
    "\n",
    "def likelihood_ratio(contingency_table):\n",
    "        Observed = contingency_table\n",
    "        Sum_Product = np.sum(Observed)\n",
    "        Expected = np.outer(np.sum(Observed, axis=1), np.sum(Observed, axis=0)) / Sum_Product\n",
    "        likelihood_ratio = 0\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                likelihood_ratio += Observed[i, j] * np.log(Observed[i, j] / Expected[i, j])\n",
    "        likelihood_ratio = 2 * likelihood_ratio\n",
    "        p_value = 1 - scipy.stats.chi2.cdf(likelihood_ratio, 1)\n",
    "        return [likelihood_ratio, p_value]\n",
    "\n",
    "# Another definition of BM effect size that returns the maximum chi square matrix that will allow the calucaltion of chi square from the matrix while ignoring divisions by zer0\n",
    "def Berry_Mielke_Maximum_Corrected_Cramer_V_output_matrix(matrix):\n",
    "    Observed_Chi_Square = chi2_contingency(matrix).statistic\n",
    "    r, c = matrix.shape \n",
    "    sum_of_rows_vector = matrix.sum(axis=1)  \n",
    "    sum_of_cols_vector = matrix.sum(axis=0)  \n",
    "    NR = sum_of_rows_vector.sum()  \n",
    "    NC = sum_of_cols_vector.sum() \n",
    "\n",
    "    matrix = np.zeros((r, c)) \n",
    "    x = np.where(np.isin(sum_of_cols_vector, sum_of_rows_vector), np.argmax(np.isin(sum_of_rows_vector, sum_of_cols_vector), axis=0) + 1, np.nan)\n",
    "    y = np.where(np.isin(sum_of_rows_vector, sum_of_cols_vector), np.argmax(np.isin(sum_of_cols_vector, sum_of_rows_vector), axis=0) + 1, np.nan)\n",
    "    x = x[~np.isnan(x)].astype(int) - 1  \n",
    "    y = y[~np.isnan(y)].astype(int) - 1  \n",
    "\n",
    "    matrix[x,y] = sum_of_rows_vector[x]\n",
    "    sum_of_rows_vector[x] = sum_of_rows_vector[x]-sum_of_rows_vector[x] \n",
    "    sum_of_cols_vector[y] = sum_of_cols_vector[y]-sum_of_cols_vector[y] \n",
    "\n",
    "    while NR > 0 and NC > 0:\n",
    "        NR = np.sum(sum_of_rows_vector)  \n",
    "        NC = np.sum(sum_of_cols_vector)  \n",
    "        x = np.argmax(sum_of_rows_vector)  \n",
    "        y = np.argmax(sum_of_cols_vector)  \n",
    "        z = min(sum_of_rows_vector[x], sum_of_cols_vector[y]) \n",
    "        matrix[x, y] = z\n",
    "        sum_of_rows_vector[x] -= z\n",
    "        sum_of_cols_vector[y] -= z\n",
    "\n",
    "    Maximum_Corrected_Cramers_v = Observed_Chi_Square\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def multilevel_contingency_tables(contingency_table, confidence_level):\n",
    "\n",
    "    # Preperations:   \n",
    "    Sample_Size = np.sum(contingency_table)\n",
    "    Number_of_rows = np.size(contingency_table, axis = 0)\n",
    "    Number_of_Coloumns = np.size(contingency_table, axis = 1)\n",
    "    matrix = np.array(contingency_table)\n",
    "\n",
    "    # Calcualte Chi Square and its significance\n",
    "    row_totals = np.sum(contingency_table, axis=1)\n",
    "    col_totals = np.sum(contingency_table, axis=0)\n",
    "    expected = np.multiply.outer(row_totals, col_totals) / Sample_Size\n",
    "    chi_squared = np.sum( ((contingency_table - expected)**2) / expected)\n",
    "\n",
    "    degrees_of_freedom_chi_square = (Number_of_Coloumns-1) * (Number_of_rows-1)\n",
    "    p_value = calculate_p_value_from_chi_score(chi_squared, degrees_of_freedom_chi_square)\n",
    "\n",
    "    # Effect Sizes for R X C tables\n",
    "    phi_square = chi_squared/Sample_Size \n",
    "    phi = np.sqrt(phi_square) # This is also knows as Cohens w\n",
    "    Cramer_V = (phi_square / (min(Number_of_Coloumns-1, Number_of_rows-1)))**0.5\n",
    "    Tschuprows_T = (phi_square / ( (Number_of_Coloumns-1) * (Number_of_rows-1))**0.5)**0.5\n",
    "    Pearsons_Contingency_Coefficient = np.sqrt(chi_squared/(chi_squared+Sample_Size)) \n",
    "    likelihood_ratio_statistic, likelihood_ratio_pvalue = likelihood_ratio(matrix)\n",
    "\n",
    "    # Bias Corrected Effect Sizes\n",
    "    Phi_Square_Bias_Corrected = phi_square - (1 / (Sample_Size-1)) * (Number_of_Coloumns - 1) * (Number_of_rows - 1)\n",
    "    chi_square_Bias_corrected = Phi_Square_Bias_Corrected * Sample_Size\n",
    "    Phi_Bias_Corrected = np.sqrt(Phi_Square_Bias_Corrected)\n",
    "    Number_of_rows_Corrected = Number_of_rows - (1/(Sample_Size-1)) * (Number_of_rows-1)**2\n",
    "    Number_of_coloumns_Corrected = Number_of_Coloumns - (1/(Sample_Size-1)) * (Number_of_Coloumns-1)**2\n",
    "    \n",
    "    Bias_Corrected_Tschuprows_T = (Phi_Square_Bias_Corrected / ( (Number_of_coloumns_Corrected-1) * (Number_of_rows_Corrected-1))**0.5)**0.5 # Bergsma, 2013\n",
    "    Bias_Corrected_Cramer_V = (Phi_Square_Bias_Corrected / (min(Number_of_coloumns_Corrected-1, Number_of_rows_Corrected-1)))**0.5 # Bergsma, 2013\n",
    "    Bias_Corrected_Contingency_Coefficeint = np.sqrt(Phi_Square_Bias_Corrected/ (Phi_Square_Bias_Corrected+1))\n",
    "\n",
    "    # Maximum Corrected Effect Sizes\n",
    "    k = min(Number_of_coloumns_Corrected, Number_of_rows_Corrected)\n",
    "    Maximum_Contingency_Coefficient =  np.sqrt((k-1) / k) #Sakoda, 1977\n",
    "    Maximum_Corrected_Contingency_Coefficient = Pearsons_Contingency_Coefficient / Maximum_Contingency_Coefficient\n",
    "    Maximum_Tschuprows_T = np.sqrt(np.sqrt(((k-1) / (max(Number_of_Coloumns-1, Number_of_rows-1)))**0.25))\n",
    "    Maximum_Corrected_Tschuprows_T = Tschuprows_T / Maximum_Tschuprows_T\n",
    "    \n",
    "    #Maximum_Corrected_Chi_Square = Berry_Mielke_Maximum_Corrected_Cramer_V(contingency_table) # Berry and mielke Lambda\n",
    "    Maximum_Corrected_Matrix = Berry_Mielke_Maximum_Corrected_Cramer_V_output_matrix(matrix) # Berry and mielke Lambda\n",
    "    row_totals_max = np.sum(Maximum_Corrected_Matrix, axis=1)\n",
    "    col_totals_max = np.sum(Maximum_Corrected_Matrix, axis=0)\n",
    "    total_max = np.sum(Maximum_Corrected_Matrix)\n",
    "    expected_max = np.outer(row_totals_max, col_totals_max) / total_max\n",
    "    \n",
    "    # This Method removes zeros for the calucaltion to avoid division by zero when one clacualte expected values \n",
    "    observeved = Maximum_Corrected_Matrix.flatten()\n",
    "    expected_max = np.array(expected_max).flatten()\n",
    "    zero_positions = np.where((observeved == 0) & (expected_max == 0))[0]\n",
    "    observeved = np.delete(observeved, zero_positions)\n",
    "    expected_max = np.delete(expected_max, zero_positions)\n",
    "    chi_squared_max = np.sum((observeved - expected_max)**2 / expected_max)\n",
    "    Maximum_Corrected_cramers_v = chi_squared / chi_squared_max\n",
    "    \n",
    "\n",
    "    # Calculation of the Variance\n",
    "    Probabilities_Table = contingency_table/Sample_Size\n",
    "    Squared_Probabilities_Table = Probabilities_Table**2\n",
    "    Cubic_Probability_Table = Probabilities_Table**3\n",
    "\n",
    "    column_marginals = np.sum(Probabilities_Table, axis=0)\n",
    "    row_marginals = np.sum(Probabilities_Table, axis=1)\n",
    "    marginal_products = np.outer(row_marginals, column_marginals)\n",
    "    Corrected_Probabilities_Table = Squared_Probabilities_Table / marginal_products\n",
    "\n",
    "    column_marginals_Squared = np.sum(Probabilities_Table, axis=0)**2\n",
    "    row_marginals_Squared = np.sum(Probabilities_Table, axis=1)**2\n",
    "    marginal_products_Corrected= np.outer(row_marginals_Squared, column_marginals_Squared)\n",
    "    Corrected_Squared_Probabilities_Table = Cubic_Probability_Table / marginal_products_Corrected\n",
    "\n",
    "    row_marginals_corrected = np.sum(Corrected_Probabilities_Table, axis=1)\n",
    "    coloumn_marginals_corrected = np.sum(Corrected_Probabilities_Table, axis=0)\n",
    "    marginal_products_corrected_table = np.outer(row_marginals_corrected,coloumn_marginals_corrected )\n",
    "\n",
    "    term1 = np.sum(Corrected_Squared_Probabilities_Table)\n",
    "    term2 = np.sum(row_marginals_corrected**2 / row_marginals)\n",
    "    term3 = np.sum(coloumn_marginals_corrected**2 / column_marginals)  \n",
    "    term4 = np.sum(marginal_products_corrected_table)\n",
    "\n",
    "\n",
    "    # Confidence Intervals using the asymptotic standard error  (Bishop et al., 1975)\n",
    "    z_crit = scipy.stats.norm.ppf(confidence_level + ((1 - confidence_level) / 2))\n",
    "    Standart_deviation_phi_square = np.sqrt((4*term1 - 3*term2 - 3*term3 + 2*term4) / Sample_Size)\n",
    "    ci_lower_phi = phi - Standart_deviation_phi_square*z_crit\n",
    "    ci_upper_phi = phi + Standart_deviation_phi_square*z_crit\n",
    "\n",
    "\n",
    "    # Contingency Coefficient\n",
    "    Contingency_Coefficient_Standard_deviation = (1 / (2*phi*(1+phi_square)**(3/2)))*Standart_deviation_phi_square\n",
    "    Bias_Corrected_Contingency_Coefficient_Standard_deviation = 0 if Bias_Corrected_Cramer_V == 0 else (1 / (2*np.sqrt(Phi_Square_Bias_Corrected)*(1+Phi_Square_Bias_Corrected)**(3/2)))*Standart_deviation_phi_square\n",
    "\n",
    "    q = min(Number_of_Coloumns,Number_of_rows)\n",
    "    Maximum_Corrected_CC_Standard_Error = np.sqrt(  (q / (q-1)) * Contingency_Coefficient_Standard_deviation**2)\n",
    "    ci_lower_cc = Pearsons_Contingency_Coefficient - Contingency_Coefficient_Standard_deviation*z_crit\n",
    "    ci_upper_cc = Pearsons_Contingency_Coefficient + Contingency_Coefficient_Standard_deviation*z_crit\n",
    "    ci_lower_cc_corrected = Maximum_Corrected_Contingency_Coefficient - Maximum_Corrected_CC_Standard_Error*z_crit\n",
    "    ci_upper_cc_corrected = Maximum_Corrected_Contingency_Coefficient + Maximum_Corrected_CC_Standard_Error*z_crit\n",
    "\n",
    "    # Cramer's V \n",
    "    Cramer_V_standart_deviation = (1 / (2 * (min(Number_of_Coloumns-1, Number_of_rows-1))**0.5 * Cramer_V)) * Standart_deviation_phi_square\n",
    "    Bias_Corrected_Cramer_V_standard_deviation = (1 / (2 * (min(Number_of_coloumns_Corrected-1, Number_of_rows_Corrected-1))**0.5 * Bias_Corrected_Cramer_V)) * Standart_deviation_phi_square\n",
    "    ci_lower_cramer = Cramer_V - Cramer_V_standart_deviation*z_crit\n",
    "    ci_upper_cramer = Cramer_V + Cramer_V_standart_deviation*z_crit\n",
    "    ci_lower_cramer_corrected = Bias_Corrected_Cramer_V - Bias_Corrected_Cramer_V_standard_deviation*z_crit\n",
    "    ci_upper_crame_corrected = Bias_Corrected_Cramer_V + Bias_Corrected_Cramer_V_standard_deviation*z_crit\n",
    "\n",
    "    # Tschuprows T\n",
    "    Tschuprows_T_Standard_deviation = (1 / (2 * (Number_of_Coloumns-1) * (Number_of_rows-1) * Tschuprows_T) ) * Standart_deviation_phi_square\n",
    "    Bias_Corrected_Tschuprows_T_Standard_deviation = (1 / (2 * (Number_of_coloumns_Corrected-1) * (Number_of_rows_Corrected-1) * Bias_Corrected_Tschuprows_T) ) * Standart_deviation_phi_square\n",
    "    ci_lower_tschuprows = Tschuprows_T - Tschuprows_T_Standard_deviation*z_crit\n",
    "    ci_upper_tschuprows = Tschuprows_T + Tschuprows_T_Standard_deviation*z_crit\n",
    "    ci_lower_tschuprows_corrected = Bias_Corrected_Tschuprows_T - Bias_Corrected_Tschuprows_T_Standard_deviation*z_crit\n",
    "    ci_upper_tschuprows_corrected = Bias_Corrected_Tschuprows_T + Bias_Corrected_Tschuprows_T_Standard_deviation*z_crit\n",
    "\n",
    "    \n",
    "    \n",
    "    # Confidence Interals using the non-central distribution\n",
    "    lower_ci_ncp, upper_ci_ncp = ncp_ci(chi_squared ,degrees_of_freedom_chi_square, confidence_level)\n",
    "    lower_ci_ncp_bias_corrected, upper_ci_ncp_bias_corrected= ncp_ci(chi_square_Bias_corrected, degrees_of_freedom_chi_square, confidence_level)\n",
    "    lower_ci_ncp_bias_corrected, upper_ci_ncp_bias_corrected = max(lower_ci_ncp_bias_corrected,0), max(upper_ci_ncp_bias_corrected,0)\n",
    "\n",
    "    lower_ncp_max_chi_square, upper_ncp_max_chi_square = ncp_ci(chi_squared_max, degrees_of_freedom_chi_square, confidence_level)\n",
    "\n",
    "    lower_ncp_phi_square = lower_ci_ncp_bias_corrected / Sample_Size\n",
    "    upper_ncp_phi_square = upper_ci_ncp_bias_corrected / Sample_Size\n",
    "\n",
    "    \n",
    "    # NCP Confidence interval for Contingency Coefficient\n",
    "    ci_lower_cc_ncp = np.sqrt(lower_ci_ncp / (lower_ci_ncp + Sample_Size))\n",
    "    ci_upper_cc_ncp = np.sqrt(upper_ci_ncp / (upper_ci_ncp + Sample_Size))\n",
    "    ci_lower_cc_bias_corrected_ncp = np.sqrt(lower_ci_ncp_bias_corrected / (lower_ci_ncp + Sample_Size))\n",
    "    ci_upper_cc_bias_corrected_ncp = np.sqrt(upper_ci_ncp_bias_corrected / (upper_ci_ncp + Sample_Size))\n",
    "    ci_lower_cc_max_corrected_ncp = np.sqrt(((lower_ncp_phi_square * q) / ((q-1)* (1+lower_ncp_phi_square))))\n",
    "    ci_upper_cc_max_corrected_ncp = np.sqrt(((upper_ncp_phi_square * q) / ((q-1)* (1+upper_ncp_phi_square))))\n",
    "       \n",
    "    # NCP Confidence interval for Cramer V\n",
    "    ci_ncp_lower_cramer = np.sqrt(lower_ci_ncp / (Sample_Size * q))\n",
    "    ci_ncp_upper_cramer = np.sqrt(upper_ci_ncp / (Sample_Size * q))\n",
    "    ci_ncp_lower_cramer_bias_corrected = np.sqrt(lower_ci_ncp_bias_corrected / (Sample_Size * q))\n",
    "    ci_ncp_upper_cramer_bias_corrected = np.sqrt(upper_ci_ncp_bias_corrected / (Sample_Size * q))\n",
    "    ci_ncp_lower_cramer_max_corrected = lower_ci_ncp / lower_ncp_max_chi_square\n",
    "    ci_ncp_upper_cramer_max_corrected = upper_ci_ncp / upper_ncp_max_chi_square\n",
    "\n",
    "\n",
    "    # NCP confidence Intervals for Tschuprow's T\n",
    "    ci_ncp_lower_Tschuprow = np.sqrt(lower_ci_ncp / (Sample_Size * np.sqrt(degrees_of_freedom_chi_square)))\n",
    "    ci_ncp_upper_Tschuprow = np.sqrt(upper_ci_ncp / (Sample_Size * np.sqrt(degrees_of_freedom_chi_square)))\n",
    "    ci_ncp_lower_Tschuprow_bias_corrected = np.sqrt(lower_ci_ncp_bias_corrected / (Sample_Size * np.sqrt(degrees_of_freedom_chi_square)))\n",
    "    ci_ncp_upper_Tschuprow_bias_corrected = np.sqrt(upper_ci_ncp_bias_corrected / (Sample_Size * np.sqrt(degrees_of_freedom_chi_square)))\n",
    "\n",
    "\n",
    "    # 3. Uncertainty Coefficient - Needs to be in a Contingency Table\n",
    "    Sum_Of_Rows = np.sum(contingency_table, axis=1)\n",
    "    Sum_Of_Columns = np.sum(contingency_table, axis=0)\n",
    "    HX = -np.sum((Sum_Of_Rows * np.log(Sum_Of_Rows / Sample_Size)) / Sample_Size)\n",
    "    HY = -np.sum((Sum_Of_Columns * np.log(Sum_Of_Columns / Sample_Size)) / Sample_Size)\n",
    "    HXY = -np.sum(contingency_table * np.log(contingency_table / Sample_Size)) / Sample_Size\n",
    "    \n",
    "    # Calculate Effect Size UC \n",
    "    Uncertainty_Coefficient_Symmetric = 2 * (HX + HY - HXY) / (HX + HY)\n",
    "    Uncertainty_Coefficient_Rows = (HX + HY - HXY) / HX\n",
    "    Uncertainty_Coefficient_Columns = (HX + HY - HXY) / HY\n",
    "\n",
    "    # Calculate the Asymptotic Standard Errors (Standard Errors)\n",
    "    Standard_Error_Symmetric = np.sqrt((4 * np.sum(contingency_table * (HXY * np.log(np.outer(Sum_Of_Rows, Sum_Of_Columns) / Sample_Size ** 2) - (HX + HY) * np.log(contingency_table / Sample_Size)) ** 2) / (Sample_Size ** 2 * (HX + HY) ** 4)))\n",
    "    Standard_Error_Rows = np.sqrt(np.sum(contingency_table * (HX * np.log(contingency_table / Sum_Of_Columns) + (HY - HXY) * np.log(Sum_Of_Rows / Sample_Size)[:, np.newaxis]) ** 2) / (Sample_Size ** 2 * HX ** 4))\n",
    "    Standard_Error_Columns = np.sqrt(np.sum(contingency_table * (HY * np.log(contingency_table / Sum_Of_Rows[:, np.newaxis]) + (HX - HXY) * np.log(Sum_Of_Columns / Sample_Size)) ** 2) / (Sample_Size ** 2 * HY ** 4))\n",
    "\n",
    "    # Calculate p_values        \n",
    "    Z_value_Symmetric = Uncertainty_Coefficient_Symmetric / Standard_Error_Symmetric\n",
    "    Z_value_Rows = Uncertainty_Coefficient_Rows / Standard_Error_Rows\n",
    "    Z_value_Columns = Uncertainty_Coefficient_Columns / Standard_Error_Columns\n",
    "\n",
    "    # Confidence Intervals\n",
    "    Zcrit = 1 - (1 - confidence_level) / 2\n",
    "    Symmetric_CIs = Zcrit * np.sqrt(Standard_Error_Symmetric) * np.array([-1, 1]) + Uncertainty_Coefficient_Symmetric\n",
    "    Rows_CIs = Zcrit * np.sqrt(Standard_Error_Rows) * np.array([-1, 1]) + Uncertainty_Coefficient_Rows\n",
    "    Columns_CIs = Zcrit * np.sqrt(Standard_Error_Columns) * np.array([-1, 1]) + Uncertainty_Coefficient_Columns \n",
    "\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Add the relevant parameters to the dictionary\n",
    "    results[\"Sample Size\"] = Sample_Size\n",
    "    results[\"Number of Rows\"] = Number_of_rows\n",
    "    results[\"Number of Columns\"] = Number_of_Coloumns\n",
    "    results[\"____________________________________________\"] = ''  \n",
    "\n",
    "    results[\"Chi Square\"] = round(chi_squared, 4)\n",
    "    results[\"Degrees of Freedom Chi Square\"] = round(degrees_of_freedom_chi_square, 4)\n",
    "    results[\"p_value chi square\"] = round(p_value, 10) # this one is suitable for all kind of effect sizes\n",
    "\n",
    "    results[\"___________________________________________\"] = ''  \n",
    "    results[\"Phi Square\"] = round(phi_square, 10)\n",
    "    results[\"Phi / Cohen's w\"] = round(phi, 4)\n",
    "    results[\"Cramer V\"] = round(Cramer_V, 4)\n",
    "    results[\"Tschuprow's T\"] = round(Tschuprows_T, 7)\n",
    "    results[\"Pearson's Contingency Coefficient\"] = round(Pearsons_Contingency_Coefficient, 4)\n",
    "    results[\"Likelihood Ratio\"] = round(likelihood_ratio_statistic, 4)\n",
    "    results[\"Likelihood Ratio p_value\"] = round(likelihood_ratio_pvalue, 4)\n",
    "\n",
    "    # Bias Corrected Measures\n",
    "    results[\"__________________________________________________\"] = ''  \n",
    "    results[\"Adjusted Phi\"] = round(Phi_Square_Bias_Corrected, 7)\n",
    "    results[\"Adjusted Contingency Coefficient\"] = round(Bias_Corrected_Contingency_Coefficeint, 4)\n",
    "    results[\"Adjusted Cramer's V\"] = round(Bias_Corrected_Cramer_V, 7)\n",
    "    results[\"Adjusted Tschuprow's T\"] = round(Bias_Corrected_Tschuprows_T, 7)\n",
    "\n",
    "    #Maximum Corrected Measures\n",
    "    results[\"__________________________________________\"] = ''  \n",
    "    results[\"Maximum Corrected Contingency Coefficient (Sakoda, 1977)\"] = round(Maximum_Corrected_Contingency_Coefficient, 4)\n",
    "    results[\"Maximum Corrected Tschuprow's T\"] = round(Maximum_Corrected_Tschuprows_T, 7)\n",
    "    results[\"Maximum Corrected Cramers V (Berry, Mielke, Jhonston)\"] = round(Maximum_Corrected_cramers_v, 7)\n",
    "\n",
    "    # Standrd dviations\n",
    "    results[\"_________________________________________\"] = ''  \n",
    "    results[\"Standard Deviation of Phi Square\"] = round(Standart_deviation_phi_square, 4)\n",
    "    results[\"Standard Deviation of Contingency Coefficient\"] = round(Contingency_Coefficient_Standard_deviation, 4)\n",
    "    results[\"Standard Deviation of Maximum Corrected Contingency Coefficient\"] = round(Maximum_Corrected_CC_Standard_Error, 4)\n",
    "    results[\"Standard Deviation of Cramer V\"] = round(Cramer_V_standart_deviation, 4)\n",
    "    results[\"Standard Deviation of Tschuprows T\"] = round(Tschuprows_T_Standard_deviation, 4)\n",
    "\n",
    "    # Asymptotic CI's\n",
    "    results[\"______________________________________________\"] = ''\n",
    "    results[\"CI Phi / Cohens w\"] = f\"({round(ci_lower_phi, 4)}, {round(ci_upper_phi, 4)})\"\n",
    "    results[\"CI maximum corrected Contingency Coefficient\"] = f\"({round(ci_lower_cc_corrected, 4)}, {round(ci_upper_cc_corrected, 4)})\"\n",
    "    results[\"CI Contingency Coefficient\"] = f\"({round(ci_lower_cc, 4)}, {round(ci_upper_cc, 4)})\"\n",
    "    results[\"CI maximum corrected Contingency Coefficient\"] = f\"({round(ci_lower_cc_corrected, 4)}, {round(ci_upper_cc_corrected, 4)})\"\n",
    "    results[\"CI Cramer V\"] = f\"({round(ci_lower_cramer, 4)}, {round(ci_upper_cramer, 4)})\"\n",
    "    results[\"CI bias corrected Cramer V\"] = f\"({round(ci_lower_cramer_corrected, 4)}, {round(ci_upper_crame_corrected, 4)})\"\n",
    "    results[\"CI Tschuprow's T\"] = f\"({round(ci_lower_tschuprows, 4)}, {round(ci_upper_tschuprows, 4)})\"\n",
    "    results[\"CI bias corrected Tschuprows T\"] = f\"({round(ci_lower_tschuprows_corrected, 4)}, {round(ci_upper_tschuprows_corrected, 4)})\"\n",
    "\n",
    "    # Non Central CI's\n",
    "    results[\"______________________________________________________\"] = '' \n",
    "    results[\"NCP CI Contingency Coefficient\"] = f\"({round(ci_lower_cc_ncp, 4)}, {round(ci_upper_cc_ncp, 4)})\"\n",
    "    results[\"NCP CI Cramer V\"] = f\"({round(ci_ncp_lower_cramer, 4)}, {round(ci_ncp_upper_cramer, 4)})\"\n",
    "    results[\"NCP CI Tschuprow's T\"] = f\"({round(ci_ncp_lower_Tschuprow, 4)}, {round(ci_ncp_upper_Tschuprow, 4)})\"\n",
    "    results[\"NCP CI Contingency Coefficient Bias corrected\"] = f\"({round(ci_lower_cc_bias_corrected_ncp, 4)}, {round(ci_upper_cc_bias_corrected_ncp, 4)})\"\n",
    "    results[\"NCP CI Cramer's V Bias corrected\"] = f\"({round(ci_ncp_lower_cramer_bias_corrected, 4)}, {round(ci_ncp_upper_cramer_bias_corrected, 4)})\"\n",
    "    results[\"NCP CI Tschuprow's T Bias corrected\"] = f\"({round(ci_ncp_lower_Tschuprow_bias_corrected, 4)}, {round(ci_ncp_upper_Tschuprow_bias_corrected, 4)})\"\n",
    "    results[\"NCP CI Contingency Coefficient Max corrected\"] = f\"({round(ci_lower_cc_max_corrected_ncp, 4)}, {round(ci_upper_cc_max_corrected_ncp, 4)})\"\n",
    "    results[\"NCP CI Cramer's V Max corrected\"] = f\"({round(ci_ncp_lower_cramer_max_corrected, 4)}, {round(ci_ncp_upper_cramer_max_corrected, 4)})\"\n",
    "\n",
    "    # Uncertinty Coefficient\n",
    "    results[\"Uncertainty Coefficient (Symmetric)\"] = Uncertainty_Coefficient_Symmetric\n",
    "    results[\"Uncertainty Coefficient (Rows)\"] = Uncertainty_Coefficient_Rows\n",
    "    results[\"Uncertainty Coefficient (Columns)\"] = Uncertainty_Coefficient_Columns\n",
    "    results[\"Uncertainty Coefficient Standard Error (Symmetric)\"] = Standard_Error_Symmetric\n",
    "    results[\"Uncertainty Coefficient Standard Error (Rows)\"] = Standard_Error_Rows\n",
    "    results[\"Uncertainty Coefficient Standard Error (Columns)\"] = Standard_Error_Columns\n",
    "    results[\"Uncertainty Coefficient Z-value (Symmetric)\"] = Z_value_Symmetric\n",
    "    results[\"Uncertainty Coefficient Z-value (Rows)\"] = Z_value_Rows\n",
    "    results[\"Uncertainty Coefficient Z-value (Columns)\"] = Z_value_Columns\n",
    "    results[\"Uncertainty Coefficient Confidence Intervals (Symmetric)\"] = Symmetric_CIs\n",
    "    results[\"Uncertainty Coefficient Confidence Intervals (Rows)\"] = Rows_CIs\n",
    "    results[\"Uncertainty Coefficient Confidence Intervals (Columns)\"] = Columns_CIs\n",
    "\n",
    "   \n",
    "    result_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "    return result_str \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
